\section{Experiment Framework}
\label{section:method:experiment-framework}
In order to make it easier to execute multiple experiments, which all saved
enough metadata for the experiment to be recreatable and repeatable we made
a framework. It can be deconstructed into
four main modules. The configuration module, a data processing module, an experiment module, and the save experiment module.


\subsection{Pipeline module}
\label{section:Method:Pipeline}
The data processing module has two functions.
The first one is to streamline all data processing steps and structure them in one shared place.
Secondly, to have a self-documenting data module that can easily save
every processing step applied to the data before the experiments are executed.
Examples of a pipeline steps output are shown in \Cref{table:base_data_processing_steps}
and in \Cref{table:lstm_data_processing_steps}.

\begin{table}[h]
  \caption{Base data processing steps}
  \label{table:base_data_processing_steps}
  \begin{tabular}{ll}
    \toprule
    Step       & Description                                                   \\
    \midrule
    \textbf{1} & load market insight data and categories and merge them        \\
    \textbf{2} & convert date columns to date\_time format                     \\
    \textbf{3} & sum up clicks to category level [groupBy(date, cat\_id)]      \\
    \textbf{4} & rename column 'title' to 'cat\_name'                          \\
    \textbf{5} & combine feature 'hits' and 'clicks' to new feature 'interest' \\
    \textbf{6} & drop columns 'hits' and 'clicks'                              \\
    \textbf{7} & filter out data from early 2018-12-01                         \\
    \textbf{8} & drop uninteresting colums                                     \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{LSTM data processing steps}
  \label{table:lstm_data_processing_steps}
  \begin{tabular}{ll}
    \toprule
    Step       & Description                                                                  \\
    \textbf{1} & Convert input dataset to generator object                                    \\
    \textbf{2} & filter out category                                                          \\
    \textbf{3} & choose columns 'interest' and 'date'                                         \\
    \textbf{4} & fill in dates with zero values                                               \\
    \textbf{5} & convert to np.array                                                          \\
    \textbf{6} & scale data using standardization                                             \\
    \textbf{7} & generate x y pairs with sliding window with input size 10, and output size 7 \\
    \textbf{8} & generate training and validation data with training size 7                   \\
    \bottomrule
  \end{tabular}
\end{table}

% TODO: Do we need the ARIMA pipline steps as well? I think it might be overkill.
% commented out for now..
%\begin{table}[h]
%  \centering
%  \caption{LSTM data processing steps}
%  \label{table:arima_data_processing_steps}
%  \begin{tabular}{ll}
%    \toprule
%    Step       & Description                                                     \\
%    \textbf{1} & Convert input dataset to generator object                       \\
%    \textbf{2} & filter out category 2)                                          \\
%    \textbf{3} & choose columns 'interest' and 'date'                            \\
%    \textbf{4} & fill in dates with zero values                                  \\
%    \textbf{5} & Scaling data?: False                                            \\
%    \textbf{6} & split up into training set and test set of forecast window size \\
%    \bottomrule
%  \end{tabular}
%\end{table}

% TODO: What is the functionality of the pipeline, and why is it created?
% TODO: Add illustration of the project pipeline

%With the aim of running several experiments with ease, a experiment pipeline was created in this project.
%Running a project is done through the use of the shared pipeline, through the use of different data pipelines, model structure and model implementations, and different configureations.

Multiple pipelines were created in order to support all the different experiments.
While the deep learning methods such as the LSTM and the hybrid model only need two shared pipelines,
one for univariate and one for multivariate,
other experiments such as the SARIMA is in need of other pipelines.
These pipelines are designed with the aim of modularity,
making the expansion of multiple pipelines with multiple shared steps easy.




\subsection{Config module}
The configuration module is created with the aim of making model selection and configuration more easily accessable.
Using separate configuration files for model configurations,
the ammount of code that need to be changed for the different experiments are reduced.
Additionaly, by logging the configuration of models for each experiment,
backtracking the results of different configurations are more accessable.

The configuration is done through the use of two separate ``.yaml'' files.
The first one is the ``config.yaml'' file.
This contains information that is shared among all of the different experiments,
such as the selected data files, random seeds, logged error metrics etc.
Secondly, a separate configuration file contains the information more relevant to the spesific experiment.
These configuration files are contained within the ``config'' folder,
and contain information such as the selected predictive model for the experiment,
as well as the model hyper parameters and tuning parameters.

During runtime, the two configuration files are merged, adding the information from the model configuration to the more general experimen configuration.
The config is parsed through at runtime reading the needed information when it becomes relevant.
An example of a config is in the appendix at \Cref{cha:experiment-framework-example-config}, but is also available with the project source code.


% Old text
\iffalse
  The configuration module is to keep everything which is configurable and relevant
  for the experiment in one place. This helps keeping code changes down to a minumum,
  and it's easy to go back in time and check details of a ran experiment at a glans.

  The experiment pipeline contains multiple paramteres that can be adjusted and configured to alter the experiments.
  Configuration is done through the use of the ``config.yaml'' file defined in the project source code.
  This config file is used to configure the different aspects of the projects to be run.
  Ranging from the selected dataset and data files to be used, to the selected model and model structure, with model parameters and random seed.

  The config is parsed through at runtime reading the needed information when it becomes relevant.
  The file can be found in the source code at ``./config.yaml''.
  An example of a config is in the appendix at \Cref{cha:experiment-framework-example-config}.
\fi



\subsection{Save Experiment module}

The save source module handled everything related to logging and saving an experiment.
Each experiment is associated with an unique descriptive ID and a description text to easily
differentiate between experiments.
Each experiment saves trained models, configs, dataset used, stdout logs, training metrics, validation metrics,
testing metrics and figures.

Everything is save to the local ``./models/`` folder, and to an exernal ML experiment tracking tool
named \textit{neptune.ai}
\dirtree{%
  .1 arima-predict-cat-11037-7-days .
  .2 Arima-11037.pkl .
  .2 data-processing-steps.txt .
  .2 datasets.json .
  .2 figures .
  .3 11037-Data-Prediction.png .
  .3 11037-Predictions.png .
  .3 11037-Training-data-approximation.png .
  .3 11037-Training-data.png .
  .2 logging .
  .2 training-errors.csv .
  .2 metrics.txt .
  .2 options.yaml .
  .2 predictions.csv .
  .2 tags.txt .
  .2 title-description.txt .
}
2 directories, 13 files

\subsection{Packages and verions}

\begin{table}[h]
  \centering
  \caption{Experiment Python packages and versions}
  \label{table:python-packages-most-important}
  \begin{tabular}{|l|l|}\hline
    Package                      & Version \\ \hline
    \hline
    matplotlib                   & 3.5.1   \\
    matplotlib-inline            & 0.1.3   \\
    numpy                        & 1.22.2  \\
    pandas                       & 1.4.0   \\
    pandocfilters                & 1.5.0   \\
    sklearn                      & 0.0     \\
    statsmodels                  & 0.13.1  \\
    torch                        & 1.10.2  \\
    optuna                       & 2.10.0  \\
    plotly                       & 5.6.0   \\
    pytorch-lightning            & 1.5.10  \\
    keras                        & 2.8.0   \\
    tensorflow                   & 2.8.0   \\
    tensorflow-io-gcs-filesystem & 0.24.0  \\
    optkeras                     & 0.0.7   \\
    pmdarima                     & 1.8.5   \\
    \hline
  \end{tabular}
\end{table}
\Cref{table:python-packages-most-important} show the most important
python packages used for the experimental setup and their respective versions.
The full list can be found in the appendix
\Cref{table:python-packages-all}
\todo[inline]{Update the python packages list}