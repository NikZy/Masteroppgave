
\section{Loss function and Metrics}
\label{section:Method:Metrics}

Here the used loss function and metrics are described.

\subsection*{Loss function}
Since our datasets often contain a lot of outliers we decided not to use the well known
Mean Squared Error (MSE) [\Cref{section:BT:Loss}].
Instead, we used Mean Absolute Error (MAE) while training the neural networks.
We can use the MAE during training because of the data-preprocessing,
which normalizes the time series values.
Some experiments were done using both MASE and sMASE, but MAE did perform better.

\subsection*{Metrics}
%Since we are working with multiple time series and each time series might differ in scale.
When choosing an error metric, we have to accommodate a number of factors.
Since our time series are of different scales, we cannot use a scale-dependent metric
like MSE or MAE on our test data, as the test data is never touched, each time series might be on different scales.

We use MASE with 1-day naive forecast, as well as MASE with a 7-day
naive forecast. This is because the 1-day naive forecast is a regular metric used
and can give a good impression of how the model performs.
However, a 7-day naive forecast will more closely represent a real-world application measure,
because if a model gets a score higher than $1$, it means that it has no real world application,
and it's better to use the previous week as a forecast for the next week.

A problem with MASE is that if a time series follows a random walk
then the best forecast will always be the naive forecast. MASE is therefore dependent on
how good the naive prediction is.
Therefore we also include sMAPE as a metric.
sMAPE will give a better impression of how well our predicted forecasts fit the target values,
independently of the naive forecast.
In the chosen datasets zero values rarely occur.
Therefore, the sMAPE metric can be used without worrying about zero divisions [\Cref{section:BT:Loss}].

