\section{LSTM Method}
The lstm...

% Train val split
%When working with time series it is important to think about how one splits up
%the dataset into training, validation, and test set.
%One key aspect when forecasting a time series is the newer the information,
%the more relevant it is for the forecast.
%In a perfect world the model will be able to use all the available data for training,
%right up to the point of forecast, then try to forecast the wanted horizon.
%However, in order to tune hyperparameters and avoid overfitting to the training
%set, we need a validation set.

% In a stateful LSTM everything fed through the network is seen by the network
% as follow each other in time. Since the validation step is executed at the end of
% each epoch, and each epoch pass feeds all the training data through the network,
% it makes sense to use the last section of the training data as the validation data.

\subsection{LSTM Architecture}
The LSTM consisted of $n$ layers of stacked LSTM cells followed by a
dense layer with the same output size as our forecast horizon.

\begin{table}[h]
  \centering
  \caption{LSTM cell parameters. Values with a T value was tuned}
  \label{table:LSTM-cell-parameters}
  \begin{tabular}{|l|l|}\hline
    Parameter              & value             \\ \hline
    \hline
    units,                 & T                 \\
    activation             & "tanh"            \\
    recurrent\_activation  & "sigmoid"         \\
    use\_bias              & True              \\
    kernel\_initializer    & "glorot\_uniform" \\
    recurrent\_initializer & "orthogonal"      \\
    bias\_initializer      & "zeros"           \\
    unit\_forget\_bias     & True              \\
    kernel\_regularizer    & None              \\
    recurrent\_regularizer & None              \\
    bias\_regularizer      & None              \\
    activity\_regularizer  & None              \\
    kernel\_constraint     & None              \\
    recurrent\_constraint  & None              \\
    bias\_constraint       & None              \\
    dropout                & T                 \\
    recurrent\_dropout     & T                 \\
    return\_sequences      & False             \\
    return\_state          & False             \\
    go\_backwards          & False             \\
    stateful               & True              \\
    time\_major            & False             \\
    unroll                 & False             \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{LSTM dense parameters}
  \label{table:LSTM-dense-cell-parameters}
  \begin{tabular}{|l|l|}\hline
    Parameter             & value                \\ \hline
    \hline
    units                 & outout\_window\_size \\
    activation            & None                 \\
    use\_bias             & True                 \\
    kernel\_initializer   & "glorot\_uniform"    \\
    bias\_initializer     & "zeros"              \\
    kernel\_regularizer   & None                 \\
    bias\_regularizer     & None                 \\
    activity\_regularizer & None                 \\
    kernel\_constraint    & None                 \\
    bias\_constraint      & None                 \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{LSTM Hyperparameter tuning range}
  \label{table:LSTM-hyperparameters-tuning-range}
  \begin{tabular}{|l|l|}\hline
    hyperparameter       & Tuning range        \\ \hline
    \hline
    hidden\_size         & [1, 100]            \\
    number\_of\_layers   & [1, 2]              \\
    dropout              & [0.0, 0.4]          \\
    recurrent\_dropout   & [0.0, 0.4]          \\
    optimizer\_name      & ['RMSprop', 'Adam'] \\
    learning\_rate       & [1e-7, 1e-2]        \\
    number\_of\_epochs   & [1, 40]             \\
    batch\_size          & [32, 32]            \\
    input\_window\_size  & 10                  \\
    output\_window\_size & 7                   \\
    number\_of\_features & 1                   \\
    number\_of\_trials   & 200                 \\
    stateful\_lstm       & true                \\
    \hline
  \end{tabular}
\end{table}

\subsection{Handling hidden states}




% Tuning: How did we specify tuning ranges

% Model choice: 

% Hvordan har vi valgt Ã¥ tune med trening og test set

% Global model, how did we train on multiple dataset?