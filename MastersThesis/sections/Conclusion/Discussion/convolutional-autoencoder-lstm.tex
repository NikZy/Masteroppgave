
\subsection{Convolutional Autoencoder with LSTM}
\label{section:Discussion:Discussion:CNN-AE-LSTM}

% What was the aim of the experiments?
% --> Apply the CNN-AE and LSTM model to a new dataset with real world applicaitons
% --> Verrify the ability of ther hybrid model against the baseline LSTM
% --> Thus verrifying the claims done by "Zhao2019"



% Link this to RQ-5 (CNN-AE and LSTM against the baseline LSTM)
% The paper is based on "Local univariate" models. We therefor first mention this
% All withing the margin of error
% We are not able to recreate the values by Zhao2019

% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.208   -  0.208 ->  0.0%
% Dataset 2 -  0.684   -  0.716 ->  -4.57%
% Dataset 3 -  0.477   -  0.483 ->  -1.25%

After doing experiments with the LSTM and the CNN-AE and LSTM model on the three datasets used in this paper,
the results from the predictions are pressented in \cref{section:Results}.
Using the hybrid model it is clear that we are not able to recreate the values presented in \cite{Zhao2019}
for each of the different datasets.

Experiments conducted on the different datasets result in predictions that can vary vastly,
and some that are quite similar between the models.

The local univariate LSTM and the local univariate convolutional autoencoder and LSTM are one of the model-configurations
that offer rather similar results across datasets.
While the hybrid model CNN-AE-LSTM performance is more or less equal, predictions made on dataset 2 and 3 vary a bit more
with the basic LSTM model performing, on average, 4.6\% and 1.25\% better on average.
However, in addition to these similarities in average performance, the T-test conducted on the experiments reveals the same outcome.
The t-test Resultsults shown in [TODO: Link to t-test table] infer that the predictions made by the two local univariate models
are from the same group, and there is no significant difference in their distribution.

The t-test and average metrics attained through the experimentation therefore infer that the hybrid model CNN-AE-LSTM
is not able to improve performance over the LSTM model on a local univariate model.


One reason behind this lack of difference in predictions might originate in the selected data.
The task of the convolutional autoencoder is to encode and reconstruct the input data of the model, 
removing noise and other unneeded information in the process.
Lack of data could serve as an issue in this case.
As each of the time series used in the local univariate model only consists of around 3 years of data, or 1300 data points.
There is a possibility that there is not enough similar data for the autoencoder to recognize similarities in data
so that it can then remove the extra noise.
Additionally, the amount of noise contained in the data is not known.
If the datasets contain little noise, the autoencoder is more likely to remove relevant data, thus impairing the performance of the LSTM model.
Due to the implications that the lack of noise in the data potentially could infer from the predictions,
additional experiments are done on data with high noise and low noise.
We will come back to these experiments later in this discussion.

However, as is seen from the predictions made by the two models, it is clear that the convolutional autoencoder and lstm
does not have improved predictions compared to the LSTM model.
We are therefore not able to reproduce the results presented by \cite{Zhao2019}
which showed a significant improvement using the convolutional autoencoder and LSTM over the baseline LSTM model.
% Whether this is due to the used dataset or not is however not entirely certain.



% We have now seen how it works on univariate local models
% We use new models, exploring multivariate and global
% It is not always better in these cases
% How do we explain this?
% Can we compare directly with the work done in Zhao2019? What error metrics were used in these models?
While the paper \cite{Zhao2019} only focuses on applying local univariate models,
we wished to explore the expansion of the use of the hybird model with other configurations.
These inclde the use of multivariate models and global models for each dataset.
Exploring the use of such variations to the Convolutional autoencoder and LSTM,
we hoped to uncover cases where the model performance is increased over the standard local uniariate model.



\subsubsection{Global univariate models}
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.203   -  0.210   ->  -3.4 %
% Dataset 2 -  0.662   -  0.675   ->  -1.9%
% Dataset 3 -  0.425   -  0.417   ->  1.9 %


\todo[inline]{Update after T tests results arrive}
The results on dataset 1 for the local univariate and the local multivariate models
confirms our hypothesis presented in RQ5 [\ref{G&R:RQ-CNN-AE-LSTM}], that the
CNN-AE-LSTM does achieve higher accuracy over a plain LSTM by a few percentage points.

The only exception is for the global models, where the CNN-AE-LSTM in general performs worse.
It seems the benefits of a global model for RNNs are not transferable to other types
of NNs. LSTMs ability to learn across multiple time series can be attributed to the fundamentals
of how the LSTM works. \cite{Zhao2019} discusses previous work that concludes that
the LSTMs memory cell is mainly responsible for the performance of the LSTM.
When training the LSTM on multiple time series, this memory cell is local to each
time series and resets before each new time series is fed through the network.
The LSTM weights on the other hand are global across all the series.
These local and global characteristics are unique to RNNs, and can explain
why the Convolutional Autoencoder does not see the same benefits when trained on multiple
time series.

The results of Experiment 4 [\Cref{section:results:additional-experimental-plan:Experiment-4}],
and Experiment 1 shows a small increase in performance for the hybrid model on high variance datasets.
Even though these results are not statistically significant, the sample sizes were relatively small
(20 samples, and 7 samples), which might be too small to conclude when the
performance increase is low.

However, from the results of Experiment 4, we can empirically conclude that the CNN-AE-LSTM will
hurt the performance if the datasets contain small amounts of noise. This downside is much
worse than the potential performance gains on noisy datasets.
This can explain the poor results on dataset 2. Since dataset 2 contains a wide variety
of time series.




%Unlike the local univariate models described above, the hybrid model performe quite similar, although somewhat worse than the LSTM model
%on both dataset 1 and 2.
%However, dataset 3 offers an improvement of a little over 10\% of the sMAPE error metric by the hybrid model compared to the LSTM.
%
%The results from the global univarite model are somewhat unexpected.
%Dataset 1 and 2 were selected due to their contrast in corrolation between the time series in the dataset.
%Dataset 1 has time-series wich correlate the most with each other, while dataset 2 has time-series with the least ammount of correlating between them.
%Using an convolutional autoencoder and LSTM, it was expected that the additional data made available through the use of a global configuration
%whould be utilized in order to improve the abilities of the autoencoder.
%The autoencoder would have more data to work with in order to improve its encoding and reconstructing of data,
%and considering the data corrolated to some extend, dataset 1 should be easier to work with as much of the data corrolate.
%
%However, as the experiments have shown, both dataset 1 and 2 performes poorly, by beeing a little wors than a LSTM model with the same
%global univariate configuration.
%\todo[inline]{Why? Does it make sense that we get these results here?}
%\todo[inline]{How does the model compare with the basic local univariate hybird model?}





\subsubsection{Local multivariate models}

Afther the use of global models, a configuration of multivariate models was attempted.
Unlike the global models, the ammount of data is increased not through the addition of other time-series to the model,
but by decomposing the information within each time-series.
By decomposing information such as day of the week, or season, the amount of data supplied to the models is increased.


Using the hybrid convolutional autoencoder and LSTM model should result in both improvements and degradation in performance.

Initially, the performance of the model is likely to suffer due to the fact that
the model is required to encode and reconstruct additional input data using the same model structure.
The model might encounter problems recreating the data using the same model as for the univariate model
while attempting to recreate additional data per data point.
If the autoencoder then is limited by the number of data entries on top of this, the model performance could suffer.

However, the same reasoning might also help improve the hybrid model's performance in some situations.
While more data would need to be encoded and reconstructed, the autoencoder will retain more information
regarding the development of trends and spikes through different seasons.
% Den er ikke like grov i kantene
While a pattern migh repeat over several seasons, there migh be additional hidden information in the seasonal information.
If a pattern is known to never spike during summer but often spikes through the winter,
the autoencoder should be able to differentiate between the different seasons.
Thus, the task of the autoencoder would be to retain information about changes in trends also dependent on seasonal data.


% Second -> Use of local multivariate
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.183   -  0.181  ->  1.1%
% Dataset 2 -  0.603   -  0.743  ->  -23.2%
% Dataset 3 -  0.359   -  0.354  ->  1.4%
However, while this reasoning is based on the autoencoders ability to encode and differentiate between values based on season,
the model would be severely limited by the ammount of data.
If there is not enough data available the autoencoder would not be able to learn of such connections.
As discussed in \cref{section:Data:DataExploration}, the data attained from ``Prisguiden.no'' only include datapoints
for a little over 3 years.
As with the problems discussed with the local univariate model, the multivariate model would also likely be limited by the amount of data.

This is reflected in the results acquired through testing.
While dataset 1 and 3 supply slight improvements to predictions, there are no clear improvements.
Additionally, the t-test applied to the predictions supports this assumption.
The t-test implies that there are no significant differences between the predictions made with the local multivariate model.


However, the results are quite different for dataset 2.
Considering the sMAPE error metric there is about a 23\% performance decrease using the convolutional autoencoder and LSTM.
This is also supported by the t-test conducted. The results from the t-test imply that the difference between the hybrid model and the LSTM model
is significant within the 95\% window of certainty, supplied by a p-value of under 0.05.
Although this is only the case for dataset 2, the convolutional autoencoder and LSTM is significantly worse performing than the LSTM model.
% Why is this?
The significant decrease in performance could be due to the lack of data as discussed earlier.
With the local univariate model, there is a 4\% performance decrease with the hybrid model.
Although this is not significant, this could point to the reason behind the poor performance.
While the local univariate model and the local multivariate models both suffer from a lack of data with each dataset.
Both models perform worse on dataset 2, while the performance on dataset 1 is somewhat better both using univariate and multivariate.
This could imply that the selected dataset is less suited for using the autoencoder.
This could then again imply that the convolutional autoencoder is heavily influenced by the nature of the data that is used.
The implications of the dataset's characteristics are explored further later when the use of noisy data is discussed.
However, if this is the case, the lack of data could explain the poor performance of dataset 2.
The multivariate model would have the same number of time-steps in each time-series but would be required to encode four times the data per time-step.
Therefore, the autoencoder is more likely to perform worse in reconstructing the input data,
thus contributing to a worse performance by the LSTM component.


Although the local multivatiate model is the overall best performing of the convolutional autoencoder and LSTM models,
it is outperformed by the LSTM, indicating that the reason for the improvements by the model
is entirely contributed by the improved LSTM model.





\subsubsection{Global Multivariate models}

Expanding on the use of global models and multivariate models,
a global multivariate hybrid model is defined and tested.

Even though the model is a global method; it does not escape the problems retained with a multivariate model.
Although the number of datasets has increased, this does not nessesarily improve the performance of the models.
The reason behind this is the lack of correlation between the time-series in the datasets.
Although dataset 1 contains datasets with higher correlation than dataset 2 and 3, it appears this is not enough to make any significant difference.
While it does performe 2\% better than the global mulitivariate LSTM, this is not enough to say that the autoencoder adds
in any meaningfull way.
When it comes to dataset 2 and 3 where the correlation between datasets is excedingly low, the autoencoder ends up with decreasing the performance of the predictions.
There might be several different factors that play a part in this.

Dataset 1 contains data with high correlation, and thus the autoencoder is more likely to use the additonal data to performe well with encoding and reconstruction.
However, it appears that the autoencoder is not able to improve predictions.
\todo[inline]{Why these reuslts? Are they expected?}


The results are different on datasets 2 and 3 however.
While dataset 1 showed little difference, dataset 2 and 3 performe much worse than the LSTM for a global multivariate model.
This could be explained by the low correlation between the data, which again could result in the autoencoder having to encode a lot more data,
without the performance advantage of analysing data that is shared accorss the time-series.
The autoencoder would therefor have more dificulties in encoding and reconstructing the data,
thus making error prone reconstructions and therefor hurting the performance of the model.
For dataset 2 and 3, these results are as expected for the global multivariate model.



% Has problems with multivariate values, just like the local multivariate
% Sum it up to be the same.
% Has the same advantages as the global univariate, but what were thos?


% Third -> Use of global multivariate
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.202   -  0.198  ->  2%
% Dataset 2 -  0.642   -  0.703  ->  -9.1%
% Dataset 3 -  0.447   -  0.566  ->  -23.5%



\todo[inline]{Why? Does it make sense that we get these results here?}

\todo[inline]{How does the model compare with the basic local univariate hybird model?}







% Additional  ->  High variance vs low-variance datasets

