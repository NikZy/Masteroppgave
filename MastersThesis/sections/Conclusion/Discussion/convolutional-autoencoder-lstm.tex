
\subsection{Convolutional Autoencoder with LSTM}
\label{section:Discussion:Discussion:CNN-AE-LSTM}

% What was the aim of the experiments?
% --> Apply the CNN-AE and LSTM model to a new dataset with real world applicaitons
% --> Verrify the ability of ther hybrid model against the baseline LSTM
% --> Thus verrifying the claims done by "Zhao2019"


% Link this to RQ-5 (CNN-AE and LSTM against the baseline LSTM)
% The thesis is based on "Local univariate" models. We therefor first mention this
% All withing the margin of error
% We are not able to recreate the values by Zhao2019

% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.208   -  0.208 ->  0.0%
% Dataset 2 -  0.684   -  0.716 ->  -4.57%
% Dataset 3 -  0.477   -  0.483 ->  -1.25%

After doing experiments with the LSTM and the CNN-AE and LSTM model on the three datasets used in this thesis,
the results from the predictions are presented in \Cref{section:Results}.
Using the hybrid model, it is clear that we are not able to recreate the values presented in \cite{Zhao2019}
for each of the different datasets.

Experiments conducted on the different datasets result in predictions that can vary vastly,
and some that are quite similar between the models.

The local univariate LSTM and the local univariate convolutional autoencoder and LSTM are one of the model-configurations
that offer rather similar results across datasets.
While the hybrid model CNN-AE-LSTM performance is more or less equal, predictions made on dataset 2 and 3 vary a bit more
with the basic LSTM model performing, on average, 4.6\% and 1.25\% better on average.
However, in addition to these similarities in average performance, the T-test conducted on the experiments reveals the same outcome.
The t-test Resultsults shown in \Cref{table:ttest-p-values-main-experiments-sMAPE} infer that the predictions made by the two local univariate models
are from the same group, and there is no significant difference in their distribution.

The t-test and average metrics attained through the experimentation, therefore, infer that the hybrid model CNN-AE-LSTM
is not able to improve performance over the LSTM model on a local univariate model.


One reason behind this lack of difference in predictions might originate in the selected data.
The task of the convolutional autoencoder is to encode and reconstruct the input data of the model,
removing noise and other unneeded information in the process.
Lack of data could serve as an issue in this case.
As each of the time series used in the local univariate model only consists of around three years of data, or 1300 data points.
There is a possibility that there is not enough similar data for the autoencoder to recognize similarities in data
so that it can then remove the extra noise.
Additionally, the amount of noise contained in the data is not known.
If the datasets contain little noise, the autoencoder is more likely to remove relevant data, thus impairing the performance of the LSTM model.
Due to the implications that the lack of noise in the data potentially could infer from the predictions,
additional experiments are done on data with high noise and low noise.
We will come back to these experiments later in this discussion.

However, as is seen from the predictions made by the two models, it is clear that the convolutional autoencoder and lstm
does not have improved predictions compared to the LSTM model.
We are therefore not able to reproduce the results presented by \cite{Zhao2019}
which showed a significant improvement using the convolutional autoencoder and LSTM over the baseline LSTM model.
% Whether this is due to the used dataset or not is however not entirely certain.


% We have now seen how it works on univariate local models
% We use new models, exploring multivariate and global
% It is not always better in these cases
% How do we explain this?
% Can we compare directly with the work done in Zhao2019? What error metrics were used in these models?
While the thesis \cite{Zhao2019} only focuses on applying local univariate models,
we wished to explore the expansion of the use of the hybrid model with other configurations.
These include the use of multivariate models and global models for each dataset.
Exploring the use of such variations to the Convolutional autoencoder and LSTM,
we hoped to uncover cases where the model performance is increased over the standard local univariate model.







%Unlike the local univariate models described above, the hybrid model performe quite similar, although somewhat worse than the LSTM model
%on both dataset 1 and 2.
%However, dataset 3 offers an improvement of a little over 10\% of the sMAPE error metric by the hybrid model compared to the LSTM.
%
%The results from the global univarite model are somewhat unexpected.
%Dataset 1 and 2 were selected due to their contrast in corrolation between the time series in the dataset.
%Dataset 1 has time-series wich correlate the most with each other, while dataset 2 has time-series with the least ammount of correlating between them.
%Using an convolutional autoencoder and LSTM, it was expected that the additional data made available through the use of a global configuration
%whould be utilized in order to improve the abilities of the autoencoder.
%The autoencoder would have more data to work with in order to improve its encoding and reconstructing of data,
%and considering the data corrolated to some extend, dataset 1 should be easier to work with as much of the data corrolate.
%
%However, as the experiments have shown, both dataset 1 and 2 performes poorly, by beeing a little wors than a LSTM model with the same
%global univariate configuration.
%\todo[inline]{Why? Does it make sense that we get these results here?}
%\todo[inline]{How does the model compare with the basic local univariate hybird model?}





\subsubsection{Local multivariate models}

After the use of global models, a configuration of multivariate models was attempted.
Unlike the global models, the amount of data is increased not through the addition of other time-series to the model,
but by decomposing the information within each time-series.
By decomposing information such as day of the week, or season, the amount of data supplied to the models is increased.


Using the hybrid convolutional autoencoder and LSTM model should result in both improvements and degradation in performance.

Initially, the performance of the model is likely to suffer due to the fact that
the model is required to encode and reconstruct additional input data using the same model structure.
The model might encounter problems recreating the data using the same model as for the univariate model
while attempting to recreate additional data per data point.
If the autoencoder then is limited by the number of data entries on top of this, the model performance could suffer.

However, the same reasoning might also help improve the hybrid model's performance in some situations.
While more data would need to be encoded and reconstructed, the autoencoder will retain more information
regarding the development of trends and spikes through different seasons.
% Den er ikke like grov i kantene
While a pattern might repeat over several seasons, there might be additional hidden information in the seasonal information.
If a pattern is known to never spike during summer but often spikes through the winter,
the autoencoder should be able to differentiate between the different seasons.
Thus, the task of the autoencoder would be to retain information about changes in trends also dependent on seasonal data.


% Second -> Use of local multivariate
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.183   -  0.181  ->  1.1%
% Dataset 2 -  0.603   -  0.743  ->  -23.2%
% Dataset 3 -  0.359   -  0.354  ->  1.4%
However, while this reasoning is based on the autoencoder's ability to encode and differentiate between values based on season,
the model would be severely limited by the amount of data.
If there is not enough data available, the autoencoder would not be able to learn of such connections.
As discussed in \Cref{section:Data:DataExploration}, the data attained from ``Prisguiden.no'' only include data-points
for a little over 3 years.
As with the problems discussed with the local univariate model, the multivariate model would also likely be limited by the amount of data.

This is reflected in the results acquired through testing.
While dataset 1 and 3 implies slight improvements to predictions, there are no clear improvements.
Additionally, the t-test applied to the predictions supports this assumption.
The t-test implies that there are no significant differences between the predictions made with the local multivariate model.


However, the results are quite different for dataset 2.
Considering the sMAPE error metric there is about a 23\% performance decrease using the convolutional autoencoder and LSTM.
This is also supported by the t-test conducted. The results from the t-test imply that the difference between the hybrid model and the LSTM model
is significant within the 95\% window of certainty, supplied by a p-value of under 0.05.
Although this is only the case for dataset 2, the convolutional autoencoder and LSTM perform significantly worse than the LSTM model.
% Why is this?
The significant decrease in performance could be due to the lack of data as discussed earlier.
With the local univariate model, there is a 4\% performance decrease with the hybrid model.
Although this is not significant, this could point to the reason behind the poor performance.
While the local univariate model and the local multivariate models both suffer from a lack of data with each dataset.
Both models perform worse on dataset 2, while the performance on dataset 1 is somewhat better both using univariate and multivariate.
This could imply that the selected dataset is less suited for using the autoencoder.
This could then again imply that the convolutional autoencoder is heavily influenced by the nature of the data that is used.
The implications of the dataset's characteristics are explored further later when the use of noisy data is discussed.
However, if this is the case, the lack of data could explain the poor performance of dataset 2.
The multivariate model would have the same number of time-steps in each time-series but would be required to encode four times the data per time-step.
Therefore, the autoencoder is more likely to perform worse in reconstructing the input data,
thus contributing to a worse performance by the LSTM component.


Although the local multivatiate model is the overall best performing of the convolutional autoencoder and LSTM models,
it is outperformed by the LSTM, indicating that the reason for the improvements by the model
is entirely contributed by the improved LSTM model.


\subsubsection{Global univariate models}
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.203   -  0.210   ->  -3.4 %
% Dataset 2 -  0.662   -  0.675   ->  -1.96%
% Dataset 3 -  0.425   -  0.417   ->  1.9 %


\todo[inline]{---- This section reads a lot like a section from results, with little discussion...}
After having explored different local model configurations,
global models are also tested.
As is discussed in \cref{section:Discussion:Discussion:Global-v-local},
global univariate models generaly outperforme the local models, training on more available data for the neural network.
The same is mostly true for the convolutional autoencoder which performe better with a global univariate configuration compared to a local univariate models.
This is the case for both dataset 2 and dataset 3, where the global univariate models on average performes way better.
The model performe marginaly worse on dataset 1 on average, however, this is more or less neglectable due to the low difference, and no clear significanse either.

However, the performance between the convolutional autoencoder and LSTM, and the baseline LSTM model offers a different perspective.
The global univarite LSTM model generaly performe better than the hybird model.
Evaluating the difference with the t-test, it is also clear that the difference is of significanse.

% Dataset 1 is significantly worse (Can se this fom the individual dataset performances, where it is more or less always worse, and t-test says significanse)
% Dataset 2 is significantly different / worse (LSTM is generaly better, although there are more differences. Sometimes hybrid is better, but mostly, LSTM is best)
% Dataset 3 offers no significant difference. This could be contributed to the low sample size (only 8 time-series)
Although the average metrics performance between the hybrid model and the LSTM model on dataset 1 is only a little under 4\%,
there is still quite a difference.
By evaluating the metrics of each prediction in the dataset, as well as using the significanse test results from the t-test,
it is clear that the global univariate LSTM model is significantly better than the global univariate convolutional autoencoder and LSTM for dataset 1.
For dataset 2 the difference is not as prominant as with dataset 1, but also here there is a significant difference in predictions.
Only with dataset 3, where there hybrid model performe better than the LSTM model on average as a global univariate model,
the t-test signals that there are no significant difference between the two predictions.
It is however unclear if this is due to the dataset or if this has to do with the low sample size in comparison to dataset 1 and dataset 2.
It is clear that with a global univariate configuration the hybrid model is not able to outperforme the LSTM model,
and often performes worse.
\todo[inline]{... End of section-----}


% Sindre sin forklaring på hvorfor Autoencoderen ikke er like god
It seems the benefits of a global model for RNNs are not transferable to other types
of NNs. LSTMs ability to learn across multiple time series can be attributed to the fundamentals
of how the LSTM works. \cite{Zhao2019} discusses previous work that concludes that
the LSTMs memory cell is mainly responsible for the performance of the LSTM.
When training the LSTM on multiple time series, this memory cell is local to each
time-series and resets before each new time series is fed through the network.
The LSTM weights on the other hand are global across all the series.
These local and global characteristics are unique to RNNs, and can explain
why the Convolutional Autoencoder does not see the same benefits when trained on multiple
time series.
In order for the autoencoder to work well on such time series data,
there is a prerequisit that the data is connected and that information from one set can be applied to the others.
If this is not the case, use of global models increasing the ammount of independent data would only serve to decrease the performance of the autoencoder.


% Hva forventer vi egentlig av resultater?
% ------------------------------
%The results from this experiment is not entirely as expected.
%While there was an expectation for dataset 2 and 3 to not attain better performance,
%dataset 1 was expected to have an increase in performance using the global univariate model.
%This is due to the selection of the dataset, containing correlating data that are to some degree linearly related.
%However, while this helps with improving the LSTM model, the autoencoder is not.
%This is likely due to the fact that the corrolation between the dataset, although higher than in dataset 2 and 3,
%is not partiqularly high.
%Additionaly, auto-correlation used to select dataset 1 only referese to a linear correlation,
%not necessarily similary enough data for the autoenocer to be able to encode the trends.
%Therefor, 






\subsubsection{Global Multivariate models}

% Third -> Use of global multivariate
% ....
% Dataset   -   LSTM   -  CNN
% Dataset 1 -  0.202   -  0.198  ->  1.98% -> Not significant
% Dataset 2 -  0.642   -  0.703  ->  -9.5% -> 0.0103 - Significant (both MASE and sMAPE)
% Dataset 3 -  0.399   -  0.566  ->  -41.85% -> 0.026 - Significant (both MASE and sMAPE)

Both multivariate models and global models have been tested with the hybrid convolutional autoencoder and LSTM.
The next step is to apply a model with both of these configurations.
We therefor apply a global multivariate convolutional autoencoder and LSTM, and measure the performance of the model
compared to a global multivatiate LSTM model.

It is clear that the global multivariate model suffer the same problems that are prevelant in both the local multivariate model
and the global univariate model.
Like with the global univariate models, the autoencoder is not as well suited as predicted to encode multiple independent time series.
However, degragation of performance is also influenced by the use of a multivariate model.
The autoencoder need to encode and reconstruct more data per datta-point, and with the same autoencoder structure
as well as lack of data, the autoencoder will likely performe wors than with the use of a univariate model.

These assumptions are reflected in the results aquired from running experiments on the global multivariate models.
While the performance of dataset 1 is more or less equal betwenn the baseline global multivariate LSTM model and the hybird model,
the same is not the case for dataset 2 and dataset 3.
\todo[inline]{---- This section reads a lot like a section from results, with little discussion...}
On dataset 1 the hybrid model performe on average 2\% better than the LSTM model, however this is not a significant difference in itself.
This is also supported by the results from the t-test which implies there is no significanse in the difference in predictions.
While there were a significant difference between the models using a global univariate model, the difference in performance was not high.
However, using the local multivariate model, the hybrid model performed somewhat better than the LSTM model for dataset 1.
It appears that the same applies for the global mutlivariate model, although it has not improved the performance of the hybrid model significantly
it appears that the use of a multivariate model has counteracted the negative effects of the global model on dataset 1.

Unfortunatly, the same does not apply for dataset 2 and dataset 3.
While the hybrid model on average performe about 9.5\% worse on dataset 2, it also performes about 41.9\% worse on dataset 3.
Applying the student t-test to the predictions aslo support the notion of the global multivariate model decreasing perfromance for the
convolutional autoencoder and LSTM.
Both predictions for dataset 2 and dataset 3 are well withing the confidence interval of the t-test,
thus implying a significant difference between the predictions for datasets 2 and 3.
\todo[inline]{... End of section ---}

Performance degregation for dataset 2 were expected due to the poor results both with the global univariate model and the local multivate model.
Both of these configurations resulted in worse predictions on dataset 2.
Dataset 3 on the other hand, while expected, is not as easily explained.
While the performance was not partiqularly good on either of the previous models it would appear that the combination of the multivariate model and global model
caused degregation in performance.
However, the vast difference in performance could also be attributed to the low sample size of the dataset.
While this is not the only reason, it might be a contibuting factor to the large difference.

Either way, it is clear that the convolutional autoencoder is not well suited for use with global multivariate models, as it appears to
vastly decrease the performance of the predictions.




%%% --- OLD ---
\iffalse
  Expanding on the use of global models and multivariate models,
  a global multivariate hybrid model is defined and tested.

  Even though the model is a global method; it does not escape the problems retained with a multivariate model.
  Although the number of datasets has increased, this does not necessarily improve the performance of the models.
  The reason behind this is the lack of correlation between the time-series in the datasets.
  Although dataset 1 contains datasets with higher correlation than dataset 2 and 3, it appears this is not enough to make any significant difference.
  While it does perform 2\% better than the global multivariate LSTM, this is not enough to say that the autoencoder adds
  in any meaningful way.
  When it comes to dataset 2 and 3 where the correlation between datasets is exceedingly low, the autoencoder ends up with decreasing the performance of the predictions.
  There might be several different factors that play a part in this.

  Dataset 1 contains data with high correlation, and thus the autoencoder is more likely to use the additional data to perform well with encoding and reconstruction.
  However, it appears that the autoencoder is not able to improve predictions.
  \todo[inline]{Why these reuslts? Are they expected?}


  The results are different on datasets 2 and 3, however.
  While dataset 1 showed little difference, datasets 2 and 3 performed much worse than the LSTM for a global multivariate model.
  This could be explained by the low correlation between the data, which again could result in the autoencoder having to encode a lot more data,
  without the performance advantage of analyzing data that is shared across the time-series.
  The autoencoder would therefore have more difficulties in encoding and reconstructing the data,
  thus making error-prone reconstructions and therefore hurting the performance of the model.
  For datasets 2 and 3, these results are as expected for the global multivariate model.
\fi








% High noise and low noise and low noise data
\subsubsection{CNN-AE-LSTM on high noise datasets}

As we discussed when takling about the performance of the convolutional autoencoder and LSTM on datasets 1, 2 and 3,
we made the assumption that the characteristics of the used dataset could have severe implications in the predictive accuracy of the hybird model.

In order to test this, additional experiments were defined and ran as described in \Cref*{section:Results:AdditionalExperimentalPlan}.
By applying these experiments using the hybird model to 3 new dataset with varying degree of data noise,
the assumption was that the degree of noise in the dataset would heavily influence the predictive abilities of the hybrid model.

Due to the design of the hybrid model, applying a convolutional autoencoder to the input data of the model,
the data would be altered in accordance to the autoencoder.
The assumption is that when applying the autoencoder to a dataset with a low ammount of noise,
the autoencoder would be more likely to remove important information on which the LSTM part of the model is dependent on.
On the other hand, with a higher level of noise in the dataset,
the model is more likely to reduce the noisy values, contributing to input data which is easier for the LSTM model to interpret.

% What were the results from the predictions?

% What does this have to say for the usability of the model on different data

% Does this explain the results from any of the experiments above?

% Are there other implications from these results?










%%% --- OLD ---
\iffalse
The results of Experiment 4 [\Cref{section:results:additional-experimental-plan:Experiment-4}],
and Experiment 1 shows a small increase in performance for the hybrid model on high variance datasets.
Even though these results are not below the threshold, they do show a trend given that the sample sizes were relatively small
(20 samples, and 7 samples), which might be too small to conclude when the
the performance increase is low.

However, from the results of Experiment 4, we can empirically conclude that the CNN-AE-LSTM will
hurt the performance if the datasets contain small amounts of noise. This downside is much
worse than the potential performance gains on noisy datasets.
This can explain the poor results on dataset 2. Since dataset 2 contains a wide variety
of time series.
\fi

