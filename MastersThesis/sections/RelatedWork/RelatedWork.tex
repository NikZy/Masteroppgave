\chapter{Related work}
\label{section:RelatedWork}

In order to find a fitting solution to the goal proposed in \Cref{section:Introduction:Goal} and its related research questions,
we need to assess what advances have been accomplished within the problem space.
We assess state-of-the-art methods in time-series prediction and loss functions.
With this, we are able to compare current solutions and methodology in order to answer our research questions.

This chapter starts with \Cref{section:RelatedWork:forecasting-ecommerce} introducing current and previous advances within e-commerce time-series forecasting.
\Cref{section:RelatedWork:Statistical-NN} explores the use of deep learning method in comparison with statistical methods.
Later, \Cref{section:RelatedWork:Model-structure} introduces the use of global and local methods, as well as univariate and multivariate methods.
\Cref{section:RelatedWork:Hybrid} introduces current state-of-the-art hybrid methods for forecasting.
Lastly, \Cref{section:RelatedWork:Loss} discuss relevant loss functions and their usecases.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% Related work sections %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\import{./sections/RelatedWork/}{Financial-forecasting.tex}

\import{./sections/RelatedWork/}{statistical-vs-nn.tex}
\import{./sections/RelatedWork/}{Model-Structure.tex}

\import{./sections/RelatedWork/}{Hybrid-Framework.tex}

\import{./sections/RelatedWork/}{rl-loss-functions.tex}
\import{./sections/RelatedWork/}{Multi-step-ahead.tex}

\section*{Criticue}
The book \cite{Bharadi2021} describes the decoupling of resetting hidden
states during training of LSTM, and describes the issue of
needing a fixed batch size. We found countless of internet forum posts
and some articles that touched on this issue, few good solutions
to the problem were found.

The litarature seems to give few answers for how they handle hidden state
resets, if their models are stateful or stateless. And if stateful,
and how they handle the fixed batch size problem.
%When working with RNNs and long sequences of time series it is important
%to be transparent with how resetting of hidden states are done.

\cite{Bandara2017} has the following sentences
\begin{quotation}
  "All training patches relevant to a particular time series are read as one
  sequence. Therefore, the LSTM state needs to be initialized for each
  series".

\end{quotation}
This indicates that they are using a stateful LSTM [\Cref{section:BT:stateful-vs-stateless}].
They are training global models accross multiple time series, but they do not
describe how or when they are resetting the hidden states.
If they pass through historic relevant data before predicting
the test set, or how they handle the equal batch size constraint.

\cite{Hewamalage2021} describes in great detail the different architectures used,
but they do not explisitly reveal how they handle initializing and reseting
of hidden states.

We found one papers wich explisitly names reseting hidden states
as a tool \cite{Smyl2020}, but explenation of how this was done.


