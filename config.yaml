data:
  data_path: './datasets/raw/market_insights_overview_all_2022_04_26.csv'
  # data_path: './datasets/raw/market_insights_overview_all_2021_11_12.csv'
  # data_path: './datasets/raw/market_insights_overview_5p_full.csv'
  #categories_path: './datasets/raw/solr_categories_all_16_09_2021.csv'
  categories_path: './datasets/raw/solr_categories_all_2022_02_14.csv'

logger:
  log_level: 'INFO'
  log_file: './log-file.log' # Currently does nothing

use_gpu_if_available: false # Some experience indicate that GPU is slower

experiment:
  tags:
    - market_insights
  save_sources_to_use: # Which sources the experiment should be saved to
    - 'disk'
    #- 'neptune'

  checkpoint_save_location: './models/0_current_model_checkpoints/'
  log_model_every_n_epoch: 10

  # Possible metrics: MSE, MAE, MSE; MAE
  error_metrics:
    - 'MAE'
    - 'MASE'
    - 'MSE'
    - 'SMAPE'
    - 'MAPE'
    #- 'MASE_Periodic'

  save_source:
    disk:
      model_save_location: './models/'

    neptune:
      project_id: 'sjsivertandsanderkk/Masteroppgave'
      # Set api token with env variable NEPTUNE_API_TOKEN
      #api_token: ${NEPTUNE_API_TOKEN}

model:
  # Model types: 'validation_model'. 'local_univariate_arima', 'local_univariate_lstm', 'local_cnn_ae', 'local_cnn_ae_lstm'
  # global
  model_type: 'local_univariate_lstm'
  rng_seed: 42
  validation_model:
    placeholder: 0

  univariate_lstm:
    hyperparameter_tuning_range:
      hidden_size: [2, 100]
      number_of_layers: [1, 4]
      dropout: [0.0, 0.4]
      recurrent_dropout: [0.0, 0.4]
      #optimizer_name: ['Adam', 'RMSprop']
      optimizer_name: ['RMSprop', 'Adam']
      #optimizer_name: ['Adam']
      learning_rate: [1e-7, 1e-2]
      number_of_epochs: [5, 40]
      batch_size: [32, 32]
      input_window_size: 10
      output_window_size: 7 # must be equal output_window_size
      multi_variable_nr: 1 # must be equal to number of variables used in multi variable (1 if uni variate)
      number_of_features: 1 # must be equal to number of features in data
      number_of_trials: 1 # Number of tuning trials to run. The more the better.
      #time_to_tune_in_minutes: 6000  # Time to tune in minutes, If both number of trials or time to tune is set, first one to finnish.
      stateful_lstm: true

    global_model:
      model_structure:
        # If int <= 1 then the testing set will be equal to output_window_size
        input_window_size: 10
        output_window_size: 7
        multi_variable_nr: 4
        batch_size: 32
        number_of_epochs: 22
        stateful_lstm: yes
        should_shuffle_batches: no
        optimizer_name: Adam
        learning_rate: 0.00043177648728211254
        number_of_features: 1
        layers:
            - hidden_size: 94
              dropout: 0.21199
              recurrent_dropout: 0.0
      datasets:
        - 11573
        - 11037
        # - 12322
        # - 11428
        # - 11850
        # - 11852
        # - 273
        # - 11036
        # - 11213
        # - 2
        # - 6
        # - 10
        # - 11
        # - 13
        # - 20
        # - 22
        # - 24
        # - 26
        # - 27
        # - 28
        # - 29
        # - 32
        # - 33
        # - 34
        # - 39
        # - 41
        # - 51
        # - 54


    local_model:
      common_parameters_for_all_models:
        # If int <= 1 then the testing set will be equal to output_window_size
        #training_size: 50 # Number of samles in validation set
        input_window_size: 10
        output_window_size: 7
        multi_variable_nr: 1
        batch_size: 32
        stateful_lstm: yes
        should_shuffle_batches: no
        number_of_features: 1

      model_structure:
        # - time_series_id: 2
        - time_series_id: 11573
          number_of_epochs: 30 
          optimizer_name: 'RMSprop'
          learning_rate: 0.0016330596392901727 
          #number_of_layers: 1
          layers:
            - hidden_size: 23 
              dropout: 0.016406595527129978 
              recurrent_dropout: 0.0
        # - time_series_id: 6
        #   learning_rate: 0.0005122276184287306
        #   number_of_epochs: 25
        #   optimizer_name: 'RMSprop'
        #   learning_rate: 0.00013756
        #   #number_of_layers: 1
        #   layers:
        #     - recurrent_dropout: 0.0
        #       dropout: 0.08613713284048026
        #       hidden_size: 51
        # - time_series_id: 10
        #   learning_rate: 0.00983984268591986
        #   number_of_epochs: 11
        #   optimizer_name: 'RMSprop'
        #   #number_of_layers: 1
        #   layers:
        #     - dropout: 0.39992188220583275
        #       hidden_size: 7
        #       recurrent_dropout: 0.0
        #     - dropout: 0.39992188220583275
        #       hidden_size: 7
        #       recurrent_dropout: 0.0
        #     - dropout: 0.39992188220583275
        #       hidden_size: 7
        #       recurrent_dropout: 0.0
        #     - dropout: 0.39992188220583275
        #       hidden_size: 7
        #       recurrent_dropout: 0.0
        #     - dropout: 0.39992188220583275
        #       hidden_size: 7
        #       recurrent_dropout: 0.0
        # - time_series_id: 11
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 13
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 20
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 22
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 24
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 26
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 27
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 28
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 29
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 32
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 33
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 34
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 39
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 41
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 51
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0
        # - time_series_id: 54
        #   learning_rate: 0.00013756
        #   number_of_features: 1
        #   #number_of_layers: 1
        #   layers:
        #     - hidden_size: 94
        #       dropout: 0.21199
        #       recurrent_dropout: 0.0

#        - time_series_id: 33
#          learning_rate: 0.00013756
#          hidden_layer_size: 94
#          dropout: 0.21199
#          number_of_features: 1
#          number_of_layers: 1
#        - time_series_id: 11091
#          learning_rate: 0.00013756
#          hidden_layer_size: 94
#          dropout: 0.21199
#          number_of_features: 1
#        - time_series_id: 11573
#          learning_rate: 0.00013756
#          hidden_layer_size: 94
#          dropout: 0.21199
#          number_of_features: 1
  local_univariate_cnn_ae:
    hyperparameter_tuning_range:
      placeholder: 4
    common_parameters_for_all_models:
      training_size: 50
      input_window_size: 10
      output_window_size: 7
      multi_variable_nr: 1
      batch_size: 32
      number_of_epochs: 100
      optimizer_name: 'Adam'
      loss: "mean_absolute_percentage_error"
      should_shuffle_batches: True

    model_structure:
      #- time_series_id: 11573
      - time_series_id: 12322
        learning_rate: 0.0003
        encoder:
          - layer: "Conv1d"
            filters: 64
            kernel_size: 3
            activation: "relu"
          - layer: "Conv1d"
            filters: 128
            kernel_size: 5
            activation: "relu"
        decoder:
          - layer: "Conv1dTranspose"
            kernel_size: 5
            filters: 64
            activation: "relu"
          - layer: "Conv1dTranspose"
            kernel_size: 3
            filters: 1
            activation: "relu"

  local_univariate_cnn_ae_lstm:
    hyperparameter_tuning_range:
      hidden_size: [ 2, 100 ]
      number_of_layers: [ 1, 4 ]
      dropout: [ 0.0, 0.4 ]
      #optimizer_name: ['Adam', 'RMSprop']
      optimizer_name: [ 'RMSprop', 'Adam' ]
      #optimizer_name: ['Adam']
      learning_rate: [ 1e-7, 1e-2 ]
      number_of_epochs: [ 5, 40 ]
      batch_size: [ 32, 32 ]
      input_window_size: 10
      output_window_size: 7 # must be equal output_window_size
      multi_variable_nr: 1 # must be equal to number of variables used in multi variable (1 if uni variate)
      number_of_features: 1 # must be equal to number of features in data
      number_of_trials: 200 # Number of tuning trials to run. The more the better.
      #time_to_tune_in_minutes: 6000  # Time to tune in minutes, If both number of trials or time to tune is set, first one to finnish.
      stateful_lstm: true
    common_parameters_for_all_models:
      should_shuffle_batches: True
      training_size: 50
      optimizer_name: 'Adam'
      loss: 'mae'
      batch_size: 32
      epochs: 20
      number_of_epochs: 20
      input_window_size: 10
      output_window_size: 7
      multi_variable_nr: 1
      lstm-shared:
        input_window_size: 10
        output_window_size: 7
        multi_variable_nr: 1
        epochs: 9

    model_structure:
      - time_series_id: 11573
      #- time_series_id: 12322
        lstm:
          optimizer_name: 'RMSprop'
          stateful_lstm: True
          loss: 'mae'
          learning_rate: 7.88e-05
          hidden_layer_size: 14
          dropout: 0.132
          number_of_features: 1
          number_of_layers: 1
        ae:
          optimizer_name: 'Adam'
          loss: "mean_absolute_percentage_error"
          learning_rate: 0.0003
          epochs: 100
        encoder:
          - layer: "Conv1d"
            filters: 64
            kernel_size: 3
            activation: "relu"
          - layer: "Conv1d"
            filters: 128
            kernel_size: 5
            activation: "relu"
        decoder:
          - layer: "Conv1dTranspose"
            kernel_size: 5
            filters: 64
            activation: "relu"
          - layer: "Conv1dTranspose"
            kernel_size: 3
            filters: 1
            activation: "relu"


  local_univariate_arima:
    forecast_window_size: 7
    steps_to_predict: 7
    multi_step_forecast: true # alternative is recursive single step
    auto_arima: true
    seasonal: true
    # Ranges used for autotuning if --tune parameter is set
    hyperparameter_tuning_range:
      p: [1, 3]
      d: [1, 3]
      q: [1, 3]
      P: [0, 3]
      D: [0, 3]
      Q: [0, 3]
      s: [12, 12]

    # metric_to_use_when_tuning: 'MASE'
    metric_to_use_when_tuning: 'MAE'

    model_structure:
      - time_series_id: 12322
        hyperparameters:
          p: 0
          d: 1
          q: 0
          P: 5
          D: 1
          Q: 0
          s: 4
      - time_series_id: 11428
        hyperparameters:
          p: 2
          d: 1
          q: 0
          P: 5
          D: 1
          Q: 0
          s: 4
      - time_series_id: 11850
        hyperparameters:
          p: 0
          d: 1
          q: 0
          P: 4
          D: 1
          Q: 0
          s: 4
      - time_series_id: 11852
        hyperparameters:
          p: 0
          d: 1
          q: 0
          P: 5
          D: 1
          Q: 1
          s: 4
      - time_series_id: 273
        hyperparameters:
          p: 2
          d: 1
          q: 0
          P: 5
          D: 1
          Q: 0
          s: 4
      - time_series_id: 11036
        hyperparameters:
          p: 0
          d: 1
          q: 0
          P: 5
          D: 1
          Q: 0
          s: 4
      - time_series_id: 11213
        hyperparameters:
          p: 0
          d: 1
          q: 1
          P: 5
          D: 1
          Q: 0
          s: 4
