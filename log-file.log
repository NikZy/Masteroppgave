Logger initialized. Log level: INFO, log file: ./log-file.log
Started
Starting experiment: "lstm-tune-simple-dataset": "lstm-tune-simple-dataset"
Wiping and initializing checkpoint save location models/0_current_model_checkpoints
Creating model save location models/lstm-tune-simple-dataset
Creating new Neptune experiment
Neptune run URL: https://app.neptune.ai/sjsivertandsanderkk/Masteroppgave/e/MAS-331
Running tuning experiment with saving set to True
Choosing model structure: <src.model_strutures.local_univariate_lstm_structure.LocalUnivariateLstmStructure object at 0x150155cc44f0>
Running model on device: cuda
Initializing LSTM with 3 layers, 30 hidden size, 0.03 learning rate, 0.113 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning
data preprocessing steps: 
 ---- Start ----
1- load market insight data and categories and merge them
2- convert date columns to date_time format
3- sum up clicks to category level [groupBy(date, cat_id)]
4- rename column 'title' to 'cat_name'
5- combine feature 'hits' and 'clicks' to new feature 'interest'
6- drop columns 'hits' and 'clicks'
7- filter out data from early 2018-12-01
8- drop uninteresting colums: ['id_x', 'manufacturer_id', '_version_', 'id_y', 'internal_doc_id_x', 'internal_doc_id_y']
---- End ----
        

------------------
Starting new timer at 16:31:32
Data Pipeline for 11573: ---- Start ----
1- load data
2- choose columns
3- Scale data, expand dims, split, train test, val
---- End ----
        
Converting dataset to dataloader using batch size 100.
Ending timer at: 16:31:33 
Elapsed CPU time: 0.11201828333350743s
Used real time: 0:00:01
------------------
Tuning model: 11573
Loading or creating optuna study with name: lstm-tune-simple-dataset_11573
Number of previous Trials with this name are #2
Starting tuning trial number #2 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.005182953394862478, 'batch_first': True, 'batch_size': 164, 'dropout': 0.33951413567458993, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 97}
Converting dataset to dataloader using batch size 164.
Initializing LSTM with 15 layers, 42 hidden size, 0.005182953394862478 learning rate, 0.33951413567458993 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:31:34
Ending timer at: 16:35:28 
Elapsed CPU time: 1935.0173536833333s
Used real time: 0:03:54
------------------
Starting tuning trial number #3 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 97, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.012564823301322229, 'batch_first': True, 'batch_size': 254, 'dropout': 0.036252957504288974, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 22}
Converting dataset to dataloader using batch size 254.
Initializing LSTM with 14 layers, 97 hidden size, 0.012564823301322229 learning rate, 0.036252957504288974 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:35:28
Ending timer at: 16:36:30 
Elapsed CPU time: 2038.1915697666666s
Used real time: 0:01:02
------------------
Starting tuning trial number #4 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 301, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.00047305546195233663, 'batch_first': True, 'batch_size': 195, 'dropout': 0.14825155136476126, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 195.
Initializing LSTM with 14 layers, 301 hidden size, 0.00047305546195233663 learning rate, 0.14825155136476126 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:36:31
Ending timer at: 16:39:00 
Elapsed CPU time: 1884.0972932166674s
Used real time: 0:02:29
------------------
Starting tuning trial number #5 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 275, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 2.362358659234183e-06, 'batch_first': True, 'batch_size': 348, 'dropout': 0.02284348477323067, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 348.
Initializing LSTM with 25 layers, 275 hidden size, 2.362358659234183e-06 learning rate, 0.02284348477323067 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:39:01
Ending timer at: 16:42:02 
Elapsed CPU time: 2866.3475759500006s
Used real time: 0:03:01
------------------
Starting tuning trial number #6 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 300, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.0003575865549150975, 'batch_first': True, 'batch_size': 231, 'dropout': 0.08669963920906881, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 231.
Initializing LSTM with 10 layers, 300 hidden size, 0.0003575865549150975 learning rate, 0.08669963920906881 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:42:03
Ending timer at: 16:42:53 
Elapsed CPU time: 949.8047330166665s
Used real time: 0:00:50
------------------
Starting tuning trial number #7 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 394, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 12, 'learning_rate': 0.005030453786349151, 'batch_first': True, 'batch_size': 97, 'dropout': 0.229564270471098, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 16}
Converting dataset to dataloader using batch size 97.
Initializing LSTM with 12 layers, 394 hidden size, 0.005030453786349151 learning rate, 0.229564270471098 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:42:53
Ending timer at: 16:43:55 
Elapsed CPU time: 2950.849843366666s
Used real time: 0:01:02
------------------
Starting tuning trial number #8 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 356, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.0807482144094446, 'batch_first': True, 'batch_size': 274, 'dropout': 0.25943619697833953, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 274.
Initializing LSTM with 7 layers, 356 hidden size, 0.0807482144094446 learning rate, 0.25943619697833953 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:43:55
Starting tuning trial number #9 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 73, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 12, 'learning_rate': 0.00030356692319595074, 'batch_first': True, 'batch_size': 86, 'dropout': 0.24257380127243705, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 86.
Initializing LSTM with 12 layers, 73 hidden size, 0.00030356692319595074 learning rate, 0.24257380127243705 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:43:57
Starting tuning trial number #10 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 109, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 0.004719020583293797, 'batch_first': True, 'batch_size': 167, 'dropout': 0.09225641309813282, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 167.
Initializing LSTM with 23 layers, 109 hidden size, 0.004719020583293797 learning rate, 0.09225641309813282 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:01
Starting tuning trial number #11 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 165, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 3.698925384132881e-05, 'batch_first': True, 'batch_size': 30, 'dropout': 0.22752317431756575, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 30.
Initializing LSTM with 11 layers, 165 hidden size, 3.698925384132881e-05 learning rate, 0.22752317431756575 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:02
Starting tuning trial number #12 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 218, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 1, 'learning_rate': 1.5207393309950723e-06, 'batch_first': True, 'batch_size': 359, 'dropout': 0.13967197192565606, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 359.
Initializing LSTM with 1 layers, 218 hidden size, 1.5207393309950723e-06 learning rate, 0.13967197192565606 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:04
Starting tuning trial number #13 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 258, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 2.2498070590087316e-06, 'batch_first': True, 'batch_size': 340, 'dropout': 0.031171144524019142, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 340.
Initializing LSTM with 28 layers, 258 hidden size, 2.2498070590087316e-06 learning rate, 0.031171144524019142 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:06
Starting tuning trial number #14 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 317, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 2.2737407817035143e-05, 'batch_first': True, 'batch_size': 302, 'dropout': 0.4904508547140246, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 302.
Initializing LSTM with 21 layers, 317 hidden size, 2.2737407817035143e-05 learning rate, 0.4904508547140246 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:08
Starting tuning trial number #15 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 274, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.00030110864240918267, 'batch_first': True, 'batch_size': 208, 'dropout': 0.1584038714510075, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 45}
Converting dataset to dataloader using batch size 208.
Initializing LSTM with 19 layers, 274 hidden size, 0.00030110864240918267 learning rate, 0.1584038714510075 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:10
Starting tuning trial number #16 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 206, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 30, 'learning_rate': 1.617354577861004e-05, 'batch_first': True, 'batch_size': 118, 'dropout': 0.0009640541101518553, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 118.
Initializing LSTM with 30 layers, 206 hidden size, 1.617354577861004e-05 learning rate, 0.0009640541101518553 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:11
Starting tuning trial number #17 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 349, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.00010342185914820923, 'batch_first': True, 'batch_size': 308, 'dropout': 0.14198758304459155, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 308.
Initializing LSTM with 25 layers, 349 hidden size, 0.00010342185914820923 learning rate, 0.14198758304459155 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:17
Starting tuning trial number #18 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 156, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 4.8863788530175156e-06, 'batch_first': True, 'batch_size': 27, 'dropout': 0.3535090216281943, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 34}
Converting dataset to dataloader using batch size 27.
Initializing LSTM with 18 layers, 156 hidden size, 4.8863788530175156e-06 learning rate, 0.3535090216281943 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:20
Starting tuning trial number #19 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 245, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0006406305847764134, 'batch_first': True, 'batch_size': 137, 'dropout': 0.06453446103259855, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 137.
Initializing LSTM with 6 layers, 245 hidden size, 0.0006406305847764134 learning rate, 0.06453446103259855 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:22
Starting tuning trial number #20 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 312, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.001112101196100375, 'batch_first': True, 'batch_size': 212, 'dropout': 0.17573074963990643, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 6}
Converting dataset to dataloader using batch size 212.
Initializing LSTM with 25 layers, 312 hidden size, 0.001112101196100375 learning rate, 0.17573074963990643 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:24
Starting tuning trial number #21 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 168, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 5.6354036988916666e-05, 'batch_first': True, 'batch_size': 59, 'dropout': 0.10716529953392034, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 59.
Initializing LSTM with 17 layers, 168 hidden size, 5.6354036988916666e-05 learning rate, 0.10716529953392034 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:26
Starting tuning trial number #22 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 391, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 9.99606733140883e-06, 'batch_first': True, 'batch_size': 298, 'dropout': 0.3118412386617071, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 85}
Converting dataset to dataloader using batch size 298.
Initializing LSTM with 21 layers, 391 hidden size, 9.99606733140883e-06 learning rate, 0.3118412386617071 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:28
Starting tuning trial number #23 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 112, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.06455942273588687, 'batch_first': True, 'batch_size': 259, 'dropout': 0.030382520028734206, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 32}
Converting dataset to dataloader using batch size 259.
Initializing LSTM with 15 layers, 112 hidden size, 0.06455942273588687 learning rate, 0.030382520028734206 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:30
Starting tuning trial number #24 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 20, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.020236913313011314, 'batch_first': True, 'batch_size': 246, 'dropout': 0.004180964762761384, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 246.
Initializing LSTM with 15 layers, 20 hidden size, 0.020236913313011314 learning rate, 0.004180964762761384 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:31
Starting tuning trial number #25 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 239, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.0018605878422271386, 'batch_first': True, 'batch_size': 196, 'dropout': 0.04770879754805189, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 196.
Initializing LSTM with 8 layers, 239 hidden size, 0.0018605878422271386 learning rate, 0.04770879754805189 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:44:33
Ending timer at: 16:47:14 
Elapsed CPU time: 1948.095764266666s
Used real time: 0:02:41
------------------
Starting tuning trial number #26 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 275, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0017811566946591146, 'batch_first': True, 'batch_size': 189, 'dropout': 0.1919897062989171, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 189.
Initializing LSTM with 6 layers, 275 hidden size, 0.0017811566946591146 learning rate, 0.1919897062989171 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:47:15
Starting tuning trial number #27 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 239, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 0.00013454586048584089, 'batch_first': True, 'batch_size': 150, 'dropout': 0.10882523575978856, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 150.
Initializing LSTM with 2 layers, 239 hidden size, 0.00013454586048584089 learning rate, 0.10882523575978856 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:47:17
Starting tuning trial number #28 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 344, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.0017148428373163379, 'batch_first': True, 'batch_size': 195, 'dropout': 0.06865245878731094, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 195.
Initializing LSTM with 9 layers, 344 hidden size, 0.0017148428373163379 learning rate, 0.06865245878731094 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:47:18
Starting tuning trial number #29 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 291, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.00012938560163319903, 'batch_first': True, 'batch_size': 322, 'dropout': 0.05872929017729407, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 322.
Initializing LSTM with 5 layers, 291 hidden size, 0.00012938560163319903 learning rate, 0.05872929017729407 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:47:20
Starting tuning trial number #30 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 225, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.0026248410587277086, 'batch_first': True, 'batch_size': 278, 'dropout': 0.13238735761976914, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 278.
Initializing LSTM with 8 layers, 225 hidden size, 0.0026248410587277086 learning rate, 0.13238735761976914 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:49:22
Ending timer at: 16:52:50 
Elapsed CPU time: 2532.2608236499987s
Used real time: 0:03:28
------------------
Starting tuning trial number #31 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 196, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.012904588597068258, 'batch_first': True, 'batch_size': 171, 'dropout': 0.127403883308081, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 171.
Initializing LSTM with 4 layers, 196 hidden size, 0.012904588597068258 learning rate, 0.127403883308081 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:52:51
Starting tuning trial number #32 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 223, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.0028985974902640895, 'batch_first': True, 'batch_size': 218, 'dropout': 0.1925103933574349, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 218.
Initializing LSTM with 8 layers, 223 hidden size, 0.0028985974902640895 learning rate, 0.1925103933574349 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:52:53
Ending timer at: 16:55:56 
Elapsed CPU time: 1986.2167591499997s
Used real time: 0:03:03
------------------
Starting tuning trial number #33 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 228, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.002855899729839475, 'batch_first': True, 'batch_size': 222, 'dropout': 0.1863303553225135, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 222.
Initializing LSTM with 8 layers, 228 hidden size, 0.002855899729839475 learning rate, 0.1863303553225135 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:00
Starting tuning trial number #34 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 183, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.0007326847480966524, 'batch_first': True, 'batch_size': 275, 'dropout': 0.29597863253957846, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 38}
Converting dataset to dataloader using batch size 275.
Initializing LSTM with 4 layers, 183 hidden size, 0.0007326847480966524 learning rate, 0.29597863253957846 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:01
Starting tuning trial number #35 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 134, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.008721646024197005, 'batch_first': True, 'batch_size': 233, 'dropout': 0.2042633853394591, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 233.
Initializing LSTM with 9 layers, 134 hidden size, 0.008721646024197005 learning rate, 0.2042633853394591 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:03
Starting tuning trial number #36 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 243, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.00241003156222233, 'batch_first': True, 'batch_size': 187, 'dropout': 0.15067232695845714, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 187.
Initializing LSTM with 13 layers, 243 hidden size, 0.00241003156222233 learning rate, 0.15067232695845714 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:05
Starting tuning trial number #37 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 217, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.000653685237655255, 'batch_first': True, 'batch_size': 269, 'dropout': 0.3994283578939264, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 269.
Initializing LSTM with 9 layers, 217 hidden size, 0.000653685237655255 learning rate, 0.3994283578939264 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:07
Starting tuning trial number #38 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 264, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.02604476854187383, 'batch_first': True, 'batch_size': 238, 'dropout': 0.11489684565630923, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 238.
Initializing LSTM with 11 layers, 264 hidden size, 0.02604476854187383 learning rate, 0.11489684565630923 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:08
Starting tuning trial number #39 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 326, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.004996285704124702, 'batch_first': True, 'batch_size': 146, 'dropout': 0.22269472414324298, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 146.
Initializing LSTM with 13 layers, 326 hidden size, 0.004996285704124702 learning rate, 0.22269472414324298 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:56:10
Ending timer at: 16:58:08 
Elapsed CPU time: 2509.1073659000017s
Used real time: 0:01:58
------------------
Starting tuning trial number #40 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 283, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 0.0002652136507182643, 'batch_first': True, 'batch_size': 206, 'dropout': 0.2746207750243218, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 206.
Initializing LSTM with 3 layers, 283 hidden size, 0.0002652136507182643 learning rate, 0.2746207750243218 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:58:08
Starting tuning trial number #41 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 188, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.008223645995137316, 'batch_first': True, 'batch_size': 287, 'dropout': 0.07145938526965785, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 27}
Converting dataset to dataloader using batch size 287.
Initializing LSTM with 10 layers, 188 hidden size, 0.008223645995137316 learning rate, 0.07145938526965785 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:58:10
Starting tuning trial number #42 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 299, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.0011239309145681785, 'batch_first': True, 'batch_size': 171, 'dropout': 0.17138585922316754, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 171.
Initializing LSTM with 7 layers, 299 hidden size, 0.0011239309145681785 learning rate, 0.17138585922316754 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:58:12
Ending timer at: 17:00:35 
Elapsed CPU time: 2187.250077449998s
Used real time: 0:02:23
------------------
Starting tuning trial number #43 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 300, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.001135246475029843, 'batch_first': True, 'batch_size': 119, 'dropout': 0.16160664860140023, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 8 layers, 300 hidden size, 0.001135246475029843 learning rate, 0.16160664860140023 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:35
Starting tuning trial number #44 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 256, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.003128424283688138, 'batch_first': True, 'batch_size': 167, 'dropout': 0.21126446733272986, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 167.
Initializing LSTM with 7 layers, 256 hidden size, 0.003128424283688138 learning rate, 0.21126446733272986 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:38
Starting tuning trial number #45 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 216, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.0004533299250181449, 'batch_first': True, 'batch_size': 181, 'dropout': 0.09292073700189189, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 181.
Initializing LSTM with 11 layers, 216 hidden size, 0.0004533299250181449 learning rate, 0.09292073700189189 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:40
Starting tuning trial number #46 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 334, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0013093956672750805, 'batch_first': True, 'batch_size': 225, 'dropout': 0.1746769063552795, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 225.
Initializing LSTM with 6 layers, 334 hidden size, 0.0013093956672750805 learning rate, 0.1746769063552795 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:41
Starting tuning trial number #47 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 296, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.003769057561890306, 'batch_first': True, 'batch_size': 255, 'dropout': 0.24730921527888422, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 255.
Initializing LSTM with 13 layers, 296 hidden size, 0.003769057561890306 learning rate, 0.24730921527888422 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:43
Starting tuning trial number #48 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 366, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 1, 'learning_rate': 0.008264618875274755, 'batch_first': True, 'batch_size': 155, 'dropout': 0.1281602634423938, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 155.
Initializing LSTM with 1 layers, 366 hidden size, 0.008264618875274755 learning rate, 0.1281602634423938 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:45
Starting tuning trial number #49 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 230, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.00020650134462992092, 'batch_first': True, 'batch_size': 125, 'dropout': 0.08520023786025681, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 125.
Initializing LSTM with 7 layers, 230 hidden size, 0.00020650134462992092 learning rate, 0.08520023786025681 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:46
Starting tuning trial number #50 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 375, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.0007493079891092104, 'batch_first': True, 'batch_size': 105, 'dropout': 0.15077616952915274, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 105.
Initializing LSTM with 17 layers, 375 hidden size, 0.0007493079891092104 learning rate, 0.15077616952915274 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:48
Starting tuning trial number #51 of total 50
with params: {'number_of_features': 1, 'hidden_layer_size': 259, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.0003789967631199365, 'batch_first': True, 'batch_size': 212, 'dropout': 0.26410118094815943, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 212.
Initializing LSTM with 10 layers, 259 hidden size, 0.0003789967631199365 learning rate, 0.26410118094815943 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:00:52
Best params!
Params updated with best params
Initializing LSTM with 7 layers, 299 hidden size, 0.0011239309145681785 learning rate, 0.17138585922316754 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning
Best trial: 11573,42
best_score: 0.8029837012290955
best_params: {'batch_size': 171, 'dropout': 0.17138585922316754, 'hidden_layer_size': 299, 'learning_rate': 0.0011239309145681785, 'number_of_epochs': 58, 'number_of_layers': 7, 'optimizer_name': 'Adam'}
Training model
Testing model
Testing error: {'MASE': 0.8298162817955017, 'SMAPE': 0.3400062322616577, 'MSE': 0.06130383536219597, 'MAE': 0.21616224944591522}.
Saving model
Finished
