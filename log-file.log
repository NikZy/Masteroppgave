Logger initialized. Log level: INFO, log file: ./log-file.log
Started
Starting experiment: "lstm-tune-overfit-cat-6": "lstm-tune-overfit-cat-6"
Wiping and initializing checkpoint save location models/0_current_model_checkpoints
Creating model save location models/lstm-tune-overfit-cat-6
Creating new Neptune experiment
Neptune run URL: https://app.neptune.ai/sjsivertandsanderkk/Masteroppgave/e/MAS-337
Running tuning experiment with saving set to True
Choosing model structure: <src.model_strutures.local_univariate_lstm_structure.LocalUnivariateLstmStructure object at 0x154a390cdfd0>
Running model on device: cuda
Initializing LSTM with 3 layers, 30 hidden size, 0.03 learning rate, 0.113 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning
data preprocessing steps: 
 ---- Start ----
1- load market insight data and categories and merge them
2- convert date columns to date_time format
3- sum up clicks to category level [groupBy(date, cat_id)]
4- rename column 'title' to 'cat_name'
5- combine feature 'hits' and 'clicks' to new feature 'interest'
6- drop columns 'hits' and 'clicks'
7- filter out data from early 2018-12-01
8- drop uninteresting colums: ['id_x', 'manufacturer_id', '_version_', 'id_y', 'internal_doc_id_x', 'internal_doc_id_y']
---- End ----
        

------------------
Starting new timer at 14:59:23
Data Pipeline for 6: ---- Start ----
1- Convert input dataset to generator object
2- filter out category 6)
3- choose columns 'interest' and 'date'
4- fill in dates with zero values
5- convert to np.array
6- scale data between -1 and 1
7- split up into training set (7) and test set (-6)
8- split training set into train set with val window of 7) 
9- convert to timeseries dataset with input window size of 10, and output window size of 7
---- End ----
        
Converting dataset to dataloader using batch size 100.
Ending timer at: 14:59:23 
Elapsed CPU time: 0.342133966666817s
Used real time: 0:00:00
------------------
Tuning model: 6
Loading or creating optuna study with name: lstm-tune-overfit-cat-6_6
Number of previous Trials with this name are #3
Starting tuning trial number #3 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 310, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.00787466885546738, 'batch_first': True, 'batch_size': 29, 'dropout': 0.42335132287365457, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 84}
Converting dataset to dataloader using batch size 29.
Initializing LSTM with 5 layers, 310 hidden size, 0.00787466885546738 learning rate, 0.42335132287365457 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 14:59:24
Ending timer at: 15:00:41 
Elapsed CPU time: 2606.2089149166663s
Used real time: 0:01:17
------------------
Starting tuning trial number #4 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 60, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 2.348340822051925e-05, 'batch_first': True, 'batch_size': 258, 'dropout': 0.12625116083987098, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 76}
Converting dataset to dataloader using batch size 258.
Initializing LSTM with 3 layers, 60 hidden size, 2.348340822051925e-05 learning rate, 0.12625116083987098 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:00:42
Ending timer at: 15:00:54 
Elapsed CPU time: 257.8790922166661s
Used real time: 0:00:12
------------------
Starting tuning trial number #5 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 386, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 2.174728632611342e-06, 'batch_first': True, 'batch_size': 347, 'dropout': 0.3571864934569046, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 10}
Converting dataset to dataloader using batch size 347.
Initializing LSTM with 20 layers, 386 hidden size, 2.174728632611342e-06 learning rate, 0.3571864934569046 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:00:55
Ending timer at: 15:01:28 
Elapsed CPU time: 2742.914035816667s
Used real time: 0:00:33
------------------
Starting tuning trial number #6 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.0011079491322338582, 'batch_first': True, 'batch_size': 101, 'dropout': 0.12031492104680286, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 101.
Initializing LSTM with 18 layers, 37 hidden size, 0.0011079491322338582 learning rate, 0.12031492104680286 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:01:29
Ending timer at: 15:02:10 
Elapsed CPU time: 1161.731487099999s
Used real time: 0:00:41
------------------
Starting tuning trial number #7 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 226, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.027358208924101304, 'batch_first': True, 'batch_size': 177, 'dropout': 0.4280024898067002, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 177.
Initializing LSTM with 15 layers, 226 hidden size, 0.027358208924101304 learning rate, 0.4280024898067002 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:02:10
Ending timer at: 15:03:03 
Elapsed CPU time: 3743.127868349999s
Used real time: 0:00:53
------------------
Starting tuning trial number #8 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 14, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 8.259738249517939e-05, 'batch_first': True, 'batch_size': 211, 'dropout': 0.06692451625657969, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 89}
Converting dataset to dataloader using batch size 211.
Initializing LSTM with 7 layers, 14 hidden size, 8.259738249517939e-05 learning rate, 0.06692451625657969 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:03:04
Starting tuning trial number #9 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 295, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.06372509227873019, 'batch_first': True, 'batch_size': 256, 'dropout': 0.3624854833762801, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 256.
Initializing LSTM with 4 layers, 295 hidden size, 0.06372509227873019 learning rate, 0.3624854833762801 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:03:05
Starting tuning trial number #10 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 201, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 3.429044159799594e-05, 'batch_first': True, 'batch_size': 358, 'dropout': 0.17265150748203034, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 358.
Initializing LSTM with 6 layers, 201 hidden size, 3.429044159799594e-05 learning rate, 0.17265150748203034 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:03:07
Starting tuning trial number #11 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 180, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 0.003056660298016349, 'batch_first': True, 'batch_size': 20, 'dropout': 0.37705436171884676, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 47}
Converting dataset to dataloader using batch size 20.
Initializing LSTM with 28 layers, 180 hidden size, 0.003056660298016349 learning rate, 0.37705436171884676 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:03:08
Ending timer at: 15:05:32 
Elapsed CPU time: 5185.5880945s
Used real time: 0:02:24
------------------
Starting tuning trial number #12 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 210, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 1.2356297350382377e-05, 'batch_first': True, 'batch_size': 72, 'dropout': 0.008830534021762426, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 12}
Converting dataset to dataloader using batch size 72.
Initializing LSTM with 28 layers, 210 hidden size, 1.2356297350382377e-05 learning rate, 0.008830534021762426 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:05:32
Starting tuning trial number #13 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 101, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.0009707917819609061, 'batch_first': True, 'batch_size': 133, 'dropout': 0.2675018778986222, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 133.
Initializing LSTM with 16 layers, 101 hidden size, 0.0009707917819609061 learning rate, 0.2675018778986222 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:05:34
Ending timer at: 15:06:14 
Elapsed CPU time: 1860.7278665999993s
Used real time: 0:00:40
------------------
Starting tuning trial number #14 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 92, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.0007818981397213786, 'batch_first': True, 'batch_size': 117, 'dropout': 0.22119747354409738, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 117.
Initializing LSTM with 17 layers, 92 hidden size, 0.0007818981397213786 learning rate, 0.22119747354409738 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:06:15
Ending timer at: 15:06:58 
Elapsed CPU time: 2029.1630068666677s
Used real time: 0:00:43
------------------
Starting tuning trial number #15 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 121, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.0003578304877123855, 'batch_first': True, 'batch_size': 121, 'dropout': 0.27604866528063754, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 121.
Initializing LSTM with 13 layers, 121 hidden size, 0.0003578304877123855 learning rate, 0.27604866528063754 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:06:58
Starting tuning trial number #16 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.0013803773728273104, 'batch_first': True, 'batch_size': 129, 'dropout': 0.26717107387115446, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 22 layers, 10 hidden size, 0.0013803773728273104 learning rate, 0.26717107387115446 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:06:59
Starting tuning trial number #17 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 131, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.0002403928030369615, 'batch_first': True, 'batch_size': 76, 'dropout': 0.12840678360025407, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 76.
Initializing LSTM with 11 layers, 131 hidden size, 0.0002403928030369615 learning rate, 0.12840678360025407 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:00
Starting tuning trial number #18 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 61, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 0.006401127487365654, 'batch_first': True, 'batch_size': 183, 'dropout': 0.1983698838209646, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 183.
Initializing LSTM with 24 layers, 61 hidden size, 0.006401127487365654 learning rate, 0.1983698838209646 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:01
Starting tuning trial number #19 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 149, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.00018132649257999463, 'batch_first': True, 'batch_size': 61, 'dropout': 0.4977943385049744, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 61.
Initializing LSTM with 10 layers, 149 hidden size, 0.00018132649257999463 learning rate, 0.4977943385049744 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:01
Starting tuning trial number #20 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 69, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.0014866420481125413, 'batch_first': True, 'batch_size': 173, 'dropout': 0.3040686876781691, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 173.
Initializing LSTM with 18 layers, 69 hidden size, 0.0014866420481125413 learning rate, 0.3040686876781691 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:03
Starting tuning trial number #21 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 104, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 0.010210360646253858, 'batch_first': True, 'batch_size': 138, 'dropout': 0.06943175244836433, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 138.
Initializing LSTM with 23 layers, 104 hidden size, 0.010210360646253858 learning rate, 0.06943175244836433 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:03
Starting tuning trial number #22 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 6.5291831205717e-06, 'batch_first': True, 'batch_size': 254, 'dropout': 0.1521566712575122, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 254.
Initializing LSTM with 14 layers, 46 hidden size, 6.5291831205717e-06 learning rate, 0.1521566712575122 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:04
Starting tuning trial number #23 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 157, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 9.230089591029126e-05, 'batch_first': True, 'batch_size': 94, 'dropout': 0.08619272710561666, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 94.
Initializing LSTM with 19 layers, 157 hidden size, 9.230089591029126e-05 learning rate, 0.08619272710561666 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:05
Starting tuning trial number #24 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 91, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.0012406909373027335, 'batch_first': True, 'batch_size': 109, 'dropout': 0.20191640607124162, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 109.
Initializing LSTM with 17 layers, 91 hidden size, 0.0012406909373027335 learning rate, 0.20191640607124162 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:05
Starting tuning trial number #25 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 91, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.0004023828046210252, 'batch_first': True, 'batch_size': 145, 'dropout': 0.22539188548121247, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 145.
Initializing LSTM with 16 layers, 91 hidden size, 0.0004023828046210252 learning rate, 0.22539188548121247 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:06
Starting tuning trial number #26 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 49, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.0007353059823246734, 'batch_first': True, 'batch_size': 51, 'dropout': 0.31159141823204534, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 51.
Initializing LSTM with 11 layers, 49 hidden size, 0.0007353059823246734 learning rate, 0.31159141823204534 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:06
Starting tuning trial number #27 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 262, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.0035468573971291677, 'batch_first': True, 'batch_size': 208, 'dropout': 0.24462525614861605, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 208.
Initializing LSTM with 21 layers, 262 hidden size, 0.0035468573971291677 learning rate, 0.24462525614861605 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:08
Starting tuning trial number #28 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 109, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.022673775520838914, 'batch_first': True, 'batch_size': 98, 'dropout': 0.003979243518413639, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 98.
Initializing LSTM with 25 layers, 109 hidden size, 0.022673775520838914 learning rate, 0.003979243518413639 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:10
Starting tuning trial number #29 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 28, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.0006404379474409303, 'batch_first': True, 'batch_size': 3, 'dropout': 0.21233681169100826, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 3.
Initializing LSTM with 17 layers, 28 hidden size, 0.0006404379474409303 learning rate, 0.21233681169100826 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:12
Starting tuning trial number #30 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 167, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 1, 'learning_rate': 0.0030277705179084418, 'batch_first': True, 'batch_size': 152, 'dropout': 0.10998138997378135, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 152.
Initializing LSTM with 1 layers, 167 hidden size, 0.0030277705179084418 learning rate, 0.10998138997378135 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:12
Starting tuning trial number #31 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 77, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.00011233130687930568, 'batch_first': True, 'batch_size': 92, 'dropout': 0.17101081384439132, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 44}
Converting dataset to dataloader using batch size 92.
Initializing LSTM with 9 layers, 77 hidden size, 0.00011233130687930568 learning rate, 0.17101081384439132 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:13
Starting tuning trial number #32 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 377, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 26, 'learning_rate': 0.00905203748882781, 'batch_first': True, 'batch_size': 209, 'dropout': 0.3191521674710128, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 209.
Initializing LSTM with 26 layers, 377 hidden size, 0.00905203748882781 learning rate, 0.3191521674710128 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:14
Starting tuning trial number #33 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 143, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.0007135208351734449, 'batch_first': True, 'batch_size': 39, 'dropout': 0.04122369471594148, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 39.
Initializing LSTM with 13 layers, 143 hidden size, 0.0007135208351734449 learning rate, 0.04122369471594148 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:14
Starting tuning trial number #34 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 249, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.07738677200383687, 'batch_first': True, 'batch_size': 168, 'dropout': 0.4409786449527182, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 36}
Converting dataset to dataloader using batch size 168.
Initializing LSTM with 15 layers, 249 hidden size, 0.07738677200383687 learning rate, 0.4409786449527182 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:15
Starting tuning trial number #35 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 242, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.017544524354780107, 'batch_first': True, 'batch_size': 157, 'dropout': 0.4192179512749414, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 33}
Converting dataset to dataloader using batch size 157.
Initializing LSTM with 19 layers, 242 hidden size, 0.017544524354780107 learning rate, 0.4192179512749414 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:16
Starting tuning trial number #36 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 347, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.03833085203089386, 'batch_first': True, 'batch_size': 192, 'dropout': 0.4650560136832926, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 192.
Initializing LSTM with 15 layers, 347 hidden size, 0.03833085203089386 learning rate, 0.4650560136832926 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:16
Starting tuning trial number #37 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 197, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 4.8256488498838765e-05, 'batch_first': True, 'batch_size': 321, 'dropout': 0.40505955375235325, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 321.
Initializing LSTM with 20 layers, 197 hidden size, 4.8256488498838765e-05 learning rate, 0.40505955375235325 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:17
Starting tuning trial number #38 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 281, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 1.1404490837029736e-06, 'batch_first': True, 'batch_size': 234, 'dropout': 0.3495712535937405, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 234.
Initializing LSTM with 13 layers, 281 hidden size, 1.1404490837029736e-06 learning rate, 0.3495712535937405 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:18
Starting tuning trial number #39 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 224, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.0034109444643936697, 'batch_first': True, 'batch_size': 113, 'dropout': 0.12641612745202122, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 113.
Initializing LSTM with 8 layers, 224 hidden size, 0.0034109444643936697 learning rate, 0.12641612745202122 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:19
Ending timer at: 15:07:41 
Elapsed CPU time: 1076.7856735999999s
Used real time: 0:00:22
------------------
Starting tuning trial number #40 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 89, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.0021503544453802806, 'batch_first': True, 'batch_size': 118, 'dropout': 0.10721039617079856, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 118.
Initializing LSTM with 8 layers, 89 hidden size, 0.0021503544453802806 learning rate, 0.10721039617079856 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:41
Starting tuning trial number #41 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.004033121661053772, 'batch_first': True, 'batch_size': 90, 'dropout': 0.14490742581813054, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 90.
Initializing LSTM with 5 layers, 32 hidden size, 0.004033121661053772 learning rate, 0.14490742581813054 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:42
Starting tuning trial number #42 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 177, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.0058817519256269945, 'batch_first': True, 'batch_size': 71, 'dropout': 0.17577312675543486, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 71.
Initializing LSTM with 21 layers, 177 hidden size, 0.0058817519256269945 learning rate, 0.17577312675543486 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:42
Starting tuning trial number #43 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 321, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 30, 'learning_rate': 0.0005079389427372262, 'batch_first': True, 'batch_size': 114, 'dropout': 0.2327734376412633, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 114.
Initializing LSTM with 30 layers, 321 hidden size, 0.0005079389427372262 learning rate, 0.2327734376412633 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:43
Starting tuning trial number #44 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 228, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.000974601873682771, 'batch_first': True, 'batch_size': 136, 'dropout': 0.2830204703350544, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 49}
Converting dataset to dataloader using batch size 136.
Initializing LSTM with 18 layers, 228 hidden size, 0.000974601873682771 learning rate, 0.2830204703350544 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:44
Starting tuning trial number #45 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 218, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 12, 'learning_rate': 0.015805612530131262, 'batch_first': True, 'batch_size': 160, 'dropout': 0.3392849143487438, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 160.
Initializing LSTM with 12 layers, 218 hidden size, 0.015805612530131262 learning rate, 0.3392849143487438 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:45
Starting tuning trial number #46 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 192, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.000204005558883967, 'batch_first': True, 'batch_size': 130, 'dropout': 0.3904960286877591, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 33}
Converting dataset to dataloader using batch size 130.
Initializing LSTM with 16 layers, 192 hidden size, 0.000204005558883967 learning rate, 0.3904960286877591 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:46
Starting tuning trial number #47 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 285, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0019546377413305724, 'batch_first': True, 'batch_size': 294, 'dropout': 0.028953392095537656, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 294.
Initializing LSTM with 6 layers, 285 hidden size, 0.0019546377413305724 learning rate, 0.028953392095537656 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:46
Starting tuning trial number #48 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 131, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 0.04924048053170817, 'batch_first': True, 'batch_size': 188, 'dropout': 0.2585047795286699, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 5}
Converting dataset to dataloader using batch size 188.
Initializing LSTM with 2 layers, 131 hidden size, 0.04924048053170817 learning rate, 0.2585047795286699 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:47
Starting tuning trial number #49 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 265, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.005487929430581946, 'batch_first': True, 'batch_size': 106, 'dropout': 0.09787972963900995, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 106.
Initializing LSTM with 14 layers, 265 hidden size, 0.005487929430581946 learning rate, 0.09787972963900995 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:47
Starting tuning trial number #50 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 31, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.0002682914027341989, 'batch_first': True, 'batch_size': 84, 'dropout': 0.13134178953195824, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 84.
Initializing LSTM with 10 layers, 31 hidden size, 0.0002682914027341989 learning rate, 0.13134178953195824 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:48
Starting tuning trial number #51 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 223, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.02882828428947633, 'batch_first': True, 'batch_size': 229, 'dropout': 0.1818584628994835, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 229.
Initializing LSTM with 17 layers, 223 hidden size, 0.02882828428947633 learning rate, 0.1818584628994835 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:49
Starting tuning trial number #52 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.011865129205467414, 'batch_first': True, 'batch_size': 53, 'dropout': 0.05622193966429784, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 77}
Converting dataset to dataloader using batch size 53.
Initializing LSTM with 19 layers, 54 hidden size, 0.011865129205467414 learning rate, 0.05622193966429784 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:50
Starting tuning trial number #53 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 119, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.0022351425077335512, 'batch_first': True, 'batch_size': 127, 'dropout': 0.29127350640522953, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 83}
Converting dataset to dataloader using batch size 127.
Initializing LSTM with 22 layers, 119 hidden size, 0.0022351425077335512 learning rate, 0.29127350640522953 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:50
Starting tuning trial number #54 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 180, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 30, 'learning_rate': 0.0010376245094442656, 'batch_first': True, 'batch_size': 18, 'dropout': 0.38314682192404603, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 47}
Converting dataset to dataloader using batch size 18.
Initializing LSTM with 30 layers, 180 hidden size, 0.0010376245094442656 learning rate, 0.38314682192404603 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:51
Starting tuning trial number #55 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 241, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 0.00038663709910909084, 'batch_first': True, 'batch_size': 31, 'dropout': 0.46345224804597407, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 31.
Initializing LSTM with 28 layers, 241 hidden size, 0.00038663709910909084 learning rate, 0.46345224804597407 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:52
Starting tuning trial number #56 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 202, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 0.0017773926471347976, 'batch_first': True, 'batch_size': 72, 'dropout': 0.3309737650241272, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 38}
Converting dataset to dataloader using batch size 72.
Initializing LSTM with 24 layers, 202 hidden size, 0.0017773926471347976 learning rate, 0.3309737650241272 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:52
Starting tuning trial number #57 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 74, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.0001445984596205918, 'batch_first': True, 'batch_size': 172, 'dropout': 0.363490630361716, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 47}
Converting dataset to dataloader using batch size 172.
Initializing LSTM with 4 layers, 74 hidden size, 0.0001445984596205918 learning rate, 0.363490630361716 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:53
Starting tuning trial number #58 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 158, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 27, 'learning_rate': 0.00405553145151333, 'batch_first': True, 'batch_size': 144, 'dropout': 0.49868088728453136, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 144.
Initializing LSTM with 27 layers, 158 hidden size, 0.00405553145151333 learning rate, 0.49868088728453136 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:54
Starting tuning trial number #59 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 18, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.001033212577012807, 'batch_first': True, 'batch_size': 110, 'dropout': 0.14731863470776577, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 96}
Converting dataset to dataloader using batch size 110.
Initializing LSTM with 7 layers, 18 hidden size, 0.001033212577012807 learning rate, 0.14731863470776577 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:54
Starting tuning trial number #60 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 110, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 12, 'learning_rate': 0.0031546302141508037, 'batch_first': True, 'batch_size': 57, 'dropout': 0.4455060015592907, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 30}
Converting dataset to dataloader using batch size 57.
Initializing LSTM with 12 layers, 110 hidden size, 0.0031546302141508037 learning rate, 0.4455060015592907 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:56
Starting tuning trial number #61 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 182, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 5.880312232344062e-05, 'batch_first': True, 'batch_size': 198, 'dropout': 0.2123896439304992, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 45}
Converting dataset to dataloader using batch size 198.
Initializing LSTM with 18 layers, 182 hidden size, 5.880312232344062e-05 learning rate, 0.2123896439304992 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:57
Starting tuning trial number #62 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 138, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.007374317898561714, 'batch_first': True, 'batch_size': 13, 'dropout': 0.0827956063152532, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 13.
Initializing LSTM with 14 layers, 138 hidden size, 0.007374317898561714 learning rate, 0.0827956063152532 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:07:57
Starting tuning trial number #63 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 86, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.0006383062453893534, 'batch_first': True, 'batch_size': 100, 'dropout': 0.3655341390941959, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 87}
Converting dataset to dataloader using batch size 100.
Initializing LSTM with 16 layers, 86 hidden size, 0.0006383062453893534 learning rate, 0.3655341390941959 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:02
Starting tuning trial number #64 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 39, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 7.930787716307293e-06, 'batch_first': True, 'batch_size': 301, 'dropout': 0.1288988731365497, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 301.
Initializing LSTM with 3 layers, 39 hidden size, 7.930787716307293e-06 learning rate, 0.1288988731365497 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:03
Starting tuning trial number #65 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 64, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 1, 'learning_rate': 3.3677069617495603e-06, 'batch_first': True, 'batch_size': 265, 'dropout': 0.15736304996762016, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 74}
Converting dataset to dataloader using batch size 265.
Initializing LSTM with 1 layers, 64 hidden size, 3.3677069617495603e-06 learning rate, 0.15736304996762016 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:03
Starting tuning trial number #66 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 51, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 1.7036648988382987e-05, 'batch_first': True, 'batch_size': 147, 'dropout': 0.19761475459636724, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 147.
Initializing LSTM with 9 layers, 51 hidden size, 1.7036648988382987e-05 learning rate, 0.19761475459636724 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:04
Starting tuning trial number #67 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 212, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 2.4250699706730978e-05, 'batch_first': True, 'batch_size': 230, 'dropout': 0.10976285671241667, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 230.
Initializing LSTM with 20 layers, 212 hidden size, 2.4250699706730978e-05 learning rate, 0.10976285671241667 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:05
Starting tuning trial number #68 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 12, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.0014416641476415457, 'batch_first': True, 'batch_size': 176, 'dropout': 0.0741479140226009, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 81}
Converting dataset to dataloader using batch size 176.
Initializing LSTM with 5 layers, 12 hidden size, 0.0014416641476415457 learning rate, 0.0741479140226009 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:06
Starting tuning trial number #69 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 100, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 0.0002996965459812008, 'batch_first': True, 'batch_size': 126, 'dropout': 0.2507968236055399, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 126.
Initializing LSTM with 23 layers, 100 hidden size, 0.0002996965459812008 learning rate, 0.2507968236055399 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:06
Starting tuning trial number #70 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 63, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.0004757054067143454, 'batch_first': True, 'batch_size': 81, 'dropout': 0.16081735981547277, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 81.
Initializing LSTM with 15 layers, 63 hidden size, 0.0004757054067143454 learning rate, 0.16081735981547277 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:07
Starting tuning trial number #71 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 77, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.0008308764469570094, 'batch_first': True, 'batch_size': 218, 'dropout': 0.30033148812404736, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 76}
Converting dataset to dataloader using batch size 218.
Initializing LSTM with 7 layers, 77 hidden size, 0.0008308764469570094 learning rate, 0.30033148812404736 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:08
Starting tuning trial number #72 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 260, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.002636008887454161, 'batch_first': True, 'batch_size': 349, 'dropout': 0.4208364601344902, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 349.
Initializing LSTM with 11 layers, 260 hidden size, 0.002636008887454161 learning rate, 0.4208364601344902 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:08
Starting tuning trial number #73 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 122, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.012778235772174424, 'batch_first': True, 'batch_size': 162, 'dropout': 0.18981632316383565, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 162.
Initializing LSTM with 17 layers, 122 hidden size, 0.012778235772174424 learning rate, 0.18981632316383565 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:09
Starting tuning trial number #74 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 311, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.021753406237143808, 'batch_first': True, 'batch_size': 40, 'dropout': 0.43823108412126116, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 84}
Converting dataset to dataloader using batch size 40.
Initializing LSTM with 4 layers, 311 hidden size, 0.021753406237143808 learning rate, 0.43823108412126116 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:09
Starting tuning trial number #75 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 396, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 0.005541233046365395, 'batch_first': True, 'batch_size': 24, 'dropout': 0.3859468671957164, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 87}
Converting dataset to dataloader using batch size 24.
Initializing LSTM with 2 layers, 396 hidden size, 0.005541233046365395 learning rate, 0.3859468671957164 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:10
Starting tuning trial number #76 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 345, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.09309045948234977, 'batch_first': True, 'batch_size': 133, 'dropout': 0.4734509923524835, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 133.
Initializing LSTM with 5 layers, 345 hidden size, 0.09309045948234977 learning rate, 0.4734509923524835 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:10
Starting tuning trial number #77 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 232, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.009367271646603188, 'batch_first': True, 'batch_size': 63, 'dropout': 0.4084587063817297, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 63.
Initializing LSTM with 6 layers, 232 hidden size, 0.009367271646603188 learning rate, 0.4084587063817297 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:11
Starting tuning trial number #78 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 303, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 0.001552401547734647, 'batch_first': True, 'batch_size': 43, 'dropout': 0.12058649234280751, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 43.
Initializing LSTM with 2 layers, 303 hidden size, 0.001552401547734647 learning rate, 0.12058649234280751 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:11
Starting tuning trial number #79 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 154, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 0.0040626859179758185, 'batch_first': True, 'batch_size': 9, 'dropout': 0.2674109701204208, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 9.
Initializing LSTM with 3 layers, 154 hidden size, 0.0040626859179758185 learning rate, 0.2674109701204208 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:12
Starting tuning trial number #80 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 276, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.032087463419077415, 'batch_first': True, 'batch_size': 119, 'dropout': 0.47982510969027, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 96}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 8 layers, 276 hidden size, 0.032087463419077415 learning rate, 0.47982510969027 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:13
Starting tuning trial number #81 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 164, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.0012249635379386707, 'batch_first': True, 'batch_size': 250, 'dropout': 0.22897667689927315, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 250.
Initializing LSTM with 14 layers, 164 hidden size, 0.0012249635379386707 learning rate, 0.22897667689927315 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:14
Starting tuning trial number #82 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 330, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.051290569035690556, 'batch_first': True, 'batch_size': 101, 'dropout': 0.43289330947949045, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 101.
Initializing LSTM with 25 layers, 330 hidden size, 0.051290569035690556 learning rate, 0.43289330947949045 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:15
Starting tuning trial number #83 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 99, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 0.002878207335932955, 'batch_first': True, 'batch_size': 85, 'dropout': 0.14275923895880507, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 85.
Initializing LSTM with 20 layers, 99 hidden size, 0.002878207335932955 learning rate, 0.14275923895880507 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:15
Starting tuning trial number #84 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 397, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 1.3892105313862226e-06, 'batch_first': True, 'batch_size': 339, 'dropout': 0.37459703423427276, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 339.
Initializing LSTM with 19 layers, 397 hidden size, 1.3892105313862226e-06 learning rate, 0.37459703423427276 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:16
Starting tuning trial number #85 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 371, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 2.0001602569275745e-06, 'batch_first': True, 'batch_size': 272, 'dropout': 0.3188114236604689, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 10}
Converting dataset to dataloader using batch size 272.
Initializing LSTM with 17 layers, 371 hidden size, 2.0001602569275745e-06 learning rate, 0.3188114236604689 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:17
Starting tuning trial number #86 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 361, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 7.049455006488711e-06, 'batch_first': True, 'batch_size': 312, 'dropout': 0.3520991549135179, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 85}
Converting dataset to dataloader using batch size 312.
Initializing LSTM with 21 layers, 361 hidden size, 7.049455006488711e-06 learning rate, 0.3520991549135179 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:18
Starting tuning trial number #87 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 23, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.0005839848193095631, 'batch_first': True, 'batch_size': 66, 'dropout': 0.40240041630165324, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 66.
Initializing LSTM with 16 layers, 23 hidden size, 0.0005839848193095631 learning rate, 0.40240041630165324 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:19
Starting tuning trial number #88 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 3.13854791730174e-06, 'batch_first': True, 'batch_size': 285, 'dropout': 0.45234253520603085, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 81}
Converting dataset to dataloader using batch size 285.
Initializing LSTM with 18 layers, 41 hidden size, 3.13854791730174e-06 learning rate, 0.45234253520603085 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:19
Starting tuning trial number #89 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 57, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 12, 'learning_rate': 0.004396168643579992, 'batch_first': True, 'batch_size': 195, 'dropout': 0.4220139332355749, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 195.
Initializing LSTM with 12 layers, 57 hidden size, 0.004396168643579992 learning rate, 0.4220139332355749 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:20
Starting tuning trial number #90 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 192, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 0.007226547631702563, 'batch_first': True, 'batch_size': 3, 'dropout': 0.39444029985201484, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 12}
Converting dataset to dataloader using batch size 3.
Initializing LSTM with 28 layers, 192 hidden size, 0.007226547631702563 learning rate, 0.39444029985201484 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:21
Starting tuning trial number #91 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 251, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.0020414596441481036, 'batch_first': True, 'batch_size': 330, 'dropout': 0.05208671691484902, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 330.
Initializing LSTM with 22 layers, 251 hidden size, 0.0020414596441481036 learning rate, 0.05208671691484902 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:22
Starting tuning trial number #92 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 333, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 1.3636212634082423e-05, 'batch_first': True, 'batch_size': 244, 'dropout': 0.017794027040605537, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 244.
Initializing LSTM with 15 layers, 333 hidden size, 1.3636212634082423e-05 learning rate, 0.017794027040605537 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:22
Starting tuning trial number #93 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 290, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 8.204550555831273e-05, 'batch_first': True, 'batch_size': 181, 'dropout': 0.09588443849921949, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 181.
Initializing LSTM with 13 layers, 290 hidden size, 8.204550555831273e-05 learning rate, 0.09588443849921949 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:24
Starting tuning trial number #94 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 85, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.0008244905900711291, 'batch_first': True, 'batch_size': 26, 'dropout': 0.31556497430278363, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 26.
Initializing LSTM with 7 layers, 85 hidden size, 0.0008244905900711291 learning rate, 0.31556497430278363 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:25
Starting tuning trial number #95 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 44, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.00016951692860152313, 'batch_first': True, 'batch_size': 46, 'dropout': 0.34299448315197273, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 49}
Converting dataset to dataloader using batch size 46.
Initializing LSTM with 10 layers, 44 hidden size, 0.00016951692860152313 learning rate, 0.34299448315197273 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:25
Starting tuning trial number #96 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 68, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.0003616849666864347, 'batch_first': True, 'batch_size': 359, 'dropout': 0.28351762252739526, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 359.
Initializing LSTM with 11 layers, 68 hidden size, 0.0003616849666864347 learning rate, 0.28351762252739526 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:26
Starting tuning trial number #97 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.001228303916689607, 'batch_first': True, 'batch_size': 143, 'dropout': 0.33221734589797736, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 89}
Converting dataset to dataloader using batch size 143.
Initializing LSTM with 9 layers, 37 hidden size, 0.001228303916689607 learning rate, 0.33221734589797736 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:26
Starting tuning trial number #98 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 212, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0007525137174961399, 'batch_first': True, 'batch_size': 31, 'dropout': 0.24530015268667643, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 31.
Initializing LSTM with 6 layers, 212 hidden size, 0.0007525137174961399 learning rate, 0.24530015268667643 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:27
Starting tuning trial number #99 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 217, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 3.2845617236346596e-05, 'batch_first': True, 'batch_size': 33, 'dropout': 0.2453427608466351, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 33.
Initializing LSTM with 5 layers, 217 hidden size, 3.2845617236346596e-05 learning rate, 0.2453427608466351 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:29
Starting tuning trial number #100 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 236, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 4.325769005920508e-06, 'batch_first': True, 'batch_size': 51, 'dropout': 0.20847361799240624, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 51.
Initializing LSTM with 3 layers, 236 hidden size, 4.325769005920508e-06 learning rate, 0.20847361799240624 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:29
Starting tuning trial number #101 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 187, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0004435233637031196, 'batch_first': True, 'batch_size': 92, 'dropout': 0.16825330272324657, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 92.
Initializing LSTM with 6 layers, 187 hidden size, 0.0004435233637031196 learning rate, 0.16825330272324657 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:30
Starting tuning trial number #102 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 172, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.015323345753742319, 'batch_first': True, 'batch_size': 155, 'dropout': 0.13660683342386426, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 155.
Initializing LSTM with 18 layers, 172 hidden size, 0.015323345753742319 learning rate, 0.13660683342386426 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:08:30
Best params!
Params updated with best params
Initializing LSTM with 18 layers, 37 hidden size, 0.0011079491322338582 learning rate, 0.12031492104680286 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning
Best trial: 6,6
best_score: 1.6609801054000854
best_params: {'batch_size': 101, 'dropout': 0.12031492104680286, 'hidden_layer_size': 37, 'learning_rate': 0.0011079491322338582, 'number_of_epochs': 90, 'number_of_layers': 18, 'optimizer_name': 'Adam'}
Training model
Testing model
Testing error: {'MASE': 1.0571842193603516, 'SMAPE': 0.18322032690048218, 'MSE': 0.012971380725502968, 'MAE': 0.09758161008358002}.
Saving model
Finished
