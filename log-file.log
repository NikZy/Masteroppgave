Logger initialized. Log level: INFO, log file: ./log-file.log
Started
Starting experiment: "lstm-tune-cat-6-multi-output-mse": "lstm-tune-cat-6-multi-output-mse"
Wiping and initializing checkpoint save location models/0_current_model_checkpoints
Creating model save location models/lstm-tune-cat-6-multi-output-mse
Creating new Neptune experiment
Neptune run URL: https://app.neptune.ai/sjsivertandsanderkk/Masteroppgave/e/MAS-323
Running tuning experiment with saving set to True
Choosing model structure: <src.model_strutures.local_univariate_lstm_structure.LocalUnivariateLstmStructure object at 0x148be213bd00>
Running model on device: cuda
Initializing LSTM with 3 layers, 30 hidden size, 0.03 learning rate, 0.113 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning
data preprocessing steps: 
 ---- Start ----
1- load market insight data and categories and merge them
2- convert date columns to date_time format
3- sum up clicks to category level [groupBy(date, cat_id)]
4- rename column 'title' to 'cat_name'
5- combine feature 'hits' and 'clicks' to new feature 'interest'
6- drop columns 'hits' and 'clicks'
7- filter out data from early 2018-12-01
8- drop uninteresting colums: ['id_x', 'manufacturer_id', '_version_', 'id_y', 'internal_doc_id_x', 'internal_doc_id_y']
---- End ----
        

------------------
Starting new timer at 20:10:56
Data Pipeline for 11573: ---- Start ----
1- Convert input dataset to generator object
2- filter out category 11573)
3- choose columns 'interest' and 'date'
4- fill in dates with zero values
5- convert to np.array
6- scale data between -1 and 1
7- split up into training set (7) and test set (-6)
8- split training set into train set (7) and validation set (-6)
9- convert to timeseries dataset with input window size of 10, and output window size of 7
---- End ----
        
Converting dataset to dataloader using batch size 100.
Ending timer at: 20:10:56 
Elapsed CPU time: 0.5477053333332075s
Used real time: 0:00:00
------------------
Tuning model: 11573
Loading or creating optuna study with name: lstm-tune-cat-6-multi-output-mse_11573
Number of previous Trials with this name are #0
Starting tuning trial number #0 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 52, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.08064359810985887, 'batch_first': True, 'batch_size': 243, 'dropout': 0.17073131512310274, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 243.
Initializing LSTM with 18 layers, 52 hidden size, 0.08064359810985887 learning rate, 0.17073131512310274 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:10:57
Ending timer at: 20:11:48 
Elapsed CPU time: 796.6024493333331s
Used real time: 0:00:51
------------------
Starting tuning trial number #1 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.001746633171583667, 'batch_first': True, 'batch_size': 192, 'dropout': 0.08681814071708832, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 192.
Initializing LSTM with 22 layers, 42 hidden size, 0.001746633171583667 learning rate, 0.08681814071708832 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:11:49
Ending timer at: 20:13:07 
Elapsed CPU time: 1266.5255144s
Used real time: 0:01:18
------------------
Starting tuning trial number #2 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 156, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 8.332386006202114e-05, 'batch_first': True, 'batch_size': 164, 'dropout': 0.3334244507740705, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 164.
Initializing LSTM with 16 layers, 156 hidden size, 8.332386006202114e-05 learning rate, 0.3334244507740705 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:13:08
Ending timer at: 20:13:36 
Elapsed CPU time: 458.0827308333336s
Used real time: 0:00:28
------------------
Starting tuning trial number #3 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 145, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 1.1347923519260236e-05, 'batch_first': True, 'batch_size': 63, 'dropout': 0.07470285085106626, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 15}
Converting dataset to dataloader using batch size 63.
Initializing LSTM with 24 layers, 145 hidden size, 1.1347923519260236e-05 learning rate, 0.07470285085106626 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:13:36
Ending timer at: 20:14:16 
Elapsed CPU time: 1366.6352513500008s
Used real time: 0:00:40
------------------
Starting tuning trial number #4 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 131, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.0030715224499998257, 'batch_first': True, 'batch_size': 271, 'dropout': 0.1770289193217538, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 271.
Initializing LSTM with 25 layers, 131 hidden size, 0.0030715224499998257 learning rate, 0.1770289193217538 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:14:17
Ending timer at: 20:14:49 
Elapsed CPU time: 470.3412536166667s
Used real time: 0:00:32
------------------
Starting tuning trial number #5 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 334, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 9.485242420719483e-06, 'batch_first': True, 'batch_size': 127, 'dropout': 0.05026633629536159, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 42}
Converting dataset to dataloader using batch size 127.
Initializing LSTM with 21 layers, 334 hidden size, 9.485242420719483e-06 learning rate, 0.05026633629536159 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:14:52
Starting tuning trial number #6 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 367, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.003529778865848591, 'batch_first': True, 'batch_size': 185, 'dropout': 0.0005383637193096535, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 31}
Converting dataset to dataloader using batch size 185.
Initializing LSTM with 5 layers, 367 hidden size, 0.003529778865848591 learning rate, 0.0005383637193096535 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:14:57
Ending timer at: 20:15:41 
Elapsed CPU time: 650.8353848666673s
Used real time: 0:00:44
------------------
Starting tuning trial number #7 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 0.022896926063953958, 'batch_first': True, 'batch_size': 191, 'dropout': 0.12561875081413676, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 191.
Initializing LSTM with 24 layers, 32 hidden size, 0.022896926063953958 learning rate, 0.12561875081413676 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:15:42
Starting tuning trial number #8 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 395, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.018952062819412734, 'batch_first': True, 'batch_size': 259, 'dropout': 0.3540595652314424, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 259.
Initializing LSTM with 10 layers, 395 hidden size, 0.018952062819412734 learning rate, 0.3540595652314424 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:15:43
Starting tuning trial number #9 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 232, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.0010620136924145247, 'batch_first': True, 'batch_size': 328, 'dropout': 0.14524412273938042, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 328.
Initializing LSTM with 16 layers, 232 hidden size, 0.0010620136924145247 learning rate, 0.14524412273938042 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:15:44
Starting tuning trial number #10 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 273, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 30, 'learning_rate': 0.0001439240958232468, 'batch_first': True, 'batch_size': 4, 'dropout': 0.49143555782526327, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 99}
Converting dataset to dataloader using batch size 4.
Initializing LSTM with 30 layers, 273 hidden size, 0.0001439240958232468 learning rate, 0.49143555782526327 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:16:03
Starting tuning trial number #11 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 314, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 0.002352390989421187, 'batch_first': True, 'batch_size': 104, 'dropout': 0.007407976937089444, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 83}
Converting dataset to dataloader using batch size 104.
Initializing LSTM with 3 layers, 314 hidden size, 0.002352390989421187 learning rate, 0.007407976937089444 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:33
Starting tuning trial number #12 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 392, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 1.135463034281331e-06, 'batch_first': True, 'batch_size': 198, 'dropout': 0.010581916592736718, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 34}
Converting dataset to dataloader using batch size 198.
Initializing LSTM with 9 layers, 392 hidden size, 1.135463034281331e-06 learning rate, 0.010581916592736718 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:38
Starting tuning trial number #13 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 207, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 1, 'learning_rate': 0.0006421214721823036, 'batch_first': True, 'batch_size': 336, 'dropout': 0.22952290661572344, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 336.
Initializing LSTM with 1 layers, 207 hidden size, 0.0006421214721823036 learning rate, 0.22952290661572344 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:42
Starting tuning trial number #14 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 70, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 10, 'learning_rate': 0.008262153729281654, 'batch_first': True, 'batch_size': 145, 'dropout': 0.09736080587049674, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 5}
Converting dataset to dataloader using batch size 145.
Initializing LSTM with 10 layers, 70 hidden size, 0.008262153729281654 learning rate, 0.09736080587049674 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:46
Starting tuning trial number #15 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 113, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0003053456219624632, 'batch_first': True, 'batch_size': 210, 'dropout': 0.25070726388200765, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 32}
Converting dataset to dataloader using batch size 210.
Initializing LSTM with 6 layers, 113 hidden size, 0.0003053456219624632 learning rate, 0.25070726388200765 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:47
Starting tuning trial number #16 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 243, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.0042178209675383955, 'batch_first': True, 'batch_size': 78, 'dropout': 0.005529891680627258, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 78.
Initializing LSTM with 13 layers, 243 hidden size, 0.0042178209675383955 learning rate, 0.005529891680627258 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:48
Starting tuning trial number #17 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 343, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 3.556222753539759e-05, 'batch_first': True, 'batch_size': 297, 'dropout': 0.058804049320925596, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 297.
Initializing LSTM with 28 layers, 343 hidden size, 3.556222753539759e-05 learning rate, 0.058804049320925596 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:50
Starting tuning trial number #18 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 11, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.06542899368571858, 'batch_first': True, 'batch_size': 231, 'dropout': 0.23891347254346618, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 91}
Converting dataset to dataloader using batch size 231.
Initializing LSTM with 19 layers, 11 hidden size, 0.06542899368571858 learning rate, 0.23891347254346618 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:28:55
Ending timer at: 20:31:05 
Elapsed CPU time: 1644.2660243499972s
Used real time: 0:02:10
------------------
Starting tuning trial number #19 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 16, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 0.08417015414891048, 'batch_first': True, 'batch_size': 301, 'dropout': 0.3044380859614865, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 97}
Converting dataset to dataloader using batch size 301.
Initializing LSTM with 20 layers, 16 hidden size, 0.08417015414891048 learning rate, 0.3044380859614865 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:06
Starting tuning trial number #20 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 82, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.01760199546775433, 'batch_first': True, 'batch_size': 228, 'dropout': 0.4720926464073878, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 228.
Initializing LSTM with 21 layers, 82 hidden size, 0.01760199546775433 learning rate, 0.4720926464073878 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:10
Starting tuning trial number #21 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 172, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.001234154815087144, 'batch_first': True, 'batch_size': 169, 'dropout': 0.410794425476698, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 169.
Initializing LSTM with 14 layers, 172 hidden size, 0.001234154815087144 learning rate, 0.410794425476698 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:11
Starting tuning trial number #22 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 85, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.007412226358871667, 'batch_first': True, 'batch_size': 229, 'dropout': 0.2673312639189639, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 229.
Initializing LSTM with 19 layers, 85 hidden size, 0.007412226358871667 learning rate, 0.2673312639189639 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:12
Starting tuning trial number #23 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 14, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 26, 'learning_rate': 0.03180786912493818, 'batch_first': True, 'batch_size': 144, 'dropout': 0.220214999102533, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 144.
Initializing LSTM with 26 layers, 14 hidden size, 0.03180786912493818 learning rate, 0.220214999102533 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:14
Starting tuning trial number #24 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 103, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0004254822404210772, 'batch_first': True, 'batch_size': 205, 'dropout': 0.11498236554977463, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 76}
Converting dataset to dataloader using batch size 205.
Initializing LSTM with 6 layers, 103 hidden size, 0.0004254822404210772 learning rate, 0.11498236554977463 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:15
Starting tuning trial number #25 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 12, 'learning_rate': 0.00735987359712767, 'batch_first': True, 'batch_size': 280, 'dropout': 0.1861821554079241, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 89}
Converting dataset to dataloader using batch size 280.
Initializing LSTM with 12 layers, 54 hidden size, 0.00735987359712767 learning rate, 0.1861821554079241 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:17
Starting tuning trial number #26 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 285, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 0.04940978735172797, 'batch_first': True, 'batch_size': 184, 'dropout': 0.04402272603570732, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 47}
Converting dataset to dataloader using batch size 184.
Initializing LSTM with 23 layers, 285 hidden size, 0.04940978735172797 learning rate, 0.04402272603570732 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:18
Starting tuning trial number #27 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 183, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.0017606222881184509, 'batch_first': True, 'batch_size': 110, 'dropout': 0.08912732972770937, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 110.
Initializing LSTM with 18 layers, 183 hidden size, 0.0017606222881184509 learning rate, 0.08912732972770937 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:19
Starting tuning trial number #28 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 36, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.011432435052035318, 'batch_first': True, 'batch_size': 232, 'dropout': 0.3797182040853552, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 232.
Initializing LSTM with 22 layers, 36 hidden size, 0.011432435052035318 learning rate, 0.3797182040853552 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:31:20
Ending timer at: 20:32:52 
Elapsed CPU time: 1370.6028209166675s
Used real time: 0:01:32
------------------
Starting tuning trial number #29 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 44, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.07557142312453445, 'batch_first': True, 'batch_size': 239, 'dropout': 0.40301476235005196, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 239.
Initializing LSTM with 22 layers, 44 hidden size, 0.07557142312453445 learning rate, 0.40301476235005196 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:32:52
Starting tuning trial number #30 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 62, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 27, 'learning_rate': 0.0148334808691204, 'batch_first': True, 'batch_size': 253, 'dropout': 0.2924802059672063, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 77}
Converting dataset to dataloader using batch size 253.
Initializing LSTM with 27 layers, 62 hidden size, 0.0148334808691204 learning rate, 0.2924802059672063 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:32:53
Starting tuning trial number #31 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.005985827899988129, 'batch_first': True, 'batch_size': 216, 'dropout': 0.3699903219788771, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 216.
Initializing LSTM with 18 layers, 37 hidden size, 0.005985827899988129 learning rate, 0.3699903219788771 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:32:55
Ending timer at: 20:34:05 
Elapsed CPU time: 1032.7161123666656s
Used real time: 0:01:10
------------------
Starting tuning trial number #32 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 14, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.03802336811182988, 'batch_first': True, 'batch_size': 163, 'dropout': 0.43903830639624264, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 37}
Converting dataset to dataloader using batch size 163.
Initializing LSTM with 17 layers, 14 hidden size, 0.03802336811182988 learning rate, 0.43903830639624264 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:34:06
Ending timer at: 20:35:01 
Elapsed CPU time: 724.1987412666655s
Used real time: 0:00:55
------------------
Starting tuning trial number #33 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 92, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.011957856678309968, 'batch_first': True, 'batch_size': 174, 'dropout': 0.32523081529264386, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 174.
Initializing LSTM with 22 layers, 92 hidden size, 0.011957856678309968 learning rate, 0.32523081529264386 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:02
Starting tuning trial number #34 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 121, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.0032504978823889273, 'batch_first': True, 'batch_size': 240, 'dropout': 0.15199999082336174, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 240.
Initializing LSTM with 15 layers, 121 hidden size, 0.0032504978823889273 learning rate, 0.15199999082336174 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:04
Starting tuning trial number #35 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 137, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 0.0008393189835987744, 'batch_first': True, 'batch_size': 140, 'dropout': 0.20347226562566167, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 140.
Initializing LSTM with 20 layers, 137 hidden size, 0.0008393189835987744 learning rate, 0.20347226562566167 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:05
Starting tuning trial number #36 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 366, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.002111437213366678, 'batch_first': True, 'batch_size': 277, 'dropout': 0.03574755130992616, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 7}
Converting dataset to dataloader using batch size 277.
Initializing LSTM with 25 layers, 366 hidden size, 0.002111437213366678 learning rate, 0.03574755130992616 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:07
Starting tuning trial number #37 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 157, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 0.0002076486497093818, 'batch_first': True, 'batch_size': 216, 'dropout': 0.06945685014912996, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 29}
Converting dataset to dataloader using batch size 216.
Initializing LSTM with 24 layers, 157 hidden size, 0.0002076486497093818 learning rate, 0.06945685014912996 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:08
Starting tuning trial number #38 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 34, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.0993917492387557, 'batch_first': True, 'batch_size': 252, 'dropout': 0.3542226566097857, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 13}
Converting dataset to dataloader using batch size 252.
Initializing LSTM with 17 layers, 34 hidden size, 0.0993917492387557 learning rate, 0.3542226566097857 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:09
Starting tuning trial number #39 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 56, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.028693071259302337, 'batch_first': True, 'batch_size': 184, 'dropout': 0.126540595594127, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 184.
Initializing LSTM with 7 layers, 56 hidden size, 0.028693071259302337 learning rate, 0.126540595594127 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:35:11
Ending timer at: 20:36:28 
Elapsed CPU time: 1052.2510571500031s
Used real time: 0:01:17
------------------
Starting tuning trial number #40 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 290, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 29, 'learning_rate': 0.012430675641180002, 'batch_first': True, 'batch_size': 312, 'dropout': 0.16922709528510294, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 76}
Converting dataset to dataloader using batch size 312.
Initializing LSTM with 29 layers, 290 hidden size, 0.012430675641180002 learning rate, 0.16922709528510294 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:29
Starting tuning trial number #41 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 34, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.005609483390288768, 'batch_first': True, 'batch_size': 218, 'dropout': 0.35503054306274245, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 218.
Initializing LSTM with 19 layers, 34 hidden size, 0.005609483390288768 learning rate, 0.35503054306274245 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:30
Starting tuning trial number #42 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 75, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.003756354917838993, 'batch_first': True, 'batch_size': 268, 'dropout': 0.3742035424329683, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 45}
Converting dataset to dataloader using batch size 268.
Initializing LSTM with 18 layers, 75 hidden size, 0.003756354917838993 learning rate, 0.3742035424329683 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:31
Starting tuning trial number #43 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 35, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.0016168178649443696, 'batch_first': True, 'batch_size': 156, 'dropout': 0.41078927175357116, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 156.
Initializing LSTM with 22 layers, 35 hidden size, 0.0016168178649443696 learning rate, 0.41078927175357116 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:32
Starting tuning trial number #44 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.05566945917581239, 'batch_first': True, 'batch_size': 191, 'dropout': 0.4517401451446177, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 191.
Initializing LSTM with 16 layers, 10 hidden size, 0.05566945917581239 learning rate, 0.4517401451446177 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:33
Starting tuning trial number #45 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 224, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 0.0006959283477525417, 'batch_first': True, 'batch_size': 226, 'dropout': 0.31841458973839265, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 38}
Converting dataset to dataloader using batch size 226.
Initializing LSTM with 23 layers, 224 hidden size, 0.0006959283477525417 learning rate, 0.31841458973839265 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:35
Starting tuning trial number #46 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 53, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 0.0046429004501197775, 'batch_first': True, 'batch_size': 355, 'dropout': 0.2809073386871425, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 355.
Initializing LSTM with 20 layers, 53 hidden size, 0.0046429004501197775 learning rate, 0.2809073386871425 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:36
Starting tuning trial number #47 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 102, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 1, 'learning_rate': 0.023932033726112103, 'batch_first': True, 'batch_size': 197, 'dropout': 0.031231452549277977, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 33}
Converting dataset to dataloader using batch size 197.
Initializing LSTM with 1 layers, 102 hidden size, 0.023932033726112103 learning rate, 0.031231452549277977 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:37
Starting tuning trial number #48 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 31, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.0098567626274865, 'batch_first': True, 'batch_size': 2, 'dropout': 0.3908716400395361, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 2.
Initializing LSTM with 4 layers, 31 hidden size, 0.0098567626274865 learning rate, 0.3908716400395361 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:36:38
Ending timer at: 20:38:28 
Elapsed CPU time: 1549.976602483332s
Used real time: 0:01:50
------------------
Starting tuning trial number #49 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 254, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 6.325904188294542e-05, 'batch_first': True, 'batch_size': 29, 'dropout': 0.3848271022331945, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 29.
Initializing LSTM with 4 layers, 254 hidden size, 6.325904188294542e-05 learning rate, 0.3848271022331945 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:38:29
Starting tuning trial number #50 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 318, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 2.05888898878462e-06, 'batch_first': True, 'batch_size': 37, 'dropout': 0.4931644349896047, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 11}
Converting dataset to dataloader using batch size 37.
Initializing LSTM with 2 layers, 318 hidden size, 2.05888898878462e-06 learning rate, 0.4931644349896047 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:38:30
Starting tuning trial number #51 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 26, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.002616366503200677, 'batch_first': True, 'batch_size': 115, 'dropout': 0.4363667688310077, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 115.
Initializing LSTM with 8 layers, 26 hidden size, 0.002616366503200677 learning rate, 0.4363667688310077 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:38:31
Starting tuning trial number #52 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 70, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.007967609544475708, 'batch_first': True, 'batch_size': 205, 'dropout': 0.3718615847810255, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 28}
Converting dataset to dataloader using batch size 205.
Initializing LSTM with 11 layers, 70 hidden size, 0.007967609544475708 learning rate, 0.3718615847810255 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:38:35
Starting tuning trial number #53 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 47, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.010633451285588952, 'batch_first': True, 'batch_size': 93, 'dropout': 0.34139390460224306, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 93.
Initializing LSTM with 5 layers, 47 hidden size, 0.010633451285588952 learning rate, 0.34139390460224306 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:38:37
Ending timer at: 20:39:58 
Elapsed CPU time: 1018.0346122166687s
Used real time: 0:01:21
------------------
Starting tuning trial number #54 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 28, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 3, 'learning_rate': 0.02197332637883641, 'batch_first': True, 'batch_size': 129, 'dropout': 0.38243884370434256, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 3 layers, 28 hidden size, 0.02197332637883641 learning rate, 0.38243884370434256 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:39:58
Starting tuning trial number #55 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 65, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.005142529442363614, 'batch_first': True, 'batch_size': 232, 'dropout': 0.25876123229599, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 24}
Converting dataset to dataloader using batch size 232.
Initializing LSTM with 14 layers, 65 hidden size, 0.005142529442363614 learning rate, 0.25876123229599 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:39:59
Starting tuning trial number #56 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 377, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.0166468549800328, 'batch_first': True, 'batch_size': 154, 'dropout': 0.2266222377004128, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 154.
Initializing LSTM with 19 layers, 377 hidden size, 0.0166468549800328 learning rate, 0.2266222377004128 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:40:01
Starting tuning trial number #57 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.045298387292872506, 'batch_first': True, 'batch_size': 213, 'dropout': 0.02030295488986303, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 36}
Converting dataset to dataloader using batch size 213.
Initializing LSTM with 21 layers, 46 hidden size, 0.045298387292872506 learning rate, 0.02030295488986303 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:40:02
Starting tuning trial number #58 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 90, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.0004933517682932654, 'batch_first': True, 'batch_size': 259, 'dropout': 0.10032409127992209, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 259.
Initializing LSTM with 25 layers, 90 hidden size, 0.0004933517682932654 learning rate, 0.10032409127992209 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:40:07
Starting tuning trial number #59 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 22, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.001054075805647376, 'batch_first': True, 'batch_size': 289, 'dropout': 0.4719985957542505, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 289.
Initializing LSTM with 17 layers, 22 hidden size, 0.001054075805647376 learning rate, 0.4719985957542505 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:40:08
Starting tuning trial number #60 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 118, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.0013846525689885703, 'batch_first': True, 'batch_size': 174, 'dropout': 0.3975593959622422, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 96}
Converting dataset to dataloader using batch size 174.
Initializing LSTM with 21 layers, 118 hidden size, 0.0013846525689885703 learning rate, 0.3975593959622422 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:40:13
Starting tuning trial number #61 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 12, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.04476456280750039, 'batch_first': True, 'batch_size': 159, 'dropout': 0.43534841251716566, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 159.
Initializing LSTM with 17 layers, 12 hidden size, 0.04476456280750039 learning rate, 0.43534841251716566 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:40:14
Ending timer at: 20:41:16 
Elapsed CPU time: 798.5622384333359s
Used real time: 0:01:02
------------------
Starting tuning trial number #62 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.03823405069410075, 'batch_first': True, 'batch_size': 11, 'dropout': 0.43505554729521956, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 31}
Converting dataset to dataloader using batch size 11.
Initializing LSTM with 15 layers, 37 hidden size, 0.03823405069410075 learning rate, 0.43505554729521956 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:41:16
Starting tuning trial number #63 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 13, 'learning_rate': 0.006535060728539034, 'batch_first': True, 'batch_size': 243, 'dropout': 0.42166078120386336, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 37}
Converting dataset to dataloader using batch size 243.
Initializing LSTM with 13 layers, 10 hidden size, 0.006535060728539034 learning rate, 0.42166078120386336 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:41:18
Starting tuning trial number #64 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 23, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 0.06931762071813403, 'batch_first': True, 'batch_size': 65, 'dropout': 0.4648857957968362, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 65.
Initializing LSTM with 23 layers, 23 hidden size, 0.06931762071813403 learning rate, 0.4648857957968362 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:41:21
Starting tuning trial number #65 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 61, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.010601362116169858, 'batch_first': True, 'batch_size': 197, 'dropout': 0.29957771798284877, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 44}
Converting dataset to dataloader using batch size 197.
Initializing LSTM with 18 layers, 61 hidden size, 0.010601362116169858 learning rate, 0.29957771798284877 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:41:22
Starting tuning trial number #66 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 39, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 0.030729720855443673, 'batch_first': True, 'batch_size': 223, 'dropout': 0.3400272097947676, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 34}
Converting dataset to dataloader using batch size 223.
Initializing LSTM with 20 layers, 39 hidden size, 0.030729720855443673 learning rate, 0.3400272097947676 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:41:27
Starting tuning trial number #67 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 194, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.002865020832073835, 'batch_first': True, 'batch_size': 177, 'dropout': 0.39437420507189175, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 83}
Converting dataset to dataloader using batch size 177.
Initializing LSTM with 9 layers, 194 hidden size, 0.002865020832073835 learning rate, 0.39437420507189175 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:41:28
Ending timer at: 20:43:27 
Elapsed CPU time: 1620.1881942499995s
Used real time: 0:01:59
------------------
Starting tuning trial number #68 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 349, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.009344216309082271, 'batch_first': True, 'batch_size': 249, 'dropout': 0.06777569356054336, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 249.
Initializing LSTM with 4 layers, 349 hidden size, 0.009344216309082271 learning rate, 0.06777569356054336 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:29
Starting tuning trial number #69 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 83, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 0.003838567610575371, 'batch_first': True, 'batch_size': 265, 'dropout': 0.36586123384513397, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 9}
Converting dataset to dataloader using batch size 265.
Initializing LSTM with 24 layers, 83 hidden size, 0.003838567610575371 learning rate, 0.36586123384513397 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:31
Starting tuning trial number #70 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 155, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.017319565413819824, 'batch_first': True, 'batch_size': 166, 'dropout': 0.24444821776370648, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 27}
Converting dataset to dataloader using batch size 166.
Initializing LSTM with 16 layers, 155 hidden size, 0.017319565413819824 learning rate, 0.24444821776370648 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:32
Starting tuning trial number #71 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 202, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 9, 'learning_rate': 0.0021511034008199477, 'batch_first': True, 'batch_size': 182, 'dropout': 0.42038706982408736, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 182.
Initializing LSTM with 9 layers, 202 hidden size, 0.0021511034008199477 learning rate, 0.42038706982408736 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:38
Starting tuning trial number #72 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 22, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 6, 'learning_rate': 0.0031921577948804346, 'batch_first': True, 'batch_size': 204, 'dropout': 0.38907225277870555, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 204.
Initializing LSTM with 6 layers, 22 hidden size, 0.0031921577948804346 learning rate, 0.38907225277870555 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:39
Starting tuning trial number #73 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 270, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 5, 'learning_rate': 0.006261090353560876, 'batch_first': True, 'batch_size': 184, 'dropout': 0.3194536776765972, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 88}
Converting dataset to dataloader using batch size 184.
Initializing LSTM with 5 layers, 270 hidden size, 0.006261090353560876 learning rate, 0.3194536776765972 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:40
Starting tuning trial number #74 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 201, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.023772465367388758, 'batch_first': True, 'batch_size': 135, 'dropout': 0.44946663509207985, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 81}
Converting dataset to dataloader using batch size 135.
Initializing LSTM with 19 layers, 201 hidden size, 0.023772465367388758 learning rate, 0.44946663509207985 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:43
Starting tuning trial number #75 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 48, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 8, 'learning_rate': 0.014396797631578983, 'batch_first': True, 'batch_size': 235, 'dropout': 0.3985389023487139, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 235.
Initializing LSTM with 8 layers, 48 hidden size, 0.014396797631578983 learning rate, 0.3985389023487139 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:44
Starting tuning trial number #76 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 105, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.06348011639228078, 'batch_first': True, 'batch_size': 150, 'dropout': 0.1462739468306231, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 150.
Initializing LSTM with 18 layers, 105 hidden size, 0.06348011639228078 learning rate, 0.1462739468306231 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:45
Starting tuning trial number #77 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 174, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 0.034398924984807046, 'batch_first': True, 'batch_size': 210, 'dropout': 0.0022184358753896405, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 210.
Initializing LSTM with 2 layers, 174 hidden size, 0.034398924984807046 learning rate, 0.0022184358753896405 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:46
Starting tuning trial number #78 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 74, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.002795491113721048, 'batch_first': True, 'batch_size': 171, 'dropout': 0.20360672043110262, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 171.
Initializing LSTM with 22 layers, 74 hidden size, 0.002795491113721048 learning rate, 0.20360672043110262 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:48
Starting tuning trial number #79 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 57, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 11, 'learning_rate': 0.0018758088400223636, 'batch_first': True, 'batch_size': 223, 'dropout': 0.05037118790730874, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 223.
Initializing LSTM with 11 layers, 57 hidden size, 0.0018758088400223636 learning rate, 0.05037118790730874 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:52
Starting tuning trial number #80 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 217, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 14, 'learning_rate': 0.001029425604204081, 'batch_first': True, 'batch_size': 98, 'dropout': 0.08683085957102017, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 98.
Initializing LSTM with 14 layers, 217 hidden size, 0.001029425604204081 learning rate, 0.08683085957102017 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:53
Starting tuning trial number #81 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 250, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 27, 'learning_rate': 0.004685148657442957, 'batch_first': True, 'batch_size': 310, 'dropout': 0.16939199025890714, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 15}
Converting dataset to dataloader using batch size 310.
Initializing LSTM with 27 layers, 250 hidden size, 0.004685148657442957 learning rate, 0.16939199025890714 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:54
Starting tuning trial number #82 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 135, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 25, 'learning_rate': 0.00851220397121052, 'batch_first': True, 'batch_size': 194, 'dropout': 0.1303170389107255, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 194.
Initializing LSTM with 25 layers, 135 hidden size, 0.00851220397121052 learning rate, 0.1303170389107255 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:43:55
Starting tuning trial number #83 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 20, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 26, 'learning_rate': 0.0026981259088007517, 'batch_first': True, 'batch_size': 281, 'dropout': 0.2736988204935864, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 281.
Initializing LSTM with 26 layers, 20 hidden size, 0.0026981259088007517 learning rate, 0.2736988204935864 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:02
Starting tuning trial number #84 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 34, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 7, 'learning_rate': 0.0014879280251854365, 'batch_first': True, 'batch_size': 268, 'dropout': 0.2113816557525201, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 268.
Initializing LSTM with 7 layers, 34 hidden size, 0.0014879280251854365 learning rate, 0.2113816557525201 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:03
Starting tuning trial number #85 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 191, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.0038985504172641726, 'batch_first': True, 'batch_size': 242, 'dropout': 0.18145214618412558, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 30}
Converting dataset to dataloader using batch size 242.
Initializing LSTM with 17 layers, 191 hidden size, 0.0038985504172641726 learning rate, 0.18145214618412558 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:04
Starting tuning trial number #86 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.006157169696948237, 'batch_first': True, 'batch_size': 119, 'dropout': 0.11207345978193776, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 21 layers, 41 hidden size, 0.006157169696948237 learning rate, 0.11207345978193776 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:06
Starting tuning trial number #87 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 399, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 4, 'learning_rate': 0.08720941074999294, 'batch_first': True, 'batch_size': 202, 'dropout': 0.23617653958042892, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 39}
Converting dataset to dataloader using batch size 202.
Initializing LSTM with 4 layers, 399 hidden size, 0.08720941074999294 learning rate, 0.23617653958042892 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:07
Starting tuning trial number #88 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 238, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 24, 'learning_rate': 0.0003936703284802267, 'batch_first': True, 'batch_size': 331, 'dropout': 0.35681929889764047, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 331.
Initializing LSTM with 24 layers, 238 hidden size, 0.0003936703284802267 learning rate, 0.35681929889764047 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:08
Starting tuning trial number #89 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 29, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 20, 'learning_rate': 0.00011917896955420608, 'batch_first': True, 'batch_size': 190, 'dropout': 0.41463387690495324, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 46}
Converting dataset to dataloader using batch size 190.
Initializing LSTM with 20 layers, 29 hidden size, 0.00011917896955420608 learning rate, 0.41463387690495324 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:09
Starting tuning trial number #90 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 316, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 23, 'learning_rate': 1.6321343365692882e-05, 'batch_first': True, 'batch_size': 177, 'dropout': 0.3343115226716408, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 35}
Converting dataset to dataloader using batch size 177.
Initializing LSTM with 23 layers, 316 hidden size, 1.6321343365692882e-05 learning rate, 0.3343115226716408 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:14
Starting tuning trial number #91 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 19, 'learning_rate': 0.09322007662984215, 'batch_first': True, 'batch_size': 251, 'dropout': 0.15944301806289524, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 251.
Initializing LSTM with 19 layers, 54 hidden size, 0.09322007662984215 learning rate, 0.15944301806289524 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:16
Starting tuning trial number #92 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 16, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 17, 'learning_rate': 0.05241545457818948, 'batch_first': True, 'batch_size': 219, 'dropout': 0.18332919866351766, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 219.
Initializing LSTM with 17 layers, 16 hidden size, 0.05241545457818948 learning rate, 0.18332919866351766 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:17
Ending timer at: 20:44:55 
Elapsed CPU time: 497.64365823333486s
Used real time: 0:00:38
------------------
Starting tuning trial number #93 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 71, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 16, 'learning_rate': 0.01352059173905857, 'batch_first': True, 'batch_size': 232, 'dropout': 0.020371465528845466, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 232.
Initializing LSTM with 16 layers, 71 hidden size, 0.01352059173905857 learning rate, 0.020371465528845466 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:55
Starting tuning trial number #94 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 18, 'learning_rate': 0.021241401477103242, 'batch_first': True, 'batch_size': 258, 'dropout': 0.13750127184030753, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 258.
Initializing LSTM with 18 layers, 46 hidden size, 0.021241401477103242 learning rate, 0.13750127184030753 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:56
Starting tuning trial number #95 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 29, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 22, 'learning_rate': 0.02723116473202519, 'batch_first': True, 'batch_size': 211, 'dropout': 0.3779309531477082, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 32}
Converting dataset to dataloader using batch size 211.
Initializing LSTM with 22 layers, 29 hidden size, 0.02723116473202519 learning rate, 0.3779309531477082 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:58
Starting tuning trial number #96 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 296, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 28, 'learning_rate': 0.06326675179830804, 'batch_first': True, 'batch_size': 292, 'dropout': 0.2886141898506302, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 13}
Converting dataset to dataloader using batch size 292.
Initializing LSTM with 28 layers, 296 hidden size, 0.06326675179830804 learning rate, 0.2886141898506302 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:44:59
Starting tuning trial number #97 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 131, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 15, 'learning_rate': 0.005221956981944133, 'batch_first': True, 'batch_size': 55, 'dropout': 0.19765331824525345, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 55.
Initializing LSTM with 15 layers, 131 hidden size, 0.005221956981944133 learning rate, 0.19765331824525345 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:45:00
Starting tuning trial number #98 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 161, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 21, 'learning_rate': 0.040530730153131925, 'batch_first': True, 'batch_size': 274, 'dropout': 0.4853113043529581, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 22}
Converting dataset to dataloader using batch size 274.
Initializing LSTM with 21 layers, 161 hidden size, 0.040530730153131925 learning rate, 0.4853113043529581 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:45:01
Starting tuning trial number #99 of total 100
with params: {'number_of_features': 1, 'hidden_layer_size': 92, 'input_window_size': 10, 'output_window_size': 7, 'number_of_layers': 2, 'learning_rate': 0.0007407072947544876, 'batch_first': True, 'batch_size': 164, 'dropout': 0.36240757855141575, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 6}
Converting dataset to dataloader using batch size 164.
Initializing LSTM with 2 layers, 92 hidden size, 0.0007407072947544876 learning rate, 0.36240757855141575 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 20:45:03
Best params!
Params updated with best params
Initializing LSTM with 22 layers, 42 hidden size, 0.001746633171583667 learning rate, 0.08681814071708832 dropout. Optimiser: Adam,  input window size: 10, output window size: 7
Using pre-existing neptune run for PytorchLightning
Best trial: 11573,1
best_score: 0.004025255795568228
best_params: {'batch_size': 192, 'dropout': 0.08681814071708832, 'hidden_layer_size': 42, 'learning_rate': 0.001746633171583667, 'number_of_epochs': 61, 'number_of_layers': 22, 'optimizer_name': 'Adam'}
Training model
Testing model
Testing error: {'MASE': 0.7602750658988953, 'SMAPE': 0.06310375034809113, 'MSE': 0.0040577726904302835, 'MAE': 0.05648663453757763}.
Saving model
Finished
