\section{Statistical methods VS Neural Nets}
If a time series is stationary, then using its statistical properties is shown to be an effective and computationally cheap method 
\cite{Makridakis2018}.
In a paper written by \citeauthor{Makridakis2018} they test statistical methods versus machine learning methods for 
forecasting time series.
They evaluate perfomance across multiple forecasting horizons using a large subset of 1045 montly time seires.
Among the the machine learning methods they evaluate are 
\begin{itemize}
  \item \textit{Bayesian Neural Network (BNN)}
  \item \textit{Multi-Layer Perceptron (MLP)}
  \item \textit{K-Nearest Neighbor regression (KNN)}
  \item \textit{Recurrent Neural Network (RNN)} and
  \item \textit{Long Short Term Memory neural network (LSTM)}.
\end{itemize}

Among the statistical methods evaluated are
\begin{itemize}
  \item \textit{Naive2}
  \item \textit{ETS}
  \item \textit{ARIMA}
\end{itemize}
They evaluate with different loss functions and metrics, with simple one-step-ahead and multiple steps ahead methods.

Their findings are that all the simple statistical methods outperformed all the ML methods in terms of accuracy.
The statistical methods also had a lot less model complexity and computational cost for training.

\todo[]{Define sMAPE?}
The best performing model was ETS with a sMAPE score of 7.12. The second-best model was ARIMA with a score
of 7.19. The third worst model was LSTM with a score of 11.67.

The paper \cite{Makridakis2018} highlights some of the drawbacks of ML methods. Especially, their complexity,
their lack of explainability and their inability to show certainty in their predictions.
However, the study has gotten some well-defined critic. The authors \citeauthor*{Cerqueira2019} points 
out that \cite{Makridakis2018} does their experiments on datasets of too-small sample size.
Their largest time series sample size among the 1045 datasets is 144, and their smallest is 118.
They hypothesize that these datasets are too small for machine learning methods to generalize properly.

Their contribution is doing a similar study on 90 univariate time series, in which 
all the datasets have a sample size above 1000. They evaluate statistical methods vs ML methods
at different sample sizes, to test weather the sample size matters.

The paper \cite{Cerqueira2019} concludes that sample size does matter a whole lot for ML methods.
Statistical methods outperformed ML methods up to around a sample size of 130. After that ML methods
in general outperformed the simpler methods.

One drawback of the \cite{Makridakis2018} study is that it does not test any of our choicen ML methods.

\begin{itemize}
  \item \textit{A rule-based model (RBR)}
  \item \textit{A random Forest method (RF)}
  \item \textit{Guassion Process regression (GP)}
  \item \textit{The Multivariate adative regression (MARS)}
  \item \textit{Generalized linear model (GLM)}
\end{itemize}

% Skal refleksjon mot v√•r situasjon diskuteres her?
This is good news for us, as our datasets does have a sample size of well above 660 at this moment.
Depending on when we conduct the experiment, the number will grow to the range of 1095 to 1860,
as data is being gathered every day as we speak.

The paper \citet{Bandara2019} points out a that in non-stationary time series, the distant past is typically less
useful for forecast, as underlying patterns and relationships will have changed in the meantime.


\subsection{Univariate vs Multivariate time series}


\subsubsection{Limitations of statistical methods}
\todo[inline]{Skrive om limitations til statistiske metoder and
* Univariate
* stationary time series
* Dealing with extreme values
https://towardsdatascience.com/limitations-of-arima-dealing-with-outliers-30cc0c6ddf33
}
Pro ML: size matters \cite{Cerqueira2019}

From \cite{Guen2019}:
Traditional methods for time series forecasting include linear autoregressive models, such as
ARIMA odel, and exponential smoothing, which both fall into the broad category of of linear 
state space models (SSMs). These methods handle linear
dynamics and stationary time series (or made stationary by temporal differences).
However the stationarity assumption is not satisfied for many real world tmie series that can present
abrupt changes of distribution...