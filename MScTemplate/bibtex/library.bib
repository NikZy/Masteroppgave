Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Hewamalage2021,
abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.},
archivePrefix = {arXiv},
arxivId = {1909.00590},
author = {Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
doi = {10.1016/j.ijforecast.2020.06.008},
eprint = {1909.00590},
file = {::},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Best practices,Big data,Forecasting,Framework},
month = {jan},
number = {1},
pages = {388--427},
publisher = {Elsevier},
title = {{Recurrent Neural Networks for Time Series Forecasting: Current status and future directions}},
volume = {37},
year = {2021}
}
@article{Rabanser2020,
abstract = {Time series modeling techniques based on deep learning have seen many advancements in recent years, especially in data-abundant settings and with the central aim of learning global models that can extract patterns across multiple time series. While the crucial importance of appropriate data pre-processing and scaling has often been noted in prior work, most studies focus on improving model architectures. In this paper we empirically investigate the effect of data input and output transformations on the predictive performance of several neural forecasting architectures. In particular, we investigate the effectiveness of several forms of data binning, i.e. converting real-valued time series into categorical ones, when combined with feed-forward, recurrent neural networks, and convolution-based sequence models. In many non-forecasting applications where these models have been very successful, the model inputs and outputs are categorical (e.g. words from a fixed vocabulary in natural language processing applications or quantized pixel color intensities in computer vision). For forecasting applications, where the time series are typically real-valued, various ad-hoc data transformations have been proposed, but have not been systematically compared. To remedy this, we evaluate the forecasting accuracy of instances of the aforementioned model classes when combined with different types of data scaling and binning. We find that binning almost always improves performance (compared to using normalized real-valued inputs), but that the particular type of binning chosen is of lesser importance.},
archivePrefix = {arXiv},
arxivId = {2005.10111},
author = {Rabanser, Stephan and Januschowski, Tim and Flunkert, Valentin and Salinas, David and Gasthaus, Jan},
eprint = {2005.10111},
file = {:home/archie/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rabanser et al. - 2020 - The Effectiveness of Discretization in Forecasting An Empirical Study on Neural Time Series Models(2).pdf:pdf},
issn = {2331-8422},
month = {may},
title = {{The Effectiveness of Discretization in Forecasting: An Empirical Study on Neural Time Series Models}},
url = {http://arxiv.org/abs/2005.10111},
year = {2020}
}
@misc{Hayes,
author = {Kenton, Will},
booktitle = {Investopedia},
title = {{Time Series Definition}},
url = {https://www.investopedia.com/terms/t/timeseries.asp},
urldate = {2021-10-27},
year = {2020}
}
@book{Utlaut2008,
abstract = {applicability for this approach.},
author = {Utlaut, Theresa L.},
booktitle = {Journal of Quality Technology},
doi = {10.1080/00224065.2008.11917751},
isbn = {978-0-471-65397-4},
issn = {0022-4065},
number = {4},
pages = {476--478},
publisher = {Wiley-Interscience},
title = {{Introduction to Time Series Analysis and Forecasting}},
volume = {40},
year = {2008}
}
@article{Guen2019,
abstract = {This paper addresses the problem of time series forecasting for
non-stationary signals and multiple future steps prediction. To handle this
challenging task, we introduce DILATE (DIstortion Loss including shApe and
TimE), a new objective function for training deep neural networks. DILATE aims
at accurately predicting sudden changes, and explicitly incorporates two terms
supporting precise shape and temporal change detection. We introduce a
differentiable loss function suitable for training deep neural nets, and
provide a custom back-prop implementation for speeding up optimization. We also
introduce a variant of DILATE, which provides a smooth generalization of
temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on
various non-stationary datasets reveal the very good behaviour of DILATE
compared to models trained with the standard Mean Squared Error (MSE) loss
function, and also to DTW and variants. DILATE is also agnostic to the choice
of the model, and we highlight its benefit for training fully connected
networks as well as specialized recurrent architectures, showing its capacity
to improve over state-of-the-art trajectory forecasting approaches.},
archivePrefix = {arXiv},
arxivId = {1909.09020},
author = {Guen, Vincent Le and Thome, Nicolas},
eprint = {1909.09020},
file = {::},
journal = {Advances in Neural Information Processing Systems},
month = {sep},
publisher = {Neural information processing systems foundation},
title = {{Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models}},
url = {https://arxiv.org/abs/1909.09020v4},
volume = {32},
year = {2019}
}
@article{Bandara2019,
abstract = {Generating accurate and reliable sales forecasts is crucial in the E-commerce business. The current state-of-the-art techniques are typically univariate methods, which produce forecasts considering only the historical sales data of a single product. However, in a situation where large quantities of related time series are available, conditioning the forecast of an individual time series on past behaviour of similar, related time series can be beneficial. Since the product assortment hierarchy in an E-commerce platform contains large numbers of related products, in which the sales demand patterns can be correlated, our attempt is to incorporate this cross-series information in a unified model. We achieve this by globally training a Long Short-Term Memory network (LSTM) that exploits the non-linear demand relationships available in an E-commerce product assortment hierarchy. Aside from the forecasting framework, we also propose a systematic pre-processing framework to overcome the challenges in the E-commerce business. We also introduce several product grouping strategies to supplement the LSTM learning schemes, in situations where sales patterns in a product portfolio are disparate. We empirically evaluate the proposed forecasting framework on a real-world online marketplace dataset from Walmart.com. Our method achieves competitive results on category level and super-departmental level datasets, outperforming state-of-the-art techniques.},
author = {Bandara, Kasun and Shi, Peibei and Bergmeir, Christoph and Hewamalage, Hansika and Tran, Quoc and Seaman, Brian},
doi = {10.1007/978-3-030-36718-3_39},
file = {::},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Demand forecasting,E-commerce,LSTM,Time series},
pages = {462--474},
publisher = {Springer},
title = {{Sales demand forecast in E-commerce using a long short-term memory neural network methodology}},
url = {https://www.webofscience.com/wos/woscc/full-record/WOS:000612961500039},
volume = {11955 LNCS},
year = {2019}
}
@article{Jiang2021a,
abstract = {Using time series data to predict future sales changes of products is of great significance to every retailing company in terms of management and planning of resources. In order to find an effective method to improve the accuracy of sales forecasting of retail goods which strongly influenced by season and holiday, this paper analyzes the feasibility of traditional time series model, hybrid models based on time series model and machine learning model, and machine learning model in predicting Walmart sales. The Prophet model which decomposes trend, season, and holiday and the machine learning model-lightGBM model-are used to train and test Walmart supermarket sales data from 2011-01-29 to 2016-06-19, and use data from 2016-06-19 to 2016-08-14 to make prediction and empirical analysis. The results suggest that the Root Mean Square Error (RMSE) of the Prophet model and the LightLGB model are 0.694 and 0.617, respectively, indicating that the machine learning model performs well in the sales forecast of retail stores. This provides a new idea for retailers to forecast sales by category and region.},
author = {Jiang, Haichen and Ruan, Jiatong and Sun, Jianmin},
doi = {10.1109/ICBDA51983.2021.9403224},
journal = {2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021},
keywords = {LightGBM model,Machine learning,Prophet model,Sales forecast,Time series model},
month = {mar},
pages = {69--75},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Application of Machine Learning Model and Hybrid Model in Retail Sales Forecast}},
year = {2021}
}
@article{Montero-Manso2021,
abstract = {Global methods that fit a single forecasting method to all time series in a set have recently shown surprising accuracy, even when forecasting large groups of heterogeneous time series. We provide the following contributions that help understand the potential and applicability of global methods and how they relate to traditional local methods that fit a separate forecasting method to each series: • Global and local methods can produce the same forecasts without any assumptions about similarity of the series in the set. • The complexity of local methods grows with the size of the set while it remains constant for global methods. This result supports the recent evidence and provides principles for the design of new algorithms. • In an extensive empirical study, we show that purposely na{\"{i}}ve algorithms derived from these principles show outstanding accuracy. In particular, global linear models provide competitive accuracy with far fewer parameters than the simplest of local methods.},
author = {Montero-Manso, Pablo and Hyndman, Rob J.},
doi = {10.1016/J.IJFORECAST.2021.03.004},
file = {::},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Cross-learning,Forecasting,Generalization,Global,Local,Pooled regression,Time series},
month = {oct},
number = {4},
pages = {1632--1653},
publisher = {Elsevier},
title = {{Principles and algorithms for forecasting groups of time series: Locality and globality}},
volume = {37},
year = {2021}
}
@article{Makridakis2018,
abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1371/JOURNAL.PONE.0194889},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {Algorithms,Artificial intelligence,Computing methods,Forecasting,Neural networks,Preprocessing,Statistical methods,Support vector machines},
month = {mar},
number = {3},
pages = {e0194889},
publisher = {Public Library of Science},
title = {{Statistical and Machine Learning forecasting methods: Concerns and ways forward}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
volume = {13},
year = {2018}
}
@article{Cerqueira2019,
abstract = {Time series forecasting is one of the most active research topics. Machine learning methods have been increasingly adopted to solve these predictive tasks. However, in a recent work, these were shown to systematically present a lower predictive performance relative to simple statistical methods. In this work, we counter these results. We show that these are only valid under an extremely low sample size. Using a learning curve method, our results suggest that machine learning methods improve their relative predictive performance as the sample size grows. The code to reproduce the experiments is available at https://github.com/vcerqueira/MLforForecasting.},
archivePrefix = {arXiv},
arxivId = {1909.13316},
author = {Cerqueira, Vitor and Torgo, Luis and Soares, Carlos},
eprint = {1909.13316},
issn = {2331-8422},
month = {sep},
title = {{Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters}},
url = {http://arxiv.org/abs/1909.13316},
year = {2019}
}
@book{Box2016,
abstract = {Stochastic models and their forecasting. The autocorrelation function and spectrum. Linear stationary models. Linear nonstationary models. Forecasting. Stochastic model building. Model estimation. Model diagnostic checking. Seasonal models. Transfer function model building. Transfer function models. Identification, fitting, and checking of transfer function models. Design of discrete control schemes. Design of feedforward and feedback control schemes. Some further problems in control.},
author = {Ziegel, Eric R. and Box, G. and Jenkins, G. and Reinsel, G.},
booktitle = {Technometrics},
doi = {10.2307/1269640},
isbn = {978-1-118-67502-1},
issn = {00401706},
number = {2},
pages = {238},
publisher = {John Wiley {\&} Sons},
title = {{Time Series Analysis, Forecasting, and Control}},
volume = {37},
year = {1995}
}
@book{Geron2017,
abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. Using concrete examples, minimal theory, and two production-ready Python frameworks—scikit-learn and TensorFlow—author Aur{\'{e}}lien G{\'{e}}ron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks.},
author = {G{\'{e}}ron, Aur{\'{e}}lien},
edition = {1},
isbn = {9781491962299},
publisher = {O'Reilly Media},
title = {{Hands-On Machine Learning with Scikit-Learn and TensorFlow}},
year = {2017}
}
@article{Bandara2017,
abstract = {With the advent of Big Data, nowadays in many applications databases
containing large quantities of similar time series are available. Forecasting
time series in these domains with traditional univariate forecasting procedures
leaves great potentials for producing accurate forecasts untapped. Recurrent
neural networks (RNNs), and in particular Long Short-Term Memory (LSTM)
networks, have proven recently that they are able to outperform
state-of-the-art univariate time series forecasting methods in this context
when trained across all available time series. However, if the time series
database is heterogeneous, accuracy may degenerate, so that on the way towards
fully automatic forecasting methods in this space, a notion of similarity
between the time series needs to be built into the methods. To this end, we
present a prediction model that can be used with different types of RNN models
on subgroups of similar time series, which are identified by time series
clustering techniques. We assess our proposed methodology using LSTM networks,
a widely popular RNN variant. Our method achieves competitive results on
benchmarking datasets under competition evaluation procedures. In particular,
in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM
model and outperforms all other methods on the CIF2016 forecasting competition
dataset.},
archivePrefix = {arXiv},
arxivId = {1710.03222},
author = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
eprint = {1710.03222},
file = {::},
journal = {Expert Systems with Applications},
keywords = {Big data forecasting,LSTM,Neural networks,RNN,Time series clustering},
month = {oct},
publisher = {Elsevier Ltd},
title = {{Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach}},
url = {https://arxiv.org/abs/1710.03222v2},
volume = {140},
year = {2017}
}
@phdthesis{Ding2019,
abstract = {Time series prediction is an intensively studied topic in data mining. In spite of the considerable improvements, recent deep learning-based methods overlook the existence of extreme events, which result in weak performance when applying them to real time series. Extreme events are rare and random, but do play a critical role in many real applications, such as the forecasting of financial crisis and natural disasters. In this paper, we explore the central theme of improving the ability of deep learning on modeling extreme events for time series prediction. Through the lens of formal analysis, we first find that the weakness of deep learning methods roots in the conventional form of quadratic loss. To address this issue, we take inspirations from the Extreme Value Theory, developing a new form of loss called Extreme Value Loss (EVL) for detecting the future occurrence of extreme events. Furthermore, we propose to employ Memory Network in order to memorize extreme events in historical records. By incorporating EVL with an adapted memory network module, we achieve an end-to-end framework for time series prediction with extreme events. Through extensive experiments on synthetic data and two real datasets of stock and climate, we empirically validate the effectiveness of our framework. Besides, we also provide a proper choice for hyper-parameters in our proposed framework by conducting several additional experiments.},
author = {Ding, Daizong and Zhang, Mi and Pan, Xudong and Yang, Min and He, Xiangnan},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330896},
isbn = {9781450362016},
keywords = {Attention Model,DNN,Extreme Event,Extreme Events,Loss function,Memory Network,Time Series},
mendeley-tags = {DNN,Extreme Events,Loss function,Time Series},
pages = {1114--1122},
title = {{Modeling extreme events in time series prediction}},
url = {https://raw.githubusercontent.com/NikZy/tdt99-2020/master/Topic 2/Modeling Extreme Events in Time Series Prediction.pdf},
year = {2019}
}
@book{RobJHyndman2014,
abstract = {1. Guru: I wrote the book, done it for decades, now I do the conference circuit. 2. Expert: It has been my full time job for more than a decade. 3. Skilled: I have been doing it for years. 4. Comfortable: I understand it and have done it. 5. Learner: I am still learning. 6. Beginner: I have heard of it and would like to learn more. 7. Unknown: What is forecasting? Is that what the weather people do?},
author = {{Rob J Hyndman}},
isbn = {9780987507105},
number = {September},
pages = {138},
title = {{Forecasting: Forecasting: Principles {\&} Practice}},
url = {robjhyndman.com/uwa{\%}5Cnhttp://robjhyndman.com/papers/forecasting-age-specific-breast-cancer-mortality-using-functional-data-models/},
year = {2014}
}
