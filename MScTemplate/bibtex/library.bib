Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Cerqueira2019,
abstract = {Time series forecasting is one of the most active research topics. Machine learning methods have been increasingly adopted to solve these predictive tasks. However, in a recent work, these were shown to systematically present a lower predictive performance relative to simple statistical methods. In this work, we counter these results. We show that these are only valid under an extremely low sample size. Using a learning curve method, our results suggest that machine learning methods improve their relative predictive performance as the sample size grows. The code to reproduce the experiments is available at https://github.com/vcerqueira/MLforForecasting.},
archivePrefix = {arXiv},
arxivId = {1909.13316},
author = {Cerqueira, Vitor and Torgo, Luis and Soares, Carlos},
eprint = {1909.13316},
issn = {2331-8422},
month = {sep},
title = {{Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters}},
url = {http://arxiv.org/abs/1909.13316},
year = {2019}
}
@book{Box2016,
abstract = {Stochastic models and their forecasting. The autocorrelation function and spectrum. Linear stationary models. Linear nonstationary models. Forecasting. Stochastic model building. Model estimation. Model diagnostic checking. Seasonal models. Transfer function model building. Transfer function models. Identification, fitting, and checking of transfer function models. Design of discrete control schemes. Design of feedforward and feedback control schemes. Some further problems in control.},
author = {Ziegel, Eric R. and Box, G. and Jenkins, G. and Reinsel, G.},
booktitle = {Technometrics},
doi = {10.2307/1269640},
isbn = {978-1-118-67502-1},
issn = {00401706},
number = {2},
pages = {238},
publisher = {John Wiley {\&} Sons},
title = {{Time Series Analysis, Forecasting, and Control}},
volume = {37},
year = {1995}
}
@phdthesis{Ding2019,
abstract = {Time series prediction is an intensively studied topic in data mining. In spite of the considerable improvements, recent deep learning-based methods overlook the existence of extreme events, which result in weak performance when applying them to real time series. Extreme events are rare and random, but do play a critical role in many real applications, such as the forecasting of financial crisis and natural disasters. In this paper, we explore the central theme of improving the ability of deep learning on modeling extreme events for time series prediction. Through the lens of formal analysis, we first find that the weakness of deep learning methods roots in the conventional form of quadratic loss. To address this issue, we take inspirations from the Extreme Value Theory, developing a new form of loss called Extreme Value Loss (EVL) for detecting the future occurrence of extreme events. Furthermore, we propose to employ Memory Network in order to memorize extreme events in historical records. By incorporating EVL with an adapted memory network module, we achieve an end-to-end framework for time series prediction with extreme events. Through extensive experiments on synthetic data and two real datasets of stock and climate, we empirically validate the effectiveness of our framework. Besides, we also provide a proper choice for hyper-parameters in our proposed framework by conducting several additional experiments.},
author = {Ding, Daizong and Zhang, Mi and Pan, Xudong and Yang, Min and He, Xiangnan},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330896},
isbn = {9781450362016},
keywords = {Attention Model,DNN,Extreme Event,Extreme Events,Loss function,Memory Network,Time Series},
mendeley-tags = {DNN,Extreme Events,Loss function,Time Series},
pages = {1114--1122},
title = {{Modeling extreme events in time series prediction}},
url = {https://raw.githubusercontent.com/NikZy/tdt99-2020/master/Topic 2/Modeling Extreme Events in Time Series Prediction.pdf},
year = {2019}
}
@book{Geron2017,
abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. Using concrete examples, minimal theory, and two production-ready Python frameworks—scikit-learn and TensorFlow—author Aur{\'{e}}lien G{\'{e}}ron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks.},
author = {G{\'{e}}ron, Aur{\'{e}}lien},
edition = {1},
isbn = {9781491962299},
publisher = {O'Reilly Media},
title = {{Hands-On Machine Learning with Scikit-Learn and TensorFlow}},
year = {2017}
}
@article{Guen2019,
abstract = {This paper addresses the problem of time series forecasting for
non-stationary signals and multiple future steps prediction. To handle this
challenging task, we introduce DILATE (DIstortion Loss including shApe and
TimE), a new objective function for training deep neural networks. DILATE aims
at accurately predicting sudden changes, and explicitly incorporates two terms
supporting precise shape and temporal change detection. We introduce a
differentiable loss function suitable for training deep neural nets, and
provide a custom back-prop implementation for speeding up optimization. We also
introduce a variant of DILATE, which provides a smooth generalization of
temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on
various non-stationary datasets reveal the very good behaviour of DILATE
compared to models trained with the standard Mean Squared Error (MSE) loss
function, and also to DTW and variants. DILATE is also agnostic to the choice
of the model, and we highlight its benefit for training fully connected
networks as well as specialized recurrent architectures, showing its capacity
to improve over state-of-the-art trajectory forecasting approaches.},
archivePrefix = {arXiv},
arxivId = {1909.09020},
author = {Guen, Vincent Le and Thome, Nicolas},
eprint = {1909.09020},
file = {::},
journal = {Advances in Neural Information Processing Systems},
month = {sep},
publisher = {Neural information processing systems foundation},
title = {{Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models}},
url = {https://arxiv.org/abs/1909.09020v4},
volume = {32},
year = {2019}
}
@article{Bandara2019,
abstract = {Generating accurate and reliable sales forecasts is crucial in the E-commerce business. The current state-of-the-art techniques are typically univariate methods, which produce forecasts considering only the historical sales data of a single product. However, in a situation where large quantities of related time series are available, conditioning the forecast of an individual time series on past behaviour of similar, related time series can be beneficial. Since the product assortment hierarchy in an E-commerce platform contains large numbers of related products, in which the sales demand patterns can be correlated, our attempt is to incorporate this cross-series information in a unified model. We achieve this by globally training a Long Short-Term Memory network (LSTM) that exploits the non-linear demand relationships available in an E-commerce product assortment hierarchy. Aside from the forecasting framework, we also propose a systematic pre-processing framework to overcome the challenges in the E-commerce business. We also introduce several product grouping strategies to supplement the LSTM learning schemes, in situations where sales patterns in a product portfolio are disparate. We empirically evaluate the proposed forecasting framework on a real-world online marketplace dataset from Walmart.com. Our method achieves competitive results on category level and super-departmental level datasets, outperforming state-of-the-art techniques.},
author = {Bandara, Kasun and Shi, Peibei and Bergmeir, Christoph and Hewamalage, Hansika and Tran, Quoc and Seaman, Brian},
doi = {10.1007/978-3-030-36718-3_39},
file = {::},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Demand forecasting,E-commerce,LSTM,Time series},
pages = {462--474},
publisher = {Springer},
title = {{Sales demand forecast in E-commerce using a long short-term memory neural network methodology}},
url = {https://www.webofscience.com/wos/woscc/full-record/WOS:000612961500039},
volume = {11955 LNCS},
year = {2019}
}
@misc{Hayes,
author = {Kenton, Will},
booktitle = {Investopedia},
title = {{Time Series Definition}},
url = {https://www.investopedia.com/terms/t/timeseries.asp},
urldate = {2021-10-27},
year = {2020}
}
@article{Jiang2021a,
abstract = {Using time series data to predict future sales changes of products is of great significance to every retailing company in terms of management and planning of resources. In order to find an effective method to improve the accuracy of sales forecasting of retail goods which strongly influenced by season and holiday, this paper analyzes the feasibility of traditional time series model, hybrid models based on time series model and machine learning model, and machine learning model in predicting Walmart sales. The Prophet model which decomposes trend, season, and holiday and the machine learning model-lightGBM model-are used to train and test Walmart supermarket sales data from 2011-01-29 to 2016-06-19, and use data from 2016-06-19 to 2016-08-14 to make prediction and empirical analysis. The results suggest that the Root Mean Square Error (RMSE) of the Prophet model and the LightLGB model are 0.694 and 0.617, respectively, indicating that the machine learning model performs well in the sales forecast of retail stores. This provides a new idea for retailers to forecast sales by category and region.},
author = {Jiang, Haichen and Ruan, Jiatong and Sun, Jianmin},
doi = {10.1109/ICBDA51983.2021.9403224},
journal = {2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021},
keywords = {LightGBM model,Machine learning,Prophet model,Sales forecast,Time series model},
month = {mar},
pages = {69--75},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Application of Machine Learning Model and Hybrid Model in Retail Sales Forecast}},
year = {2021}
}
@book{Utlaut2008,
abstract = {applicability for this approach.},
author = {Utlaut, Theresa L.},
booktitle = {Journal of Quality Technology},
doi = {10.1080/00224065.2008.11917751},
isbn = {978-0-471-65397-4},
issn = {0022-4065},
number = {4},
pages = {476--478},
publisher = {Wiley-Interscience},
title = {{Introduction to Time Series Analysis and Forecasting}},
volume = {40},
year = {2008}
}
@article{Makridakis2018,
abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1371/JOURNAL.PONE.0194889},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {Algorithms,Artificial intelligence,Computing methods,Forecasting,Neural networks,Preprocessing,Statistical methods,Support vector machines},
month = {mar},
number = {3},
pages = {e0194889},
publisher = {Public Library of Science},
title = {{Statistical and Machine Learning forecasting methods: Concerns and ways forward}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
volume = {13},
year = {2018}
}
@book{RobJHyndman2014,
abstract = {1. Guru: I wrote the book, done it for decades, now I do the conference circuit. 2. Expert: It has been my full time job for more than a decade. 3. Skilled: I have been doing it for years. 4. Comfortable: I understand it and have done it. 5. Learner: I am still learning. 6. Beginner: I have heard of it and would like to learn more. 7. Unknown: What is forecasting? Is that what the weather people do?},
author = {{Rob J Hyndman}},
isbn = {9780987507105},
number = {September},
pages = {138},
title = {{Forecasting: Forecasting: Principles {\&} Practice}},
url = {robjhyndman.com/uwa{\%}5Cnhttp://robjhyndman.com/papers/forecasting-age-specific-breast-cancer-mortality-using-functional-data-models/},
year = {2014}
}
