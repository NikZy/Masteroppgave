Logger initialized. Log level: INFO, log file: ./log_file-cat-6-cont2.log
Started
Continues previous experiment

Continue tuning from prev experiment

Experiment title: lstm-tune-category-6
Description: lstm-predict-cat-6
Loaded preview Neptune Experiment run: MAS-311 from checkpoint
Neptune run URL: https://app.neptune.ai/sjsivertandsanderkk/Masteroppgave/e/MAS-311
Choosing model structure: <src.model_strutures.local_univariate_lstm_structure.LocalUnivariateLstmStructure object at 0x154a803216a0>
Running model on device: cuda
Initializing LSTM with 3 layers, 268 hidden size, 0.0003 learning rate, 0.113 dropout. Optimiser: AdaDelta,  input window size: 5, output window size: 1
Using pre-existing neptune run for PytorchLightning
data preprocessing steps: 
 ---- Start ----
1- load market insight data and categories and merge them
2- convert date columns to date_time format
3- sum up clicks to category level [groupBy(date, cat_id)]
4- rename column 'title' to 'cat_name'
5- combine feature 'hits' and 'clicks' to new feature 'interest'
6- drop columns 'hits' and 'clicks'
7- filter out data from early 2018-12-01
8- drop uninteresting colums: ['id_x', 'manufacturer_id', '_version_', 'id_y', 'internal_doc_id_x', 'internal_doc_id_y']
---- End ----
        

------------------
Starting new timer at 15:22:29
Data Pipeline for 6: ---- Start ----
1- Convert input dataset to generator object
2- filter out category 6)
3- choose columns 'interest' and 'date'
4- fill in dates with zero values
5- convert to np.array
6- scale data between -1 and 1
7- split up into training set (0.9) and test set (0.09999999999999998)
8- split training set into train set (0.9) and validation set (0.09999999999999998)
9- convert to timeseries dataset with input window size of 5, and output window size of 1
---- End ----
        
Converting dataset to dataloader using batch size 219.
Ending timer at: 15:22:29 
Elapsed CPU time: 0.20917711666659974s
Used real time: 0:00:00
------------------
Tuning model: 6
Loading or creating optuna study with name: lstm-tune-category-6_6
Number of previous Trials with this name are #48
Starting tuning trial number #48 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 172, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 23, 'learning_rate': 1.997903931001789e-06, 'batch_first': True, 'batch_size': 279, 'dropout': 0.047092162791047776, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 279.
Initializing LSTM with 23 layers, 172 hidden size, 1.997903931001789e-06 learning rate, 0.047092162791047776 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:30
Starting tuning trial number #49 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 79, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 13, 'learning_rate': 1.937558030612426e-05, 'batch_first': True, 'batch_size': 21, 'dropout': 0.4985655076967334, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 21.
Initializing LSTM with 13 layers, 79 hidden size, 1.937558030612426e-05 learning rate, 0.4985655076967334 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:34
Starting tuning trial number #50 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 131, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 1.2778855965023938e-05, 'batch_first': True, 'batch_size': 4, 'dropout': 0.44996824356818305, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 74}
Converting dataset to dataloader using batch size 4.
Initializing LSTM with 9 layers, 131 hidden size, 1.2778855965023938e-05 learning rate, 0.44996824356818305 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:38
Starting tuning trial number #51 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 116, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 19, 'learning_rate': 6.184638392596448e-07, 'batch_first': True, 'batch_size': 33, 'dropout': 0.4634416469184227, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 33.
Initializing LSTM with 19 layers, 116 hidden size, 6.184638392596448e-07 learning rate, 0.4634416469184227 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:51
Starting tuning trial number #52 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 392, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.00023973937017834475, 'batch_first': True, 'batch_size': 107, 'dropout': 0.10687863921128835, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 87}
Converting dataset to dataloader using batch size 107.
Initializing LSTM with 6 layers, 392 hidden size, 0.00023973937017834475 learning rate, 0.10687863921128835 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:52
Starting tuning trial number #53 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 55, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 3.248570024562835e-05, 'batch_first': True, 'batch_size': 69, 'dropout': 0.42614922189805815, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 69.
Initializing LSTM with 2 layers, 55 hidden size, 3.248570024562835e-05 learning rate, 0.42614922189805815 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:53
Starting tuning trial number #54 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 194, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 15, 'learning_rate': 0.0018717925235104834, 'batch_first': True, 'batch_size': 40, 'dropout': 0.14344862693157737, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 80}
Converting dataset to dataloader using batch size 40.
Initializing LSTM with 15 layers, 194 hidden size, 0.0018717925235104834 learning rate, 0.14344862693157737 dropout. Optimiser: Adam,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:54
Starting tuning trial number #55 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 154, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 19, 'learning_rate': 0.0008141107185195302, 'batch_first': True, 'batch_size': 19, 'dropout': 0.3225405337901672, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 19.
Initializing LSTM with 19 layers, 154 hidden size, 0.0008141107185195302 learning rate, 0.3225405337901672 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:22:56
Ending timer at: 15:26:01 
Elapsed CPU time: 2659.962104966666s
Used real time: 0:03:05
------------------
Starting tuning trial number #56 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 222, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 8.791484398468673e-05, 'batch_first': True, 'batch_size': 221, 'dropout': 0.1934064194617346, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 221.
Initializing LSTM with 10 layers, 222 hidden size, 8.791484398468673e-05 learning rate, 0.1934064194617346 dropout. Optimiser: RMSprop,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:26:02
Starting tuning trial number #57 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 302, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.005256745104376075, 'batch_first': True, 'batch_size': 340, 'dropout': 0.2535190485576254, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 340.
Initializing LSTM with 5 layers, 302 hidden size, 0.005256745104376075 learning rate, 0.2535190485576254 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:26:03
Starting tuning trial number #58 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 354, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 22, 'learning_rate': 0.00020630262321646038, 'batch_first': True, 'batch_size': 47, 'dropout': 0.16649196189238075, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 47.
Initializing LSTM with 22 layers, 354 hidden size, 0.00020630262321646038 learning rate, 0.16649196189238075 dropout. Optimiser: Adam,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:26:04
Starting tuning trial number #59 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 242, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 29, 'learning_rate': 0.0010128037036682064, 'batch_first': True, 'batch_size': 240, 'dropout': 0.3979261107039802, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 16}
Converting dataset to dataloader using batch size 240.
Initializing LSTM with 29 layers, 242 hidden size, 0.0010128037036682064 learning rate, 0.3979261107039802 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:26:05
Starting tuning trial number #60 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 276, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 28, 'learning_rate': 6.866907131565361e-05, 'batch_first': True, 'batch_size': 149, 'dropout': 0.4746200241389634, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 8}
Converting dataset to dataloader using batch size 149.
Initializing LSTM with 28 layers, 276 hidden size, 6.866907131565361e-05 learning rate, 0.4746200241389634 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:26:06
Starting tuning trial number #61 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 182, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 24, 'learning_rate': 0.0004397701994071586, 'batch_first': True, 'batch_size': 19, 'dropout': 0.3654725812633254, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 19.
Initializing LSTM with 24 layers, 182 hidden size, 0.0004397701994071586 learning rate, 0.3654725812633254 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:26:07
Ending timer at: 15:27:12 
Elapsed CPU time: 1055.8126217999993s
Used real time: 0:01:05
------------------
Starting tuning trial number #62 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 179, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 24, 'learning_rate': 0.0004437850430223766, 'batch_first': True, 'batch_size': 14, 'dropout': 0.35170220847948785, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 27}
Converting dataset to dataloader using batch size 14.
Initializing LSTM with 24 layers, 179 hidden size, 0.0004437850430223766 learning rate, 0.35170220847948785 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:27:12
Ending timer at: 15:28:34 
Elapsed CPU time: 1243.8149685666663s
Used real time: 0:01:22
------------------
Starting tuning trial number #63 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 149, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 25, 'learning_rate': 0.0020111767344542456, 'batch_first': True, 'batch_size': 2, 'dropout': 0.2768179140164475, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 35}
Converting dataset to dataloader using batch size 2.
Initializing LSTM with 25 layers, 149 hidden size, 0.0020111767344542456 learning rate, 0.2768179140164475 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:28:36
Ending timer at: 15:35:50 
Elapsed CPU time: 7548.9874098166665s
Used real time: 0:07:14
------------------
Starting tuning trial number #64 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 150, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 26, 'learning_rate': 0.008070170818136175, 'batch_first': True, 'batch_size': 10, 'dropout': 0.28594975617118096, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 35}
Converting dataset to dataloader using batch size 10.
Initializing LSTM with 26 layers, 150 hidden size, 0.008070170818136175 learning rate, 0.28594975617118096 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:35:51
Starting tuning trial number #65 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 117, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 24, 'learning_rate': 0.022040027887388847, 'batch_first': True, 'batch_size': 206, 'dropout': 0.3253469776120134, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 29}
Converting dataset to dataloader using batch size 206.
Initializing LSTM with 24 layers, 117 hidden size, 0.022040027887388847 learning rate, 0.3253469776120134 dropout. Optimiser: Adam,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:35:59
Starting tuning trial number #66 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 180, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 22, 'learning_rate': 0.0011049212667418, 'batch_first': True, 'batch_size': 62, 'dropout': 0.23798261292553397, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 62.
Initializing LSTM with 22 layers, 180 hidden size, 0.0011049212667418 learning rate, 0.23798261292553397 dropout. Optimiser: AdaDelta,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:36:00
Starting tuning trial number #67 of total 20
with params: {'number_of_features': 1, 'hidden_layer_size': 196, 'input_window_size': 7, 'output_window_size': 1, 'number_of_layers': 26, 'learning_rate': 0.001958599416662237, 'batch_first': True, 'batch_size': 26, 'dropout': 0.009670458982806954, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 26.
Initializing LSTM with 26 layers, 196 hidden size, 0.001958599416662237 learning rate, 0.009670458982806954 dropout. Optimiser: RMSprop,  input window size: 7, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 15:36:01
Best params!
Params updated with best params
Initializing LSTM with 3 layers, 268 hidden size, 0.000313012901027482 learning rate, 0.11304291701878558 dropout. Optimiser: AdaDelta,  input window size: 5, output window size: 1
Using pre-existing neptune run for PytorchLightning
Best trial: 6,25
best_score: 0.009308607317507267
best_params: {'batch_size': 219, 'dropout': 0.11304291701878558, 'hidden_layer_size': 268, 'learning_rate': 0.000313012901027482, 'number_of_epochs': 40, 'number_of_layers': 3, 'optimizer_name': 'AdaDelta'}
Saving model
Error occurred during asynchronous operation processing: Cannot upload file /cluster/home/sjsivert/Masteroppgave/log_file.log: Path not found or is a not a file.
Finished
