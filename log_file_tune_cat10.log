Logger initialized. Log level: INFO, log file: ./log_file_tune_cat10.log
Started
Starting experiment: "lu-lstm-tune-category10": "lu-lstm-tune-category10"
Wiping and initializing checkpoint save location models/0_current_model_checkpoints
Creating model save location models/lu-lstm-tune-category10
Creating new Neptune experiment
Neptune run URL: https://app.neptune.ai/sjsivertandsanderkk/Masteroppgave/e/MAS-316
Running tuning experiment with saving set to True
Choosing model structure: <src.model_strutures.local_univariate_lstm_structure.LocalUnivariateLstmStructure object at 0x1498e3ed6160>
Running model on device: cuda
Initializing LSTM with 2 layers, 165 hidden size, 0.001 learning rate, 0.2 dropout. Optimiser: Adam,  input window size: 5, output window size: 1
Using pre-existing neptune run for PytorchLightning
data preprocessing steps: 
 ---- Start ----
1- load market insight data and categories and merge them
2- convert date columns to date_time format
3- sum up clicks to category level [groupBy(date, cat_id)]
4- rename column 'title' to 'cat_name'
5- combine feature 'hits' and 'clicks' to new feature 'interest'
6- drop columns 'hits' and 'clicks'
7- filter out data from early 2018-12-01
8- drop uninteresting colums: ['id_x', 'manufacturer_id', '_version_', 'id_y', 'internal_doc_id_x', 'internal_doc_id_y']
---- End ----
        

------------------
Starting new timer at 16:54:34
Data Pipeline for 10: ---- Start ----
1- Convert input dataset to generator object
2- filter out category 10)
3- choose columns 'interest' and 'date'
4- fill in dates with zero values
5- convert to np.array
6- scale data between -1 and 1
7- split up into training set (0.9) and test set (0.09999999999999998)
8- split training set into train set (0.9) and validation set (0.09999999999999998)
9- convert to timeseries dataset with input window size of 5, and output window size of 1
---- End ----
        
Converting dataset to dataloader using batch size 32.
Ending timer at: 16:54:34 
Elapsed CPU time: 0.30428051666670325s
Used real time: 0:00:00
------------------
Tuning model: 10
Loading or creating optuna study with name: lu-lstm-tune-category10_10
Number of previous Trials with this name are #1
Starting tuning trial number #1 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 380, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 18, 'learning_rate': 0.0986994157596285, 'batch_first': True, 'batch_size': 45, 'dropout': 0.01782460184372553, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 45.
Initializing LSTM with 18 layers, 380 hidden size, 0.0986994157596285 learning rate, 0.01782460184372553 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:54:35
--------
Starting new timer at 16:53:07
Ending timer at: 16:54:57 
Elapsed CPU time: 592.313899833333s
Used real time: 0:01:50
------------------
Starting tuning trial number #2 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 476, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 30, 'learning_rate': 1.870500878755243e-07, 'batch_first': True, 'batch_size': 303, 'dropout': 0.2392364410255553, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 303.
Initializing LSTM with 30 layers, 476 hidden size, 1.870500878755243e-07 learning rate, 0.2392364410255553 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:54:58
Ending timer at: 16:57:21 
Elapsed CPU time: 1077.5088473166666s
Used real time: 0:02:23
------------------
Starting tuning trial number #3 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 56, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.05650967611068312, 'batch_first': True, 'batch_size': 122, 'dropout': 0.19832076698538842, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 24}
Converting dataset to dataloader using batch size 122.
Initializing LSTM with 5 layers, 56 hidden size, 0.05650967611068312 learning rate, 0.19832076698538842 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:57:22
Ending timer at: 16:58:31 
Elapsed CPU time: 442.9671539166667s
Used real time: 0:01:09
------------------
Starting tuning trial number #4 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 177, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 12, 'learning_rate': 2.4381048743849046e-07, 'batch_first': True, 'batch_size': 276, 'dropout': 0.08410485623736591, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 276.
Initializing LSTM with 12 layers, 177 hidden size, 2.4381048743849046e-07 learning rate, 0.08410485623736591 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 16:58:31
Ending timer at: 17:01:11 
Elapsed CPU time: 1072.7286824000005s
Used real time: 0:02:40
------------------
Starting tuning trial number #5 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 185, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 7.106403681683835e-05, 'batch_first': True, 'batch_size': 212, 'dropout': 0.27947284524937077, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 212.
Initializing LSTM with 3 layers, 185 hidden size, 7.106403681683835e-05 learning rate, 0.27947284524937077 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:01:12
Ending timer at: 17:03:58 
Elapsed CPU time: 1081.604886816667s
Used real time: 0:02:46
------------------
Starting tuning trial number #6 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 269, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 28, 'learning_rate': 0.05224115093715898, 'batch_first': True, 'batch_size': 311, 'dropout': 0.36814522604354516, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 311.
Initializing LSTM with 28 layers, 269 hidden size, 0.05224115093715898 learning rate, 0.36814522604354516 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:03:59
Starting tuning trial number #7 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 489, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 8.821622826619962e-07, 'batch_first': True, 'batch_size': 35, 'dropout': 0.03953996918935082, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 35.
Initializing LSTM with 10 layers, 489 hidden size, 8.821622826619962e-07 learning rate, 0.03953996918935082 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:04:01
Starting tuning trial number #8 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 498, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.03334835425699676, 'batch_first': True, 'batch_size': 157, 'dropout': 0.06142050149654987, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 45}
Converting dataset to dataloader using batch size 157.
Initializing LSTM with 5 layers, 498 hidden size, 0.03334835425699676 learning rate, 0.06142050149654987 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:04:03
Ending timer at: 17:06:14 
Elapsed CPU time: 920.1469537666668s
Used real time: 0:02:11
------------------
Starting tuning trial number #9 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 259, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.00015350748978133682, 'batch_first': True, 'batch_size': 96, 'dropout': 0.07401501554841411, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 96.
Initializing LSTM with 9 layers, 259 hidden size, 0.00015350748978133682 learning rate, 0.07401501554841411 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:14
Starting tuning trial number #10 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 380, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0011156377588736758, 'batch_first': True, 'batch_size': 201, 'dropout': 0.49500942560907907, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 37}
Converting dataset to dataloader using batch size 201.
Initializing LSTM with 4 layers, 380 hidden size, 0.0011156377588736758 learning rate, 0.49500942560907907 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:16
Starting tuning trial number #11 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 165, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 18, 'learning_rate': 9.353287771407569e-06, 'batch_first': True, 'batch_size': 229, 'dropout': 0.32029474511026484, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 229.
Initializing LSTM with 18 layers, 165 hidden size, 9.353287771407569e-06 learning rate, 0.32029474511026484 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:18
Starting tuning trial number #12 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 365, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0014490218225272538, 'batch_first': True, 'batch_size': 150, 'dropout': 0.17066168904249102, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 150.
Initializing LSTM with 1 layers, 365 hidden size, 0.0014490218225272538 learning rate, 0.17066168904249102 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:26
Starting tuning trial number #13 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 154, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 17, 'learning_rate': 2.8646946073987055e-05, 'batch_first': True, 'batch_size': 215, 'dropout': 0.14054760054401244, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 9}
Converting dataset to dataloader using batch size 215.
Initializing LSTM with 17 layers, 154 hidden size, 2.8646946073987055e-05 learning rate, 0.14054760054401244 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:28
Starting tuning trial number #14 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 345, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.003010801327192839, 'batch_first': True, 'batch_size': 76, 'dropout': 0.2803964421534602, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 76.
Initializing LSTM with 1 layers, 345 hidden size, 0.003010801327192839 learning rate, 0.2803964421534602 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:32
Starting tuning trial number #15 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 93, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0002222085691789991, 'batch_first': True, 'batch_size': 165, 'dropout': 0.0016954388595888104, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 165.
Initializing LSTM with 7 layers, 93 hidden size, 0.0002222085691789991 learning rate, 0.0016954388595888104 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:06:34
Ending timer at: 17:10:48 
Elapsed CPU time: 1685.8224403833333s
Used real time: 0:04:14
------------------
Starting tuning trial number #16 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 99, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 13, 'learning_rate': 0.00017576343165751185, 'batch_first': True, 'batch_size': 242, 'dropout': 0.007993650551114832, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 242.
Initializing LSTM with 13 layers, 99 hidden size, 0.00017576343165751185 learning rate, 0.007993650551114832 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:10:49
Starting tuning trial number #17 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 113, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 5.167688022511942e-06, 'batch_first': True, 'batch_size': 172, 'dropout': 0.2484241640380224, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 172.
Initializing LSTM with 8 layers, 113 hidden size, 5.167688022511942e-06 learning rate, 0.2484241640380224 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:10:56
Starting tuning trial number #18 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 209, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 21, 'learning_rate': 5.2068350767837766e-05, 'batch_first': True, 'batch_size': 16, 'dropout': 0.12581773951800165, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 84}
Converting dataset to dataloader using batch size 16.
Initializing LSTM with 21 layers, 209 hidden size, 5.2068350767837766e-05 learning rate, 0.12581773951800165 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:11:03
Starting tuning trial number #19 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 85, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0004507970741209013, 'batch_first': True, 'batch_size': 353, 'dropout': 0.3348181906866547, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 353.
Initializing LSTM with 7 layers, 85 hidden size, 0.0004507970741209013 learning rate, 0.3348181906866547 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:11:07
Ending timer at: 17:14:13 
Elapsed CPU time: 1223.2546627666657s
Used real time: 0:03:06
------------------
Starting tuning trial number #20 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 212, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 14, 'learning_rate': 2.2815447933838112e-06, 'batch_first': True, 'batch_size': 121, 'dropout': 0.4319289396563093, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 121.
Initializing LSTM with 14 layers, 212 hidden size, 2.2815447933838112e-06 learning rate, 0.4319289396563093 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:14:14
Starting tuning trial number #21 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 12, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.008590968103342854, 'batch_first': True, 'batch_size': 246, 'dropout': 0.1927600789713982, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 246.
Initializing LSTM with 3 layers, 12 hidden size, 0.008590968103342854 learning rate, 0.1927600789713982 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:14:16
Starting tuning trial number #22 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 103, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.00046376065359546157, 'batch_first': True, 'batch_size': 342, 'dropout': 0.3062652619020642, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 342.
Initializing LSTM with 7 layers, 103 hidden size, 0.00046376065359546157 learning rate, 0.3062652619020642 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:14:18
Starting tuning trial number #23 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 70, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 1.9056276426458053e-05, 'batch_first': True, 'batch_size': 353, 'dropout': 0.3514459151907155, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 353.
Initializing LSTM with 7 layers, 70 hidden size, 1.9056276426458053e-05 learning rate, 0.3514459151907155 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:14:25
Starting tuning trial number #24 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 139, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 12, 'learning_rate': 0.0003519871558641444, 'batch_first': True, 'batch_size': 193, 'dropout': 0.3757491923453822, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 193.
Initializing LSTM with 12 layers, 139 hidden size, 0.0003519871558641444 learning rate, 0.3757491923453822 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:14:27
Starting tuning trial number #25 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 63, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 9.863232349016254e-05, 'batch_first': True, 'batch_size': 267, 'dropout': 0.28669767636287335, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 91}
Converting dataset to dataloader using batch size 267.
Initializing LSTM with 10 layers, 63 hidden size, 9.863232349016254e-05 learning rate, 0.28669767636287335 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:17:43
Starting tuning trial number #26 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 206, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.004975394339428493, 'batch_first': True, 'batch_size': 67, 'dropout': 0.22882867168356882, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 67.
Initializing LSTM with 3 layers, 206 hidden size, 0.004975394339428493 learning rate, 0.22882867168356882 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:17:56
Ending timer at: 17:21:00 
Elapsed CPU time: 1231.4572353499993s
Used real time: 0:03:04
------------------
Starting tuning trial number #27 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 288, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.010432682217694254, 'batch_first': True, 'batch_size': 47, 'dropout': 0.21864400896734232, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 29}
Converting dataset to dataloader using batch size 47.
Initializing LSTM with 3 layers, 288 hidden size, 0.010432682217694254 learning rate, 0.21864400896734232 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:21:01
Starting tuning trial number #28 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 216, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0036908266476633716, 'batch_first': True, 'batch_size': 78, 'dropout': 0.12380053715777248, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 78.
Initializing LSTM with 3 layers, 216 hidden size, 0.0036908266476633716 learning rate, 0.12380053715777248 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:21:04
Ending timer at: 17:24:02 
Elapsed CPU time: 1188.8983205666666s
Used real time: 0:02:58
------------------
Starting tuning trial number #29 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 320, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.006763184196827153, 'batch_first': True, 'batch_size': 67, 'dropout': 0.11613676143887473, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 67.
Initializing LSTM with 5 layers, 320 hidden size, 0.006763184196827153 learning rate, 0.11613676143887473 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:24:03
Starting tuning trial number #30 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 224, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.01849433632422995, 'batch_first': True, 'batch_size': 114, 'dropout': 0.15822198043017674, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 114.
Initializing LSTM with 2 layers, 224 hidden size, 0.01849433632422995 learning rate, 0.15822198043017674 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:24:05
Starting tuning trial number #31 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 426, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 23, 'learning_rate': 0.0023076004346836523, 'batch_first': True, 'batch_size': 7, 'dropout': 0.0017305753061033046, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 8}
Converting dataset to dataloader using batch size 7.
Initializing LSTM with 23 layers, 426 hidden size, 0.0023076004346836523 learning rate, 0.0017305753061033046 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:24:07
Starting tuning trial number #32 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 188, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0008788676605771372, 'batch_first': True, 'batch_size': 85, 'dropout': 0.10705335379809205, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 85.
Initializing LSTM with 1 layers, 188 hidden size, 0.0008788676605771372 learning rate, 0.10705335379809205 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:24:09
Starting tuning trial number #33 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 239, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.004678617275163941, 'batch_first': True, 'batch_size': 137, 'dropout': 0.2687662661917094, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 137.
Initializing LSTM with 5 layers, 239 hidden size, 0.004678617275163941 learning rate, 0.2687662661917094 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:24:11
Starting tuning trial number #34 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 125, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 5.934439458341183e-05, 'batch_first': True, 'batch_size': 49, 'dropout': 0.22785583582565236, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 49.
Initializing LSTM with 3 layers, 125 hidden size, 5.934439458341183e-05 learning rate, 0.22785583582565236 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:24:13
Starting tuning trial number #35 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 298, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.00033432177682145384, 'batch_first': True, 'batch_size': 103, 'dropout': 0.19078287101539815, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 103.
Initializing LSTM with 6 layers, 298 hidden size, 0.00033432177682145384 learning rate, 0.19078287101539815 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:27:07
Starting tuning trial number #36 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 20, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 0.021067379687541236, 'batch_first': True, 'batch_size': 179, 'dropout': 0.036433399765793996, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 179.
Initializing LSTM with 10 layers, 20 hidden size, 0.021067379687541236 learning rate, 0.036433399765793996 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:27:09
Starting tuning trial number #37 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 187, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.09304538623201633, 'batch_first': True, 'batch_size': 63, 'dropout': 0.09088964970227872, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 63.
Initializing LSTM with 3 layers, 187 hidden size, 0.09304538623201633 learning rate, 0.09088964970227872 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:27:13
Ending timer at: 17:29:12 
Elapsed CPU time: 797.2027223833322s
Used real time: 0:01:59
------------------
Starting tuning trial number #38 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 242, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 1.1716720157799667e-07, 'batch_first': True, 'batch_size': 135, 'dropout': 0.04764362091823097, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 52}
Converting dataset to dataloader using batch size 135.
Initializing LSTM with 5 layers, 242 hidden size, 1.1716720157799667e-07 learning rate, 0.04764362091823097 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:13
Starting tuning trial number #39 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 147, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.0030570484592744255, 'batch_first': True, 'batch_size': 310, 'dropout': 0.3988590694614038, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 310.
Initializing LSTM with 9 layers, 147 hidden size, 0.0030570484592744255 learning rate, 0.3988590694614038 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:15
Starting tuning trial number #40 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 184, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0008284318273933926, 'batch_first': True, 'batch_size': 28, 'dropout': 0.22069355034909127, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 28.
Initializing LSTM with 4 layers, 184 hidden size, 0.0008284318273933926 learning rate, 0.22069355034909127 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:17
Starting tuning trial number #41 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 275, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 11, 'learning_rate': 5.580956927169451e-07, 'batch_first': True, 'batch_size': 288, 'dropout': 0.15769899943747495, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 47}
Converting dataset to dataloader using batch size 288.
Initializing LSTM with 11 layers, 275 hidden size, 5.580956927169451e-07 learning rate, 0.15769899943747495 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:18
Starting tuning trial number #42 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 88, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0001785089770483259, 'batch_first': True, 'batch_size': 328, 'dropout': 0.3505315125485451, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 328.
Initializing LSTM with 7 layers, 88 hidden size, 0.0001785089770483259 learning rate, 0.3505315125485451 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:20
Starting tuning trial number #43 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 83, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 30, 'learning_rate': 0.0005228021336944695, 'batch_first': True, 'batch_size': 163, 'dropout': 0.3233198738078844, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 163.
Initializing LSTM with 30 layers, 83 hidden size, 0.0005228021336944695 learning rate, 0.3233198738078844 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:28
Starting tuning trial number #44 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 158, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 7.838678667684138e-05, 'batch_first': True, 'batch_size': 265, 'dropout': 0.44379060956360183, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 265.
Initializing LSTM with 6 layers, 158 hidden size, 7.838678667684138e-05 learning rate, 0.44379060956360183 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:29
Starting tuning trial number #45 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.001629432775607339, 'batch_first': True, 'batch_size': 98, 'dropout': 0.2680351688185396, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 85}
Converting dataset to dataloader using batch size 98.
Initializing LSTM with 8 layers, 46 hidden size, 0.001629432775607339 learning rate, 0.2680351688185396 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:29:31
Ending timer at: 17:33:31 
Elapsed CPU time: 1617.7145639833348s
Used real time: 0:04:00
------------------
Starting tuning trial number #46 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0017207825041986083, 'batch_first': True, 'batch_size': 91, 'dropout': 0.2506076146791801, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 84}
Converting dataset to dataloader using batch size 91.
Initializing LSTM with 2 layers, 41 hidden size, 0.0017207825041986083 learning rate, 0.2506076146791801 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:33:32
Starting tuning trial number #47 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 44, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 16, 'learning_rate': 3.0023552488007157e-05, 'batch_first': True, 'batch_size': 211, 'dropout': 0.30207384561798756, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 211.
Initializing LSTM with 16 layers, 44 hidden size, 3.0023552488007157e-05 learning rate, 0.30207384561798756 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:33:34
Starting tuning trial number #48 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 202, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.00489820129083951, 'batch_first': True, 'batch_size': 107, 'dropout': 0.25382789246042964, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 107.
Initializing LSTM with 9 layers, 202 hidden size, 0.00489820129083951 learning rate, 0.25382789246042964 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:33:41
Starting tuning trial number #49 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 127, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.016108345097485918, 'batch_first': True, 'batch_size': 134, 'dropout': 0.20410874990575317, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 87}
Converting dataset to dataloader using batch size 134.
Initializing LSTM with 4 layers, 127 hidden size, 0.016108345097485918 learning rate, 0.20410874990575317 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:33:43
Ending timer at: 17:37:42 
Elapsed CPU time: 1564.384499733334s
Used real time: 0:03:59
------------------
Starting tuning trial number #50 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 112, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 19, 'learning_rate': 0.017252331365408744, 'batch_first': True, 'batch_size': 141, 'dropout': 0.2042343224537119, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 87}
Converting dataset to dataloader using batch size 141.
Initializing LSTM with 19 layers, 112 hidden size, 0.017252331365408744 learning rate, 0.2042343224537119 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:37:44
Starting tuning trial number #51 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 45, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.06375692797714345, 'batch_first': True, 'batch_size': 73, 'dropout': 0.17633002949166937, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 73.
Initializing LSTM with 4 layers, 45 hidden size, 0.06375692797714345 learning rate, 0.17633002949166937 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:37:46
Starting tuning trial number #52 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 133, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.003377330413090771, 'batch_first': True, 'batch_size': 188, 'dropout': 0.2712921801664665, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 188.
Initializing LSTM with 1 layers, 133 hidden size, 0.003377330413090771 learning rate, 0.2712921801664665 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:37:48
Ending timer at: 17:42:22 
Elapsed CPU time: 1668.3689958666682s
Used real time: 0:04:34
------------------
Starting tuning trial number #53 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 146, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.003777829031745097, 'batch_first': True, 'batch_size': 189, 'dropout': 0.2663817890400121, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 189.
Initializing LSTM with 1 layers, 146 hidden size, 0.003777829031745097 learning rate, 0.2663817890400121 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:42:23
Starting tuning trial number #54 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 117, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.03547931821930549, 'batch_first': True, 'batch_size': 167, 'dropout': 0.23726838230820138, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 88}
Converting dataset to dataloader using batch size 167.
Initializing LSTM with 6 layers, 117 hidden size, 0.03547931821930549 learning rate, 0.23726838230820138 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:42:25
Starting tuning trial number #55 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 170, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.011616725767986806, 'batch_first': True, 'batch_size': 124, 'dropout': 0.29414297331199213, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 96}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 2 layers, 170 hidden size, 0.011616725767986806 learning rate, 0.29414297331199213 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:42:27
Starting tuning trial number #56 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 68, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 26, 'learning_rate': 0.0017157875242003004, 'batch_first': True, 'batch_size': 152, 'dropout': 0.024336317316204642, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 152.
Initializing LSTM with 26 layers, 68 hidden size, 0.0017157875242003004 learning rate, 0.024336317316204642 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:42:29
Starting tuning trial number #57 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 132, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.006441344899634388, 'batch_first': True, 'batch_size': 82, 'dropout': 0.07321385307223097, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 80}
Converting dataset to dataloader using batch size 82.
Initializing LSTM with 8 layers, 132 hidden size, 0.006441344899634388 learning rate, 0.07321385307223097 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:44:54
Starting tuning trial number #58 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 29, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0011303272466904957, 'batch_first': True, 'batch_size': 53, 'dropout': 0.14299357931140072, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 53.
Initializing LSTM with 4 layers, 29 hidden size, 0.0011303272466904957 learning rate, 0.14299357931140072 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:44:56
Ending timer at: 17:48:30 
Elapsed CPU time: 1473.1479261833329s
Used real time: 0:03:34
------------------
Starting tuning trial number #59 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 25, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0009238137394003106, 'batch_first': True, 'batch_size': 48, 'dropout': 0.1390924341599255, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 74}
Converting dataset to dataloader using batch size 48.
Initializing LSTM with 4 layers, 25 hidden size, 0.0009238137394003106 learning rate, 0.1390924341599255 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:31
Starting tuning trial number #60 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 89, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.029355183122806848, 'batch_first': True, 'batch_size': 32, 'dropout': 0.09982910588465582, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 32.
Initializing LSTM with 2 layers, 89 hidden size, 0.029355183122806848 learning rate, 0.09982910588465582 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:33
Starting tuning trial number #61 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 253, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.00022647792338782442, 'batch_first': True, 'batch_size': 233, 'dropout': 0.13544089670840898, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 81}
Converting dataset to dataloader using batch size 233.
Initializing LSTM with 1 layers, 253 hidden size, 0.00022647792338782442 learning rate, 0.13544089670840898 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:36
Starting tuning trial number #62 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0021963312012227613, 'batch_first': True, 'batch_size': 93, 'dropout': 0.20782910788320036, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 85}
Converting dataset to dataloader using batch size 93.
Initializing LSTM with 4 layers, 32 hidden size, 0.0021963312012227613 learning rate, 0.20782910788320036 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:43
Starting tuning trial number #63 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 61, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0013515786196858237, 'batch_first': True, 'batch_size': 57, 'dropout': 0.1549831960895677, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 57.
Initializing LSTM with 5 layers, 61 hidden size, 0.0013515786196858237 learning rate, 0.1549831960895677 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:45
Starting tuning trial number #64 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 225, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.009830714477920069, 'batch_first': True, 'batch_size': 124, 'dropout': 0.17897583075569823, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 76}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 8 layers, 225 hidden size, 0.009830714477920069 learning rate, 0.17897583075569823 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:49
Starting tuning trial number #65 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 102, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0006246126344556156, 'batch_first': True, 'batch_size': 102, 'dropout': 0.267637921488485, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 89}
Converting dataset to dataloader using batch size 102.
Initializing LSTM with 2 layers, 102 hidden size, 0.0006246126344556156 learning rate, 0.267637921488485 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:50
Starting tuning trial number #66 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 79, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0029005279327385257, 'batch_first': True, 'batch_size': 75, 'dropout': 0.23185944879710846, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 75.
Initializing LSTM with 6 layers, 79 hidden size, 0.0029005279327385257 learning rate, 0.23185944879710846 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:48:52
Starting tuning trial number #67 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 52, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 14, 'learning_rate': 0.005212254831370501, 'batch_first': True, 'batch_size': 112, 'dropout': 0.1878316829871762, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 14}
Converting dataset to dataloader using batch size 112.
Initializing LSTM with 14 layers, 52 hidden size, 0.005212254831370501 learning rate, 0.1878316829871762 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:47
Starting tuning trial number #68 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.012518885328538796, 'batch_first': True, 'batch_size': 207, 'dropout': 0.0700800194555492, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 83}
Converting dataset to dataloader using batch size 207.
Initializing LSTM with 3 layers, 10 hidden size, 0.012518885328538796 learning rate, 0.0700800194555492 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:49
Starting tuning trial number #69 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 130, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0003091754547828244, 'batch_first': True, 'batch_size': 16, 'dropout': 0.21089779237158873, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 97}
Converting dataset to dataloader using batch size 16.
Initializing LSTM with 7 layers, 130 hidden size, 0.0003091754547828244 learning rate, 0.21089779237158873 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:51
Starting tuning trial number #70 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 166, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.00011873075047151749, 'batch_first': True, 'batch_size': 178, 'dropout': 0.12031899582959109, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 77}
Converting dataset to dataloader using batch size 178.
Initializing LSTM with 5 layers, 166 hidden size, 0.00011873075047151749 learning rate, 0.12031899582959109 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:53
Starting tuning trial number #71 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 207, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0012398330294359106, 'batch_first': True, 'batch_size': 148, 'dropout': 0.057536383126089496, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 148.
Initializing LSTM with 4 layers, 207 hidden size, 0.0012398330294359106 learning rate, 0.057536383126089496 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:55
Starting tuning trial number #72 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 192, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 3.6781643686235754e-05, 'batch_first': True, 'batch_size': 221, 'dropout': 0.2856323904800319, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 221.
Initializing LSTM with 3 layers, 192 hidden size, 3.6781643686235754e-05 learning rate, 0.2856323904800319 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:57
Starting tuning trial number #73 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 224, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0007003431021112094, 'batch_first': True, 'batch_size': 190, 'dropout': 0.3136848167219589, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 190.
Initializing LSTM with 3 layers, 224 hidden size, 0.0007003431021112094 learning rate, 0.3136848167219589 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:51:59
Ending timer at: 17:55:16 
Elapsed CPU time: 1265.9771037000003s
Used real time: 0:03:17
------------------
Starting tuning trial number #74 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 246, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0006695303474457038, 'batch_first': True, 'batch_size': 197, 'dropout': 0.2595162110154212, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 197.
Initializing LSTM with 1 layers, 246 hidden size, 0.0006695303474457038 learning rate, 0.2595162110154212 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:18
Starting tuning trial number #75 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 225, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.002489725325056761, 'batch_first': True, 'batch_size': 189, 'dropout': 0.30908760773564453, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 87}
Converting dataset to dataloader using batch size 189.
Initializing LSTM with 2 layers, 225 hidden size, 0.002489725325056761 learning rate, 0.30908760773564453 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:21
Starting tuning trial number #76 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 175, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0037181471884989116, 'batch_first': True, 'batch_size': 60, 'dropout': 0.3274294419156318, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 60.
Initializing LSTM with 3 layers, 175 hidden size, 0.0037181471884989116 learning rate, 0.3274294419156318 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:23
Starting tuning trial number #77 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 274, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.007468343615201195, 'batch_first': True, 'batch_size': 157, 'dropout': 0.35080164420902094, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 157.
Initializing LSTM with 5 layers, 274 hidden size, 0.007468343615201195 learning rate, 0.35080164420902094 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:25
Starting tuning trial number #78 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 464, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.0018246746392890434, 'batch_first': True, 'batch_size': 39, 'dropout': 0.1990177230340187, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 39.
Initializing LSTM with 8 layers, 464 hidden size, 0.0018246746392890434 learning rate, 0.1990177230340187 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:29
Starting tuning trial number #79 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 73, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0002639091267459359, 'batch_first': True, 'batch_size': 132, 'dropout': 0.24340729142315312, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 132.
Initializing LSTM with 4 layers, 73 hidden size, 0.0002639091267459359 learning rate, 0.24340729142315312 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:35
Starting tuning trial number #80 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 106, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.00040810968183966245, 'batch_first': True, 'batch_size': 88, 'dropout': 0.2796442962749122, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 88.
Initializing LSTM with 1 layers, 106 hidden size, 0.00040810968183966245 learning rate, 0.2796442962749122 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:36
Starting tuning trial number #81 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 316, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0009601031482961377, 'batch_first': True, 'batch_size': 173, 'dropout': 0.1629752161097391, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 173.
Initializing LSTM with 6 layers, 316 hidden size, 0.0009601031482961377 learning rate, 0.1629752161097391 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:44
Starting tuning trial number #82 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 198, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0001453726616911602, 'batch_first': True, 'batch_size': 224, 'dropout': 0.3076713375969782, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 224.
Initializing LSTM with 3 layers, 198 hidden size, 0.0001453726616911602 learning rate, 0.3076713375969782 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:46
Starting tuning trial number #83 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 154, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 8.77001600486826e-06, 'batch_first': True, 'batch_size': 204, 'dropout': 0.22619286606927275, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 204.
Initializing LSTM with 2 layers, 154 hidden size, 8.77001600486826e-06 learning rate, 0.22619286606927275 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:48
Starting tuning trial number #84 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 215, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 8.243416287224116e-05, 'batch_first': True, 'batch_size': 241, 'dropout': 0.34025926778664894, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 241.
Initializing LSTM with 4 layers, 215 hidden size, 8.243416287224116e-05 learning rate, 0.34025926778664894 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:50
Starting tuning trial number #85 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 234, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 11, 'learning_rate': 0.016223524098192638, 'batch_first': True, 'batch_size': 182, 'dropout': 0.27690834174360285, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 182.
Initializing LSTM with 11 layers, 234 hidden size, 0.016223524098192638 learning rate, 0.27690834174360285 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:52
Starting tuning trial number #86 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 259, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0036981844713058387, 'batch_first': True, 'batch_size': 68, 'dropout': 0.3803591488534164, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 68.
Initializing LSTM with 6 layers, 259 hidden size, 0.0036981844713058387 learning rate, 0.3803591488534164 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:54
Starting tuning trial number #87 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 180, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 5.502160250207157e-05, 'batch_first': True, 'batch_size': 162, 'dropout': 0.2911003853313812, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 99}
Converting dataset to dataloader using batch size 162.
Initializing LSTM with 7 layers, 180 hidden size, 5.502160250207157e-05 learning rate, 0.2911003853313812 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:55:57
Starting tuning trial number #88 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 121, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.001239248829556858, 'batch_first': True, 'batch_size': 145, 'dropout': 0.02171384501192425, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 145.
Initializing LSTM with 2 layers, 121 hidden size, 0.001239248829556858 learning rate, 0.02171384501192425 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:00
Starting tuning trial number #89 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 139, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0006236078651386923, 'batch_first': True, 'batch_size': 98, 'dropout': 0.24268406313626026, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 98.
Initializing LSTM with 5 layers, 139 hidden size, 0.0006236078651386923 learning rate, 0.24268406313626026 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:02
Starting tuning trial number #90 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 98, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.00020004896953267664, 'batch_first': True, 'batch_size': 53, 'dropout': 0.2567579401447613, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 44}
Converting dataset to dataloader using batch size 53.
Initializing LSTM with 3 layers, 98 hidden size, 0.00020004896953267664 learning rate, 0.2567579401447613 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:04
Starting tuning trial number #91 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 29, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.005610980937457912, 'batch_first': True, 'batch_size': 118, 'dropout': 0.318044185427662, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 118.
Initializing LSTM with 3 layers, 29 hidden size, 0.005610980937457912 learning rate, 0.318044185427662 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:06
Starting tuning trial number #92 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 91, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.00043294121054259123, 'batch_first': True, 'batch_size': 296, 'dropout': 0.3358290966493968, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 296.
Initializing LSTM with 4 layers, 91 hidden size, 0.00043294121054259123 learning rate, 0.3358290966493968 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:08
Starting tuning trial number #93 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.000790002162482226, 'batch_first': True, 'batch_size': 338, 'dropout': 0.08928735661391306, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 85}
Converting dataset to dataloader using batch size 338.
Initializing LSTM with 9 layers, 54 hidden size, 0.000790002162482226 learning rate, 0.08928735661391306 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:10
Starting tuning trial number #94 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 213, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.001627818797486978, 'batch_first': True, 'batch_size': 81, 'dropout': 0.31396321086386564, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 81.
Initializing LSTM with 7 layers, 213 hidden size, 0.001627818797486978 learning rate, 0.31396321086386564 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:17
Starting tuning trial number #95 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 39, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0005045478453684633, 'batch_first': True, 'batch_size': 252, 'dropout': 0.30068682274981906, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 252.
Initializing LSTM with 1 layers, 39 hidden size, 0.0005045478453684633 learning rate, 0.30068682274981906 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:19
Starting tuning trial number #96 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 156, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0023130811105705924, 'batch_first': True, 'batch_size': 133, 'dropout': 0.3593643039627175, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 77}
Converting dataset to dataloader using batch size 133.
Initializing LSTM with 2 layers, 156 hidden size, 0.0023130811105705924 learning rate, 0.3593643039627175 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:27
Starting tuning trial number #97 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 72, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 0.03137454334774395, 'batch_first': True, 'batch_size': 22, 'dropout': 0.27407516493821255, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 22.
Initializing LSTM with 10 layers, 72 hidden size, 0.03137454334774395 learning rate, 0.27407516493821255 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:29
Starting tuning trial number #98 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 140, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 1.411774553072004e-05, 'batch_first': True, 'batch_size': 324, 'dropout': 0.22158398830510534, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 34}
Converting dataset to dataloader using batch size 324.
Initializing LSTM with 6 layers, 140 hidden size, 1.411774553072004e-05 learning rate, 0.22158398830510534 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:31
Starting tuning trial number #99 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 114, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.008833441824585467, 'batch_first': True, 'batch_size': 184, 'dropout': 0.2970247396038112, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 184.
Initializing LSTM with 5 layers, 114 hidden size, 0.008833441824585467 learning rate, 0.2970247396038112 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:33
Starting tuning trial number #100 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 60, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.00011605870493737027, 'batch_first': True, 'batch_size': 214, 'dropout': 0.3389388214813196, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 214.
Initializing LSTM with 4 layers, 60 hidden size, 0.00011605870493737027 learning rate, 0.3389388214813196 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:44
Starting tuning trial number #101 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 194, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 21, 'learning_rate': 0.0003068495135266837, 'batch_first': True, 'batch_size': 44, 'dropout': 0.23516004627531342, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 88}
Converting dataset to dataloader using batch size 44.
Initializing LSTM with 21 layers, 194 hidden size, 0.0003068495135266837 learning rate, 0.23516004627531342 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:46
Starting tuning trial number #102 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 222, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.049297168778800915, 'batch_first': True, 'batch_size': 173, 'dropout': 0.05785586778017085, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 44}
Converting dataset to dataloader using batch size 173.
Initializing LSTM with 6 layers, 222 hidden size, 0.049297168778800915 learning rate, 0.05785586778017085 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:48
Starting tuning trial number #103 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 237, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.02170958158944847, 'batch_first': True, 'batch_size': 359, 'dropout': 0.10865387773293705, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 359.
Initializing LSTM with 5 layers, 237 hidden size, 0.02170958158944847 learning rate, 0.10865387773293705 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:50
Starting tuning trial number #104 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 429, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.07252390871690823, 'batch_first': True, 'batch_size': 197, 'dropout': 0.010703971070957247, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 197.
Initializing LSTM with 7 layers, 429 hidden size, 0.07252390871690823 learning rate, 0.010703971070957247 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:52
Starting tuning trial number #105 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 494, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.014422075548260934, 'batch_first': True, 'batch_size': 158, 'dropout': 0.03051637032883481, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 158.
Initializing LSTM with 3 layers, 494 hidden size, 0.014422075548260934 learning rate, 0.03051637032883481 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:54
Starting tuning trial number #106 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 169, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0034053701947488325, 'batch_first': True, 'batch_size': 167, 'dropout': 0.1463798816100319, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 167.
Initializing LSTM with 4 layers, 169 hidden size, 0.0034053701947488325 learning rate, 0.1463798816100319 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:56
Starting tuning trial number #107 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 376, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.04107187235204196, 'batch_first': True, 'batch_size': 66, 'dropout': 0.12663564546915562, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 52}
Converting dataset to dataloader using batch size 66.
Initializing LSTM with 8 layers, 376 hidden size, 0.04107187235204196 learning rate, 0.12663564546915562 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:56:58
Starting tuning trial number #108 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 19, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 1.689752179567038e-06, 'batch_first': True, 'batch_size': 152, 'dropout': 0.2148014003635694, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 39}
Converting dataset to dataloader using batch size 152.
Initializing LSTM with 1 layers, 19 hidden size, 1.689752179567038e-06 learning rate, 0.2148014003635694 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:00
Starting tuning trial number #109 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 94, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.024611985072336126, 'batch_first': True, 'batch_size': 79, 'dropout': 0.0612669274695907, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 79}
Converting dataset to dataloader using batch size 79.
Initializing LSTM with 5 layers, 94 hidden size, 0.024611985072336126 learning rate, 0.0612669274695907 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:08
Starting tuning trial number #110 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 352, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.0010462706036735224, 'batch_first': True, 'batch_size': 110, 'dropout': 0.1808072549449927, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 110.
Initializing LSTM with 9 layers, 352 hidden size, 0.0010462706036735224 learning rate, 0.1808072549449927 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:10
Starting tuning trial number #111 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 130, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0049620253349235855, 'batch_first': True, 'batch_size': 129, 'dropout': 0.04493245921905737, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 3 layers, 130 hidden size, 0.0049620253349235855 learning rate, 0.04493245921905737 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:12
Starting tuning trial number #112 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 81, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.049721919026245974, 'batch_first': True, 'batch_size': 99, 'dropout': 0.1684628157797191, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 14}
Converting dataset to dataloader using batch size 99.
Initializing LSTM with 8 layers, 81 hidden size, 0.049721919026245974 learning rate, 0.1684628157797191 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:14
Starting tuning trial number #113 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 53, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.06665097942198447, 'batch_first': True, 'batch_size': 145, 'dropout': 0.2608196585536021, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 29}
Converting dataset to dataloader using batch size 145.
Initializing LSTM with 2 layers, 53 hidden size, 0.06665097942198447 learning rate, 0.2608196585536021 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:16
Starting tuning trial number #114 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 45, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.026371495876435648, 'batch_first': True, 'batch_size': 118, 'dropout': 0.18777442810785025, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 18}
Converting dataset to dataloader using batch size 118.
Initializing LSTM with 5 layers, 45 hidden size, 0.026371495876435648 learning rate, 0.18777442810785025 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:57:18
Ending timer at: 17:58:12 
Elapsed CPU time: 356.69396641666634s
Used real time: 0:00:54
------------------
Starting tuning trial number #115 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.007311708307920231, 'batch_first': True, 'batch_size': 140, 'dropout': 0.19624205947825565, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 140.
Initializing LSTM with 6 layers, 41 hidden size, 0.007311708307920231 learning rate, 0.19624205947825565 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:14
Starting tuning trial number #116 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 260, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.011511144278950409, 'batch_first': True, 'batch_size': 120, 'dropout': 0.28719645928837856, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 34}
Converting dataset to dataloader using batch size 120.
Initializing LSTM with 4 layers, 260 hidden size, 0.011511144278950409 learning rate, 0.28719645928837856 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:16
Starting tuning trial number #117 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0022110783901633253, 'batch_first': True, 'batch_size': 105, 'dropout': 0.24964365556749085, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 105.
Initializing LSTM with 5 layers, 32 hidden size, 0.0022110783901633253 learning rate, 0.24964365556749085 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:18
Starting tuning trial number #118 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 109, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.00024236529761105763, 'batch_first': True, 'batch_size': 88, 'dropout': 0.10293721631703732, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 88.
Initializing LSTM with 2 layers, 109 hidden size, 0.00024236529761105763 learning rate, 0.10293721631703732 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:20
Starting tuning trial number #119 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 478, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0014826405919121566, 'batch_first': True, 'batch_size': 187, 'dropout': 0.144225687797649, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 187.
Initializing LSTM with 7 layers, 478 hidden size, 0.0014826405919121566 learning rate, 0.144225687797649 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:22
Starting tuning trial number #120 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 63, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.026213268121973835, 'batch_first': True, 'batch_size': 127, 'dropout': 0.07820502401598585, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 127.
Initializing LSTM with 4 layers, 63 hidden size, 0.026213268121973835 learning rate, 0.07820502401598585 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:24
Starting tuning trial number #121 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 205, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.08955781835673454, 'batch_first': True, 'batch_size': 178, 'dropout': 0.18644053837819485, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 178.
Initializing LSTM with 3 layers, 205 hidden size, 0.08955781835673454 learning rate, 0.18644053837819485 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:26
Starting tuning trial number #122 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 77, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0007055222112811049, 'batch_first': True, 'batch_size': 72, 'dropout': 0.47444194073499324, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 72.
Initializing LSTM with 6 layers, 77 hidden size, 0.0007055222112811049 learning rate, 0.47444194073499324 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 17:58:28
Ending timer at: 18:01:24 
Elapsed CPU time: 1197.519737899999s
Used real time: 0:02:56
------------------
Starting tuning trial number #123 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 76, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.00015269867302015837, 'batch_first': True, 'batch_size': 93, 'dropout': 0.49944741693390815, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 93.
Initializing LSTM with 6 layers, 76 hidden size, 0.00015269867302015837 learning rate, 0.49944741693390815 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:01:24
Starting tuning trial number #124 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0005981356614239588, 'batch_first': True, 'batch_size': 74, 'dropout': 0.0005337008755920713, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 74.
Initializing LSTM with 5 layers, 46 hidden size, 0.0005981356614239588 learning rate, 0.0005337008755920713 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:01:26
Starting tuning trial number #125 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 403, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0007663188774561434, 'batch_first': True, 'batch_size': 59, 'dropout': 0.46412776020102553, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 59.
Initializing LSTM with 4 layers, 403 hidden size, 0.0007663188774561434 learning rate, 0.46412776020102553 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:10
Starting tuning trial number #126 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 87, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0011499153084620136, 'batch_first': True, 'batch_size': 40, 'dropout': 0.4127871346438281, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 40.
Initializing LSTM with 1 layers, 87 hidden size, 0.0011499153084620136 learning rate, 0.4127871346438281 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:13
Starting tuning trial number #127 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 22, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.002887386108489018, 'batch_first': True, 'batch_size': 54, 'dropout': 0.4642805619420612, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 54.
Initializing LSTM with 7 layers, 22 hidden size, 0.002887386108489018 learning rate, 0.4642805619420612 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:15
Starting tuning trial number #128 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 68, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0017861055618930903, 'batch_first': True, 'batch_size': 116, 'dropout': 0.20282280044864645, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 116.
Initializing LSTM with 2 layers, 68 hidden size, 0.0017861055618930903 learning rate, 0.20282280044864645 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:18
Starting tuning trial number #129 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 184, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.00039532435779929086, 'batch_first': True, 'batch_size': 70, 'dropout': 0.38704005823442306, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 70.
Initializing LSTM with 5 layers, 184 hidden size, 0.00039532435779929086 learning rate, 0.38704005823442306 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:20
Starting tuning trial number #130 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 50, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 4.3124037946265266e-05, 'batch_first': True, 'batch_size': 271, 'dropout': 0.2733357637557503, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 271.
Initializing LSTM with 3 layers, 50 hidden size, 4.3124037946265266e-05 learning rate, 0.2733357637557503 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:22
Starting tuning trial number #131 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 287, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.004280814857244553, 'batch_first': True, 'batch_size': 84, 'dropout': 0.13156538644557492, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 84.
Initializing LSTM with 6 layers, 287 hidden size, 0.004280814857244553 learning rate, 0.13156538644557492 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:24
Starting tuning trial number #132 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0203777147940767, 'batch_first': True, 'batch_size': 110, 'dropout': 0.2279817315233453, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 110.
Initializing LSTM with 5 layers, 37 hidden size, 0.0203777147940767 learning rate, 0.2279817315233453 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:27
Starting tuning trial number #133 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 59, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.03649808023665096, 'batch_first': True, 'batch_size': 166, 'dropout': 0.1510739612616931, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 166.
Initializing LSTM with 4 layers, 59 hidden size, 0.03649808023665096 learning rate, 0.1510739612616931 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:28
Starting tuning trial number #134 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 98, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.01622979656812806, 'batch_first': True, 'batch_size': 136, 'dropout': 0.1680461320518259, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 21}
Converting dataset to dataloader using batch size 136.
Initializing LSTM with 7 layers, 98 hidden size, 0.01622979656812806 learning rate, 0.1680461320518259 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:30
Starting tuning trial number #135 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 14, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 15, 'learning_rate': 0.0005117886288705289, 'batch_first': True, 'batch_size': 154, 'dropout': 0.1147568884148172, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 13}
Converting dataset to dataloader using batch size 154.
Initializing LSTM with 15 layers, 14 hidden size, 0.0005117886288705289 learning rate, 0.1147568884148172 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:33
Starting tuning trial number #136 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 248, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.04290614905567743, 'batch_first': True, 'batch_size': 197, 'dropout': 0.21368039036558445, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 18}
Converting dataset to dataloader using batch size 197.
Initializing LSTM with 3 layers, 248 hidden size, 0.04290614905567743 learning rate, 0.21368039036558445 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:41
Starting tuning trial number #137 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 119, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 6.473747182115581e-05, 'batch_first': True, 'batch_size': 102, 'dropout': 0.23807649445622564, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 24}
Converting dataset to dataloader using batch size 102.
Initializing LSTM with 6 layers, 119 hidden size, 6.473747182115581e-05 learning rate, 0.23807649445622564 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:43
Starting tuning trial number #138 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 82, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0008971957458780591, 'batch_first': True, 'batch_size': 122, 'dropout': 0.09311340605002488, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 6}
Converting dataset to dataloader using batch size 122.
Initializing LSTM with 5 layers, 82 hidden size, 0.0008971957458780591 learning rate, 0.09311340605002488 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:04:51
Ending timer at: 18:05:12 
Elapsed CPU time: 134.69802786666682s
Used real time: 0:00:21
------------------
Starting tuning trial number #139 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 227, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0006532927743565604, 'batch_first': True, 'batch_size': 286, 'dropout': 0.09702832590545366, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 11}
Converting dataset to dataloader using batch size 286.
Initializing LSTM with 4 layers, 227 hidden size, 0.0006532927743565604 learning rate, 0.09702832590545366 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:05:13
Starting tuning trial number #140 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 80, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0009565149151103253, 'batch_first': True, 'batch_size': 63, 'dropout': 0.08267069504140487, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 83}
Converting dataset to dataloader using batch size 63.
Initializing LSTM with 5 layers, 80 hidden size, 0.0009565149151103253 learning rate, 0.08267069504140487 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:05:15
Starting tuning trial number #141 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 105, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 18, 'learning_rate': 0.0002949612199507306, 'batch_first': True, 'batch_size': 174, 'dropout': 0.32672350198636807, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 10}
Converting dataset to dataloader using batch size 174.
Initializing LSTM with 18 layers, 105 hidden size, 0.0002949612199507306 learning rate, 0.32672350198636807 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:05:17
Starting tuning trial number #142 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 72, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0013907388038541678, 'batch_first': True, 'batch_size': 120, 'dropout': 0.26696072364848983, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 28}
Converting dataset to dataloader using batch size 120.
Initializing LSTM with 5 layers, 72 hidden size, 0.0013907388038541678 learning rate, 0.26696072364848983 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:05:19
Ending timer at: 18:06:40 
Elapsed CPU time: 537.1021900833323s
Used real time: 0:01:21
------------------
Starting tuning trial number #143 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 71, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0012217594444998655, 'batch_first': True, 'batch_size': 116, 'dropout': 0.2628516736672649, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 116.
Initializing LSTM with 4 layers, 71 hidden size, 0.0012217594444998655 learning rate, 0.2628516736672649 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:06:41
Ending timer at: 18:07:31 
Elapsed CPU time: 340.48170500000197s
Used real time: 0:00:50
------------------
Starting tuning trial number #144 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 87, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0013161795549090707, 'batch_first': True, 'batch_size': 115, 'dropout': 0.2648433964428985, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 16}
Converting dataset to dataloader using batch size 115.
Initializing LSTM with 4 layers, 87 hidden size, 0.0013161795549090707 learning rate, 0.2648433964428985 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:32
Starting tuning trial number #145 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 69, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0018883994438143917, 'batch_first': True, 'batch_size': 123, 'dropout': 0.24851334143552475, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 12}
Converting dataset to dataloader using batch size 123.
Initializing LSTM with 2 layers, 69 hidden size, 0.0018883994438143917 learning rate, 0.24851334143552475 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:34
Starting tuning trial number #146 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 61, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.0007753609883625844, 'batch_first': True, 'batch_size': 94, 'dropout': 0.2748831631220731, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 7}
Converting dataset to dataloader using batch size 94.
Initializing LSTM with 8 layers, 61 hidden size, 0.0007753609883625844 learning rate, 0.2748831631220731 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:36
Starting tuning trial number #147 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 76, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0010686534791841773, 'batch_first': True, 'batch_size': 128, 'dropout': 0.2837640113031258, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 8}
Converting dataset to dataloader using batch size 128.
Initializing LSTM with 3 layers, 76 hidden size, 0.0010686534791841773 learning rate, 0.2837640113031258 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:37
Starting tuning trial number #148 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 49, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0026866330896169635, 'batch_first': True, 'batch_size': 103, 'dropout': 0.2634749534939388, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 6}
Converting dataset to dataloader using batch size 103.
Initializing LSTM with 6 layers, 49 hidden size, 0.0026866330896169635 learning rate, 0.2634749534939388 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:39
Starting tuning trial number #149 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 97, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0003859277975187323, 'batch_first': True, 'batch_size': 78, 'dropout': 0.2941687523511722, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 78.
Initializing LSTM with 5 layers, 97 hidden size, 0.0003859277975187323 learning rate, 0.2941687523511722 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:41
Starting tuning trial number #150 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 218, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0015270322377101153, 'batch_first': True, 'batch_size': 109, 'dropout': 0.2537033319841792, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 27}
Converting dataset to dataloader using batch size 109.
Initializing LSTM with 2 layers, 218 hidden size, 0.0015270322377101153 learning rate, 0.2537033319841792 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:43
Starting tuning trial number #151 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 147, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0021295625694464867, 'batch_first': True, 'batch_size': 85, 'dropout': 0.2373153469348554, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 5}
Converting dataset to dataloader using batch size 85.
Initializing LSTM with 4 layers, 147 hidden size, 0.0021295625694464867 learning rate, 0.2373153469348554 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:45
Starting tuning trial number #152 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.00600963142462846, 'batch_first': True, 'batch_size': 141, 'dropout': 0.11793039327544691, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 141.
Initializing LSTM with 6 layers, 32 hidden size, 0.00600963142462846 learning rate, 0.11793039327544691 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:47
Starting tuning trial number #153 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 207, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0031669981726693748, 'batch_first': True, 'batch_size': 161, 'dropout': 0.26842831834499425, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 31}
Converting dataset to dataloader using batch size 161.
Initializing LSTM with 3 layers, 207 hidden size, 0.0031669981726693748 learning rate, 0.26842831834499425 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:50
Starting tuning trial number #154 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 84, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0005313263463268584, 'batch_first': True, 'batch_size': 118, 'dropout': 0.30553048740270544, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 118.
Initializing LSTM with 5 layers, 84 hidden size, 0.0005313263463268584 learning rate, 0.30553048740270544 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:51
Starting tuning trial number #155 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 198, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.000792900316604311, 'batch_first': True, 'batch_size': 148, 'dropout': 0.06636742356218987, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 148.
Initializing LSTM with 7 layers, 198 hidden size, 0.000792900316604311 learning rate, 0.06636742356218987 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:07:59
Starting tuning trial number #156 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 125, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0002003836741234206, 'batch_first': True, 'batch_size': 130, 'dropout': 0.01812910251044192, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 96}
Converting dataset to dataloader using batch size 130.
Initializing LSTM with 3 layers, 125 hidden size, 0.0002003836741234206 learning rate, 0.01812910251044192 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:01
Starting tuning trial number #157 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 107, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.004200880337473128, 'batch_first': True, 'batch_size': 193, 'dropout': 0.28272179320496976, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 193.
Initializing LSTM with 4 layers, 107 hidden size, 0.004200880337473128 learning rate, 0.28272179320496976 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:03
Starting tuning trial number #158 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 69, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0012514214965066667, 'batch_first': True, 'batch_size': 52, 'dropout': 0.2228232801504454, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 94}
Converting dataset to dataloader using batch size 52.
Initializing LSTM with 6 layers, 69 hidden size, 0.0012514214965066667 learning rate, 0.2228232801504454 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:05
Starting tuning trial number #159 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 91, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 24, 'learning_rate': 0.00842771539787082, 'batch_first': True, 'batch_size': 313, 'dropout': 0.0970884320094505, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 313.
Initializing LSTM with 24 layers, 91 hidden size, 0.00842771539787082 learning rate, 0.0970884320094505 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:07
Starting tuning trial number #160 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 55, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.0010206857362676854, 'batch_first': True, 'batch_size': 136, 'dropout': 0.04650131493987331, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 136.
Initializing LSTM with 8 layers, 55 hidden size, 0.0010206857362676854 learning rate, 0.04650131493987331 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:09
Starting tuning trial number #161 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 43, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.00010873605736157032, 'batch_first': True, 'batch_size': 72, 'dropout': 0.47571652831350986, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 15}
Converting dataset to dataloader using batch size 72.
Initializing LSTM with 5 layers, 43 hidden size, 0.00010873605736157032 learning rate, 0.47571652831350986 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:11
Starting tuning trial number #162 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 235, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.06595399459709891, 'batch_first': True, 'batch_size': 121, 'dropout': 0.2119048029229442, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 121.
Initializing LSTM with 4 layers, 235 hidden size, 0.06595399459709891 learning rate, 0.2119048029229442 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:12
Starting tuning trial number #163 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 67, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.02758090685789971, 'batch_first': True, 'batch_size': 94, 'dropout': 0.1928575514734647, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 28}
Converting dataset to dataloader using batch size 94.
Initializing LSTM with 6 layers, 67 hidden size, 0.02758090685789971 learning rate, 0.1928575514734647 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:14
Starting tuning trial number #164 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 27, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0018769550013891974, 'batch_first': True, 'batch_size': 206, 'dropout': 0.20537251068112547, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 22}
Converting dataset to dataloader using batch size 206.
Initializing LSTM with 5 layers, 27 hidden size, 0.0018769550013891974 learning rate, 0.20537251068112547 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:16
Starting tuning trial number #165 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 78, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.00044702366601967903, 'batch_first': True, 'batch_size': 111, 'dropout': 0.4242994495285406, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 111.
Initializing LSTM with 7 layers, 78 hidden size, 0.00044702366601967903 learning rate, 0.4242994495285406 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:24
Starting tuning trial number #166 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 52, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.09608351385065174, 'batch_first': True, 'batch_size': 124, 'dropout': 0.24500261733535109, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 35}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 3 layers, 52 hidden size, 0.09608351385065174 learning rate, 0.24500261733535109 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:08:26
Ending timer at: 18:10:04 
Elapsed CPU time: 669.5524675000001s
Used real time: 0:01:38
------------------
Starting tuning trial number #167 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 162, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0007000788173212307, 'batch_first': True, 'batch_size': 124, 'dropout': 0.25711316282316266, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 37}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 1 layers, 162 hidden size, 0.0007000788173212307 learning rate, 0.25711316282316266 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:05
Starting tuning trial number #168 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 175, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0014577102836774508, 'batch_first': True, 'batch_size': 143, 'dropout': 0.24364048283568185, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 31}
Converting dataset to dataloader using batch size 143.
Initializing LSTM with 3 layers, 175 hidden size, 0.0014577102836774508 learning rate, 0.24364048283568185 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:07
Starting tuning trial number #169 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 55, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.08855570464994768, 'batch_first': True, 'batch_size': 169, 'dropout': 0.03595257888461746, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 45}
Converting dataset to dataloader using batch size 169.
Initializing LSTM with 2 layers, 55 hidden size, 0.08855570464994768 learning rate, 0.03595257888461746 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:09
Starting tuning trial number #170 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 116, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0025785839499909663, 'batch_first': True, 'batch_size': 104, 'dropout': 0.23071896303993406, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 35}
Converting dataset to dataloader using batch size 104.
Initializing LSTM with 3 layers, 116 hidden size, 0.0025785839499909663 learning rate, 0.23071896303993406 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:11
Starting tuning trial number #171 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 319, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.012871190070022897, 'batch_first': True, 'batch_size': 2, 'dropout': 0.24668837379513442, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 2.
Initializing LSTM with 4 layers, 319 hidden size, 0.012871190070022897 learning rate, 0.24668837379513442 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:13
Starting tuning trial number #172 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.056908048429781664, 'batch_first': True, 'batch_size': 132, 'dropout': 0.16038266025918457, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 31}
Converting dataset to dataloader using batch size 132.
Initializing LSTM with 4 layers, 42 hidden size, 0.056908048429781664 learning rate, 0.16038266025918457 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:15
Starting tuning trial number #173 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 65, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.04290009732338126, 'batch_first': True, 'batch_size': 114, 'dropout': 0.13437782509744753, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 114.
Initializing LSTM with 5 layers, 65 hidden size, 0.04290009732338126 learning rate, 0.13437782509744753 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:17
Starting tuning trial number #174 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 455, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 13, 'learning_rate': 0.032879636525053116, 'batch_first': True, 'batch_size': 182, 'dropout': 0.08795065017793453, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 182.
Initializing LSTM with 13 layers, 455 hidden size, 0.032879636525053116 learning rate, 0.08795065017793453 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:19
Starting tuning trial number #175 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 50, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 27, 'learning_rate': 0.056910740646630834, 'batch_first': True, 'batch_size': 253, 'dropout': 0.1775817203108027, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 253.
Initializing LSTM with 27 layers, 50 hidden size, 0.056910740646630834 learning rate, 0.1775817203108027 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:10:21
Ending timer at: 18:11:38 
Elapsed CPU time: 514.4710593166678s
Used real time: 0:01:17
------------------
Starting tuning trial number #176 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.09936358244886874, 'batch_first': True, 'batch_size': 237, 'dropout': 0.17914211905461647, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 91}
Converting dataset to dataloader using batch size 237.
Initializing LSTM with 2 layers, 37 hidden size, 0.09936358244886874 learning rate, 0.17914211905461647 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:39
Starting tuning trial number #177 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 49, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 7.891825620942253e-05, 'batch_first': True, 'batch_size': 251, 'dropout': 0.18955657502782078, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 251.
Initializing LSTM with 3 layers, 49 hidden size, 7.891825620942253e-05 learning rate, 0.18955657502782078 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:41
Starting tuning trial number #178 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 19, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.07269513553100057, 'batch_first': True, 'batch_size': 213, 'dropout': 0.36696862848863154, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 88}
Converting dataset to dataloader using batch size 213.
Initializing LSTM with 1 layers, 19 hidden size, 0.07269513553100057 learning rate, 0.36696862848863154 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:43
Starting tuning trial number #179 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 75, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0009038786737284927, 'batch_first': True, 'batch_size': 65, 'dropout': 0.2774972180855009, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 65.
Initializing LSTM with 4 layers, 75 hidden size, 0.0009038786737284927 learning rate, 0.2774972180855009 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:45
Starting tuning trial number #180 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 190, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0014511414675925194, 'batch_first': True, 'batch_size': 221, 'dropout': 0.10975127698324011, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 221.
Initializing LSTM with 5 layers, 190 hidden size, 0.0014511414675925194 learning rate, 0.10975127698324011 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:49
Starting tuning trial number #181 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 92, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 20, 'learning_rate': 0.000568952312034322, 'batch_first': True, 'batch_size': 154, 'dropout': 0.17527810367902497, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 42}
Converting dataset to dataloader using batch size 154.
Initializing LSTM with 20 layers, 92 hidden size, 0.000568952312034322 learning rate, 0.17527810367902497 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:51
Starting tuning trial number #182 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 59, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 27, 'learning_rate': 0.05625527851424147, 'batch_first': True, 'batch_size': 125, 'dropout': 0.2007794122744608, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 125.
Initializing LSTM with 27 layers, 59 hidden size, 0.05625527851424147 learning rate, 0.2007794122744608 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:11:59
Starting tuning trial number #183 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 30, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 23, 'learning_rate': 0.04696838319151102, 'batch_first': True, 'batch_size': 338, 'dropout': 0.2225972119837464, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 28}
Converting dataset to dataloader using batch size 338.
Initializing LSTM with 23 layers, 30 hidden size, 0.04696838319151102 learning rate, 0.2225972119837464 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:12:01
Starting tuning trial number #184 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 83, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.026425441222477124, 'batch_first': True, 'batch_size': 136, 'dropout': 0.2591544993424399, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 136.
Initializing LSTM with 6 layers, 83 hidden size, 0.026425441222477124 learning rate, 0.2591544993424399 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:12:03
Starting tuning trial number #185 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 48, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0351533140714055, 'batch_first': True, 'batch_size': 187, 'dropout': 0.15271747238598093, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 187.
Initializing LSTM with 5 layers, 48 hidden size, 0.0351533140714055 learning rate, 0.15271747238598093 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:12:05
Ending timer at: 18:13:02 
Elapsed CPU time: 372.8243449166674s
Used real time: 0:00:57
------------------
Starting tuning trial number #186 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 51, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.035441686149466606, 'batch_first': True, 'batch_size': 200, 'dropout': 0.15461080520023202, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 200.
Initializing LSTM with 5 layers, 51 hidden size, 0.035441686149466606 learning rate, 0.15461080520023202 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:02
Starting tuning trial number #187 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 214, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 17, 'learning_rate': 2.3835218266271344e-05, 'batch_first': True, 'batch_size': 177, 'dropout': 0.12603501030144126, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 177.
Initializing LSTM with 17 layers, 214 hidden size, 2.3835218266271344e-05 learning rate, 0.12603501030144126 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:04
Starting tuning trial number #188 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 69, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.01912603459877942, 'batch_first': True, 'batch_size': 298, 'dropout': 0.14891497272579163, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 18}
Converting dataset to dataloader using batch size 298.
Initializing LSTM with 3 layers, 69 hidden size, 0.01912603459877942 learning rate, 0.14891497272579163 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:07
Starting tuning trial number #189 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 28, 'learning_rate': 0.0010732106483547588, 'batch_first': True, 'batch_size': 190, 'dropout': 0.14160338473348325, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 190.
Initializing LSTM with 28 layers, 37 hidden size, 0.0010732106483547588 learning rate, 0.14160338473348325 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:14
Starting tuning trial number #190 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 102, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 3.343458423641557e-07, 'batch_first': True, 'batch_size': 190, 'dropout': 0.16699048298318908, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 38}
Converting dataset to dataloader using batch size 190.
Initializing LSTM with 6 layers, 102 hidden size, 3.343458423641557e-07 learning rate, 0.16699048298318908 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:16
Starting tuning trial number #191 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 136, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.003526646788785786, 'batch_first': True, 'batch_size': 172, 'dropout': 0.26737418818430025, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 15}
Converting dataset to dataloader using batch size 172.
Initializing LSTM with 4 layers, 136 hidden size, 0.003526646788785786 learning rate, 0.26737418818430025 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:18
Starting tuning trial number #192 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 63, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.05850993848279647, 'batch_first': True, 'batch_size': 348, 'dropout': 0.1927958658890238, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 348.
Initializing LSTM with 5 layers, 63 hidden size, 0.05850993848279647 learning rate, 0.1927958658890238 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:20
Starting tuning trial number #193 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 45, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.08156365705492725, 'batch_first': True, 'batch_size': 119, 'dropout': 0.183861512128754, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 22}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 7 layers, 45 hidden size, 0.08156365705492725 learning rate, 0.183861512128754 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:22
Starting tuning trial number #194 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 60, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.031193738831997768, 'batch_first': True, 'batch_size': 263, 'dropout': 0.43997572487396946, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 263.
Initializing LSTM with 4 layers, 60 hidden size, 0.031193738831997768 learning rate, 0.43997572487396946 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:23
Starting tuning trial number #195 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 79, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.04571663741590579, 'batch_first': True, 'batch_size': 109, 'dropout': 0.29794371390140195, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 21}
Converting dataset to dataloader using batch size 109.
Initializing LSTM with 7 layers, 79 hidden size, 0.04571663741590579 learning rate, 0.29794371390140195 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:25
Starting tuning trial number #196 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 51, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.019560897516967796, 'batch_first': True, 'batch_size': 182, 'dropout': 0.17392181446394891, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 81}
Converting dataset to dataloader using batch size 182.
Initializing LSTM with 9 layers, 51 hidden size, 0.019560897516967796 learning rate, 0.17392181446394891 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:27
Starting tuning trial number #197 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 74, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0007957331772389506, 'batch_first': True, 'batch_size': 77, 'dropout': 0.2849240992302823, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 97}
Converting dataset to dataloader using batch size 77.
Initializing LSTM with 5 layers, 74 hidden size, 0.0007957331772389506 learning rate, 0.2849240992302823 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:30
Starting tuning trial number #198 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 23, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0003241301029499783, 'batch_first': True, 'batch_size': 162, 'dropout': 0.21529319858109774, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 33}
Converting dataset to dataloader using batch size 162.
Initializing LSTM with 3 layers, 23 hidden size, 0.0003241301029499783 learning rate, 0.21529319858109774 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:32
Starting tuning trial number #199 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 11, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.002149846950949214, 'batch_first': True, 'batch_size': 97, 'dropout': 0.16377540942618482, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 27}
Converting dataset to dataloader using batch size 97.
Initializing LSTM with 6 layers, 11 hidden size, 0.002149846950949214 learning rate, 0.16377540942618482 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:34
Starting tuning trial number #200 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 36, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.03739485439109596, 'batch_first': True, 'batch_size': 127, 'dropout': 0.13918960959468216, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 127.
Initializing LSTM with 3 layers, 36 hidden size, 0.03739485439109596 learning rate, 0.13918960959468216 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:13:36
Ending timer at: 18:16:42 
Elapsed CPU time: 1184.1415644166623s
Used real time: 0:03:06
------------------
Starting tuning trial number #201 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 34, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.006451747005095959, 'batch_first': True, 'batch_size': 48, 'dropout': 0.13763490832964964, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 48.
Initializing LSTM with 2 layers, 34 hidden size, 0.006451747005095959 learning rate, 0.13763490832964964 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:16:43
Starting tuning trial number #202 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.02436174470466572, 'batch_first': True, 'batch_size': 124, 'dropout': 0.11986517445689046, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 4 layers, 42 hidden size, 0.02436174470466572 learning rate, 0.11986517445689046 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:16:50
Starting tuning trial number #203 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.038529896041745615, 'batch_first': True, 'batch_size': 129, 'dropout': 0.1551834546006178, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 4 layers, 54 hidden size, 0.038529896041745615 learning rate, 0.1551834546006178 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:16:52
Ending timer at: 18:20:03 
Elapsed CPU time: 1273.7433800500032s
Used real time: 0:03:11
------------------
Starting tuning trial number #204 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 58, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.03039395181921906, 'batch_first': True, 'batch_size': 131, 'dropout': 0.1499751091531134, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 131.
Initializing LSTM with 3 layers, 58 hidden size, 0.03039395181921906 learning rate, 0.1499751091531134 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:04
Starting tuning trial number #205 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 230, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.03846479755797594, 'batch_first': True, 'batch_size': 117, 'dropout': 0.15698526336897956, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 117.
Initializing LSTM with 3 layers, 230 hidden size, 0.03846479755797594 learning rate, 0.15698526336897956 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:06
Starting tuning trial number #206 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 26, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.01419936806994263, 'batch_first': True, 'batch_size': 58, 'dropout': 0.12404583291754388, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 58.
Initializing LSTM with 2 layers, 26 hidden size, 0.01419936806994263 learning rate, 0.12404583291754388 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:08
Starting tuning trial number #207 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 200, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.04620884160110957, 'batch_first': True, 'batch_size': 146, 'dropout': 0.25040256748223194, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 146.
Initializing LSTM with 4 layers, 200 hidden size, 0.04620884160110957 learning rate, 0.25040256748223194 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:10
Starting tuning trial number #208 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 48, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.05501964244507766, 'batch_first': True, 'batch_size': 140, 'dropout': 0.13260280928745646, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 140.
Initializing LSTM with 5 layers, 48 hidden size, 0.05501964244507766 learning rate, 0.13260280928745646 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:12
Starting tuning trial number #209 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 85, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.010229230461741219, 'batch_first': True, 'batch_size': 205, 'dropout': 0.16351898213367433, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 205.
Initializing LSTM with 4 layers, 85 hidden size, 0.010229230461741219 learning rate, 0.16351898213367433 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:14
Starting tuning trial number #210 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 35, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0724568259974683, 'batch_first': True, 'batch_size': 128, 'dropout': 0.140159746532472, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 128.
Initializing LSTM with 3 layers, 35 hidden size, 0.0724568259974683 learning rate, 0.140159746532472 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:16
Starting tuning trial number #211 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 71, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0011389738372623724, 'batch_first': True, 'batch_size': 116, 'dropout': 0.313738439452801, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 116.
Initializing LSTM with 6 layers, 71 hidden size, 0.0011389738372623724 learning rate, 0.313738439452801 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:18
Starting tuning trial number #212 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 56, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.025297035482096837, 'batch_first': True, 'batch_size': 230, 'dropout': 0.2716823080386386, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 230.
Initializing LSTM with 5 layers, 56 hidden size, 0.025297035482096837 learning rate, 0.2716823080386386 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:20
Starting tuning trial number #213 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 65, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.039121186002517495, 'batch_first': True, 'batch_size': 134, 'dropout': 0.1840115296220355, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 134.
Initializing LSTM with 4 layers, 65 hidden size, 0.039121186002517495 learning rate, 0.1840115296220355 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:22
Starting tuning trial number #214 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.09797175797958699, 'batch_first': True, 'batch_size': 109, 'dropout': 0.39702175717989685, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 76}
Converting dataset to dataloader using batch size 109.
Initializing LSTM with 5 layers, 41 hidden size, 0.09797175797958699 learning rate, 0.39702175717989685 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:24
Starting tuning trial number #215 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 51, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.060134076670023386, 'batch_first': True, 'batch_size': 124, 'dropout': 0.23948690233613076, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 3 layers, 51 hidden size, 0.060134076670023386 learning rate, 0.23948690233613076 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:31
Starting tuning trial number #216 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 93, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0005969432182997988, 'batch_first': True, 'batch_size': 114, 'dropout': 0.2025425881471776, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 114.
Initializing LSTM with 4 layers, 93 hidden size, 0.0005969432182997988 learning rate, 0.2025425881471776 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:33
Starting tuning trial number #217 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 218, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.03832124391076614, 'batch_first': True, 'batch_size': 137, 'dropout': 0.25567799356826043, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 24}
Converting dataset to dataloader using batch size 137.
Initializing LSTM with 6 layers, 218 hidden size, 0.03832124391076614 learning rate, 0.25567799356826043 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:35
Starting tuning trial number #218 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 63, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.001649236585898708, 'batch_first': True, 'batch_size': 87, 'dropout': 0.07587029776615763, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 87.
Initializing LSTM with 4 layers, 63 hidden size, 0.001649236585898708 learning rate, 0.07587029776615763 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:37
Starting tuning trial number #219 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 78, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0008389196856809555, 'batch_first': True, 'batch_size': 68, 'dropout': 0.17254078094183736, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 30}
Converting dataset to dataloader using batch size 68.
Initializing LSTM with 2 layers, 78 hidden size, 0.0008389196856809555 learning rate, 0.17254078094183736 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:39
Starting tuning trial number #220 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 29, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.06983752738563581, 'batch_first': True, 'batch_size': 150, 'dropout': 0.1433151804226509, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 73}
Converting dataset to dataloader using batch size 150.
Initializing LSTM with 8 layers, 29 hidden size, 0.06983752738563581 learning rate, 0.1433151804226509 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:42
Starting tuning trial number #221 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.00013832317281435783, 'batch_first': True, 'batch_size': 279, 'dropout': 0.010911125754163632, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 279.
Initializing LSTM with 5 layers, 46 hidden size, 0.00013832317281435783 learning rate, 0.010911125754163632 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:44
Starting tuning trial number #222 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 180, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.09587112737315892, 'batch_first': True, 'batch_size': 64, 'dropout': 0.15301742401622942, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 94}
Converting dataset to dataloader using batch size 64.
Initializing LSTM with 3 layers, 180 hidden size, 0.09587112737315892 learning rate, 0.15301742401622942 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:48
Starting tuning trial number #223 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 188, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.05240136526854535, 'batch_first': True, 'batch_size': 42, 'dropout': 0.09446435722913943, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 42.
Initializing LSTM with 3 layers, 188 hidden size, 0.05240136526854535 learning rate, 0.09446435722913943 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:50
Starting tuning trial number #224 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 147, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.07494594611044662, 'batch_first': True, 'batch_size': 59, 'dropout': 0.06619772489787198, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 84}
Converting dataset to dataloader using batch size 59.
Initializing LSTM with 4 layers, 147 hidden size, 0.07494594611044662 learning rate, 0.06619772489787198 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:51
Starting tuning trial number #225 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 206, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.032514126046632556, 'batch_first': True, 'batch_size': 72, 'dropout': 0.05443320359906384, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 72.
Initializing LSTM with 3 layers, 206 hidden size, 0.032514126046632556 learning rate, 0.05443320359906384 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:53
Starting tuning trial number #226 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 400, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.02130017039897603, 'batch_first': True, 'batch_size': 123, 'dropout': 0.2645815099256547, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 123.
Initializing LSTM with 2 layers, 400 hidden size, 0.02130017039897603 learning rate, 0.2645815099256547 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:55
Starting tuning trial number #227 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 166, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.00043776620784206444, 'batch_first': True, 'batch_size': 132, 'dropout': 0.107884888796884, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 13}
Converting dataset to dataloader using batch size 132.
Initializing LSTM with 5 layers, 166 hidden size, 0.00043776620784206444 learning rate, 0.107884888796884 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:57
Starting tuning trial number #228 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 59, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 12, 'learning_rate': 0.05028193281784104, 'batch_first': True, 'batch_size': 186, 'dropout': 0.12851468182529763, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 186.
Initializing LSTM with 12 layers, 59 hidden size, 0.05028193281784104 learning rate, 0.12851468182529763 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:20:59
Starting tuning trial number #229 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 72, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 25, 'learning_rate': 0.0010757371231720533, 'batch_first': True, 'batch_size': 102, 'dropout': 0.08618115550436421, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 38}
Converting dataset to dataloader using batch size 102.
Initializing LSTM with 25 layers, 72 hidden size, 0.0010757371231720533 learning rate, 0.08618115550436421 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:21:01
Starting tuning trial number #230 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 195, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0026729266196674987, 'batch_first': True, 'batch_size': 317, 'dropout': 0.23243110529545294, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 45}
Converting dataset to dataloader using batch size 317.
Initializing LSTM with 6 layers, 195 hidden size, 0.0026729266196674987 learning rate, 0.23243110529545294 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:22:54
Starting tuning trial number #231 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 86, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0013798611156371028, 'batch_first': True, 'batch_size': 31, 'dropout': 0.3458251909510952, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 35}
Converting dataset to dataloader using batch size 31.
Initializing LSTM with 5 layers, 86 hidden size, 0.0013798611156371028 learning rate, 0.3458251909510952 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:22:57
Starting tuning trial number #232 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 344, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 29, 'learning_rate': 1.6574035852220733e-07, 'batch_first': True, 'batch_size': 262, 'dropout': 0.24386476652450242, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 22}
Converting dataset to dataloader using batch size 262.
Initializing LSTM with 29 layers, 344 hidden size, 1.6574035852220733e-07 learning rate, 0.24386476652450242 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:22:59
Starting tuning trial number #233 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0006874577179757931, 'batch_first': True, 'batch_size': 119, 'dropout': 0.27531267252190494, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 4 layers, 42 hidden size, 0.0006874577179757931 learning rate, 0.27531267252190494 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:03
Starting tuning trial number #234 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 467, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 27, 'learning_rate': 0.0039739524856413876, 'batch_first': True, 'batch_size': 194, 'dropout': 0.2557394655308285, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 46}
Converting dataset to dataloader using batch size 194.
Initializing LSTM with 27 layers, 467 hidden size, 0.0039739524856413876 learning rate, 0.2557394655308285 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:05
Starting tuning trial number #235 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 497, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 30, 'learning_rate': 0.0048979499430456255, 'batch_first': True, 'batch_size': 127, 'dropout': 0.28884138169196183, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 127.
Initializing LSTM with 30 layers, 497 hidden size, 0.0048979499430456255 learning rate, 0.28884138169196183 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:07
Starting tuning trial number #236 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 242, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 11, 'learning_rate': 3.421866462621902e-06, 'batch_first': True, 'batch_size': 50, 'dropout': 0.22045707602154765, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 48}
Converting dataset to dataloader using batch size 50.
Initializing LSTM with 11 layers, 242 hidden size, 3.421866462621902e-06 learning rate, 0.22045707602154765 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:09
Starting tuning trial number #237 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 493, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 22, 'learning_rate': 4.7857552014223234e-06, 'batch_first': True, 'batch_size': 304, 'dropout': 0.20962453396960257, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 16}
Converting dataset to dataloader using batch size 304.
Initializing LSTM with 22 layers, 493 hidden size, 4.7857552014223234e-06 learning rate, 0.20962453396960257 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:12
Starting tuning trial number #238 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 30, 'learning_rate': 5.385831186711807e-07, 'batch_first': True, 'batch_size': 349, 'dropout': 0.1915067974063188, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 349.
Initializing LSTM with 30 layers, 54 hidden size, 5.385831186711807e-07 learning rate, 0.1915067974063188 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:13
Starting tuning trial number #239 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 485, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 28, 'learning_rate': 0.032447298807985, 'batch_first': True, 'batch_size': 328, 'dropout': 0.4833726583538146, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 328.
Initializing LSTM with 28 layers, 485 hidden size, 0.032447298807985 learning rate, 0.4833726583538146 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:21
Starting tuning trial number #240 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 266, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 8.231743946347158e-06, 'batch_first': True, 'batch_size': 58, 'dropout': 0.231915507294089, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 58.
Initializing LSTM with 7 layers, 266 hidden size, 8.231743946347158e-06 learning rate, 0.231915507294089 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:25
Starting tuning trial number #241 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 475, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.06329101700823532, 'batch_first': True, 'batch_size': 77, 'dropout': 0.3312647078552586, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 77.
Initializing LSTM with 4 layers, 475 hidden size, 0.06329101700823532 learning rate, 0.3312647078552586 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:27
Starting tuning trial number #242 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 212, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 14, 'learning_rate': 1.0299558513147516e-06, 'batch_first': True, 'batch_size': 271, 'dropout': 0.09719019965078479, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 271.
Initializing LSTM with 14 layers, 212 hidden size, 1.0299558513147516e-06 learning rate, 0.09719019965078479 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:29
Starting tuning trial number #243 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 174, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 29, 'learning_rate': 1.9131310543612056e-07, 'batch_first': True, 'batch_size': 291, 'dropout': 0.07519848596728702, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 291.
Initializing LSTM with 29 layers, 174 hidden size, 1.9131310543612056e-07 learning rate, 0.07519848596728702 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:31
Starting tuning trial number #244 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 185, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 12, 'learning_rate': 0.0009095485743423883, 'batch_first': True, 'batch_size': 245, 'dropout': 0.2625348013892119, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 245.
Initializing LSTM with 12 layers, 185 hidden size, 0.0009095485743423883 learning rate, 0.2625348013892119 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:33
Starting tuning trial number #245 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 199, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 1.9487501171970258e-07, 'batch_first': True, 'batch_size': 279, 'dropout': 0.10458725889193318, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 279.
Initializing LSTM with 10 layers, 199 hidden size, 1.9487501171970258e-07 learning rate, 0.10458725889193318 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:35
Starting tuning trial number #246 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 159, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 2.730265611808969e-07, 'batch_first': True, 'batch_size': 281, 'dropout': 0.08253289612439241, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 10}
Converting dataset to dataloader using batch size 281.
Initializing LSTM with 1 layers, 159 hidden size, 2.730265611808969e-07 learning rate, 0.08253289612439241 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:37
Starting tuning trial number #247 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 67, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 2.7808822295778857e-07, 'batch_first': True, 'batch_size': 178, 'dropout': 0.15893613498287965, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 33}
Converting dataset to dataloader using batch size 178.
Initializing LSTM with 5 layers, 67 hidden size, 2.7808822295778857e-07 learning rate, 0.15893613498287965 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:39
Starting tuning trial number #248 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 182, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.04473305992108153, 'batch_first': True, 'batch_size': 332, 'dropout': 0.25143414387438945, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 332.
Initializing LSTM with 3 layers, 182 hidden size, 0.04473305992108153 learning rate, 0.25143414387438945 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:41
Starting tuning trial number #249 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 444, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.07994480735777532, 'batch_first': True, 'batch_size': 115, 'dropout': 0.178399775952369, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 41}
Converting dataset to dataloader using batch size 115.
Initializing LSTM with 4 layers, 444 hidden size, 0.07994480735777532 learning rate, 0.178399775952369 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:43
Starting tuning trial number #250 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 99, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0019397491039533235, 'batch_first': True, 'batch_size': 255, 'dropout': 0.028400313709210923, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 255.
Initializing LSTM with 6 layers, 99 hidden size, 0.0019397491039533235 learning rate, 0.028400313709210923 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:44
Starting tuning trial number #251 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 225, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 13, 'learning_rate': 1.6438319337755102e-07, 'batch_first': True, 'batch_size': 221, 'dropout': 0.1977133631830065, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 96}
Converting dataset to dataloader using batch size 221.
Initializing LSTM with 13 layers, 225 hidden size, 1.6438319337755102e-07 learning rate, 0.1977133631830065 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:46
Starting tuning trial number #252 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 38, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0005619053287818454, 'batch_first': True, 'batch_size': 67, 'dropout': 0.11446069963027702, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 67.
Initializing LSTM with 3 layers, 38 hidden size, 0.0005619053287818454 learning rate, 0.11446069963027702 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:49
Starting tuning trial number #253 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 80, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0002422334357315364, 'batch_first': True, 'batch_size': 167, 'dropout': 0.141833765395526, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 167.
Initializing LSTM with 4 layers, 80 hidden size, 0.0002422334357315364 learning rate, 0.141833765395526 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:50
Starting tuning trial number #254 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 54, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 15, 'learning_rate': 1.2770330799090728e-07, 'batch_first': True, 'batch_size': 142, 'dropout': 0.2768884453020037, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 26}
Converting dataset to dataloader using batch size 142.
Initializing LSTM with 15 layers, 54 hidden size, 1.2770330799090728e-07 learning rate, 0.2768884453020037 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:53
Starting tuning trial number #255 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 26, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0163668323152147, 'batch_first': True, 'batch_size': 300, 'dropout': 0.08775811446475963, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 89}
Converting dataset to dataloader using batch size 300.
Initializing LSTM with 7 layers, 26 hidden size, 0.0163668323152147 learning rate, 0.08775811446475963 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:55
Starting tuning trial number #256 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 127, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0014161827479619847, 'batch_first': True, 'batch_size': 200, 'dropout': 0.27010514229335214, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 29}
Converting dataset to dataloader using batch size 200.
Initializing LSTM with 6 layers, 127 hidden size, 0.0014161827479619847 learning rate, 0.27010514229335214 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:57
Starting tuning trial number #257 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 206, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.023729012149641848, 'batch_first': True, 'batch_size': 107, 'dropout': 0.2420734231806017, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 18}
Converting dataset to dataloader using batch size 107.
Initializing LSTM with 2 layers, 206 hidden size, 0.023729012149641848 learning rate, 0.2420734231806017 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:23:59
Starting tuning trial number #258 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 45, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.03793062444746051, 'batch_first': True, 'batch_size': 129, 'dropout': 0.1493859351119225, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 36}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 5 layers, 45 hidden size, 0.03793062444746051 learning rate, 0.1493859351119225 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:01
Starting tuning trial number #259 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 110, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.09702167021927463, 'batch_first': True, 'batch_size': 121, 'dropout': 0.1717627652783268, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 74}
Converting dataset to dataloader using batch size 121.
Initializing LSTM with 4 layers, 110 hidden size, 0.09702167021927463 learning rate, 0.1717627652783268 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:03
Starting tuning trial number #260 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 189, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 29, 'learning_rate': 0.0008483865708489078, 'batch_first': True, 'batch_size': 320, 'dropout': 0.05652269659919129, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 320.
Initializing LSTM with 29 layers, 189 hidden size, 0.0008483865708489078 learning rate, 0.05652269659919129 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:10
Starting tuning trial number #261 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 64, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.061723439686065036, 'batch_first': True, 'batch_size': 305, 'dropout': 0.35957495629362507, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 305.
Initializing LSTM with 2 layers, 64 hidden size, 0.061723439686065036 learning rate, 0.35957495629362507 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:12
Starting tuning trial number #262 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 499, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0004011102865520678, 'batch_first': True, 'batch_size': 360, 'dropout': 0.13408785913879603, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 360.
Initializing LSTM with 5 layers, 499 hidden size, 0.0004011102865520678 learning rate, 0.13408785913879603 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:14
Starting tuning trial number #263 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 173, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 1.0360835555547447e-07, 'batch_first': True, 'batch_size': 157, 'dropout': 0.22851352749310194, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 157.
Initializing LSTM with 3 layers, 173 hidden size, 1.0360835555547447e-07 learning rate, 0.22851352749310194 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:17
Starting tuning trial number #264 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 89, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 26, 'learning_rate': 0.002782656823278421, 'batch_first': True, 'batch_size': 185, 'dropout': 0.07002847351689989, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 185.
Initializing LSTM with 26 layers, 89 hidden size, 0.002782656823278421 learning rate, 0.07002847351689989 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:19
Starting tuning trial number #265 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 69, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0010807317001686656, 'batch_first': True, 'batch_size': 90, 'dropout': 0.12278783278553086, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 90.
Initializing LSTM with 4 layers, 69 hidden size, 0.0010807317001686656 learning rate, 0.12278783278553086 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:20
Starting tuning trial number #266 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 34, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.02834791610881894, 'batch_first': True, 'batch_size': 136, 'dropout': 0.253231439645033, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 136.
Initializing LSTM with 3 layers, 34 hidden size, 0.02834791610881894 learning rate, 0.253231439645033 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:22
Starting tuning trial number #267 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 19, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.04909247065016525, 'batch_first': True, 'batch_size': 236, 'dropout': 0.2887603707856717, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 44}
Converting dataset to dataloader using batch size 236.
Initializing LSTM with 5 layers, 19 hidden size, 0.04909247065016525 learning rate, 0.2887603707856717 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:30
Starting tuning trial number #268 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 53, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 16, 'learning_rate': 1.4976380248561024e-05, 'batch_first': True, 'batch_size': 210, 'dropout': 0.1886946187075308, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 92}
Converting dataset to dataloader using batch size 210.
Initializing LSTM with 16 layers, 53 hidden size, 1.4976380248561024e-05 learning rate, 0.1886946187075308 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:37
Starting tuning trial number #269 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 148, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 1.6843151229653632e-06, 'batch_first': True, 'batch_size': 112, 'dropout': 0.04472709120600573, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 112.
Initializing LSTM with 9 layers, 148 hidden size, 1.6843151229653632e-06 learning rate, 0.04472709120600573 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:39
Starting tuning trial number #270 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 76, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.001944891300927796, 'batch_first': True, 'batch_size': 122, 'dropout': 0.20279216792126664, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 122.
Initializing LSTM with 6 layers, 76 hidden size, 0.001944891300927796 learning rate, 0.20279216792126664 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:41
Starting tuning trial number #271 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 485, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0006808734864423333, 'batch_first': True, 'batch_size': 53, 'dropout': 0.2612683054047473, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 27}
Converting dataset to dataloader using batch size 53.
Initializing LSTM with 3 layers, 485 hidden size, 0.0006808734864423333 learning rate, 0.2612683054047473 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:43
Starting tuning trial number #272 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 196, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.007600301951182021, 'batch_first': True, 'batch_size': 82, 'dropout': 0.30106855302158875, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 82.
Initializing LSTM with 8 layers, 196 hidden size, 0.007600301951182021 learning rate, 0.30106855302158875 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:45
Starting tuning trial number #273 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 46, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0014078995559782452, 'batch_first': True, 'batch_size': 129, 'dropout': 0.16271322085506595, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 4 layers, 46 hidden size, 0.0014078995559782452 learning rate, 0.16271322085506595 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:47
Starting tuning trial number #274 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 59, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.03840452370451802, 'batch_first': True, 'batch_size': 97, 'dropout': 0.21594175863925072, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 32}
Converting dataset to dataloader using batch size 97.
Initializing LSTM with 7 layers, 59 hidden size, 0.03840452370451802 learning rate, 0.21594175863925072 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:49
Starting tuning trial number #275 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 213, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.07233802764446742, 'batch_first': True, 'batch_size': 65, 'dropout': 0.23903387432889683, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 15}
Converting dataset to dataloader using batch size 65.
Initializing LSTM with 4 layers, 213 hidden size, 0.07233802764446742 learning rate, 0.23903387432889683 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:51
Starting tuning trial number #276 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 35, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.005082062716927866, 'batch_first': True, 'batch_size': 73, 'dropout': 0.09389139251262614, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 73.
Initializing LSTM with 1 layers, 35 hidden size, 0.005082062716927866 learning rate, 0.09389139251262614 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:53
Starting tuning trial number #277 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 300, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.003300104140647672, 'batch_first': True, 'batch_size': 105, 'dropout': 0.2806626327604407, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 105.
Initializing LSTM with 2 layers, 300 hidden size, 0.003300104140647672 learning rate, 0.2806626327604407 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:55
Starting tuning trial number #278 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 91, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 18, 'learning_rate': 4.93392322878657e-07, 'batch_first': True, 'batch_size': 172, 'dropout': 0.10484856429683716, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 80}
Converting dataset to dataloader using batch size 172.
Initializing LSTM with 18 layers, 91 hidden size, 4.93392322878657e-07 learning rate, 0.10484856429683716 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:57
Starting tuning trial number #279 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 67, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.02009204730635484, 'batch_first': True, 'batch_size': 195, 'dropout': 0.3169779405425622, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 49}
Converting dataset to dataloader using batch size 195.
Initializing LSTM with 5 layers, 67 hidden size, 0.02009204730635484 learning rate, 0.3169779405425622 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:24:59
Starting tuning trial number #280 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 80, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.00018831289729907155, 'batch_first': True, 'batch_size': 119, 'dropout': 0.2685189624243453, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 5 layers, 80 hidden size, 0.00018831289729907155 learning rate, 0.2685189624243453 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:07
Starting tuning trial number #281 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 101, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.05676970798506439, 'batch_first': True, 'batch_size': 148, 'dropout': 0.14221666765740582, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 148.
Initializing LSTM with 6 layers, 101 hidden size, 0.05676970798506439 learning rate, 0.14221666765740582 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:09
Starting tuning trial number #282 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 166, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0004742901330826308, 'batch_first': True, 'batch_size': 256, 'dropout': 0.15468052398842685, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 8}
Converting dataset to dataloader using batch size 256.
Initializing LSTM with 3 layers, 166 hidden size, 0.0004742901330826308 learning rate, 0.15468052398842685 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:11
Starting tuning trial number #283 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 224, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.000965899570311946, 'batch_first': True, 'batch_size': 135, 'dropout': 0.4486632019164742, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 46}
Converting dataset to dataloader using batch size 135.
Initializing LSTM with 7 layers, 224 hidden size, 0.000965899570311946 learning rate, 0.4486632019164742 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:13
Starting tuning trial number #284 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 49, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.029340880777917085, 'batch_first': True, 'batch_size': 126, 'dropout': 0.41192373332317533, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 39}
Converting dataset to dataloader using batch size 126.
Initializing LSTM with 4 layers, 49 hidden size, 0.029340880777917085 learning rate, 0.41192373332317533 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:15
Starting tuning trial number #285 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 60, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 11, 'learning_rate': 0.0011844412671415479, 'batch_first': True, 'batch_size': 113, 'dropout': 0.18309768377998925, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 21}
Converting dataset to dataloader using batch size 113.
Initializing LSTM with 11 layers, 60 hidden size, 0.0011844412671415479 learning rate, 0.18309768377998925 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:17
Starting tuning trial number #286 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0007283641201290598, 'batch_first': True, 'batch_size': 183, 'dropout': 0.24875364852270132, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 183.
Initializing LSTM with 2 layers, 42 hidden size, 0.0007283641201290598 learning rate, 0.24875364852270132 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:19
Starting tuning trial number #287 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 118, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.010998099745975907, 'batch_first': True, 'batch_size': 61, 'dropout': 0.1302054323205103, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 42}
Converting dataset to dataloader using batch size 61.
Initializing LSTM with 3 layers, 118 hidden size, 0.010998099745975907 learning rate, 0.1302054323205103 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:28
Starting tuning trial number #288 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 253, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.002214099368861098, 'batch_first': True, 'batch_size': 289, 'dropout': 0.22308002884603553, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 25}
Converting dataset to dataloader using batch size 289.
Initializing LSTM with 5 layers, 253 hidden size, 0.002214099368861098 learning rate, 0.22308002884603553 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:30
Starting tuning trial number #289 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 28, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0016869722036413227, 'batch_first': True, 'batch_size': 162, 'dropout': 0.08011997377403463, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 162.
Initializing LSTM with 6 layers, 28 hidden size, 0.0016869722036413227 learning rate, 0.08011997377403463 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:32
Starting tuning trial number #290 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 75, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0002942703929306402, 'batch_first': True, 'batch_size': 79, 'dropout': 0.11356365146697596, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 79.
Initializing LSTM with 4 layers, 75 hidden size, 0.0002942703929306402 learning rate, 0.11356365146697596 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:34
Starting tuning trial number #291 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 139, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0761271922069915, 'batch_first': True, 'batch_size': 139, 'dropout': 0.2091292637880998, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 139.
Initializing LSTM with 6 layers, 139 hidden size, 0.0761271922069915 learning rate, 0.2091292637880998 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:36
Starting tuning trial number #292 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 201, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.03443650981621899, 'batch_first': True, 'batch_size': 124, 'dropout': 0.26001199734975555, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 12}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 5 layers, 201 hidden size, 0.03443650981621899 learning rate, 0.26001199734975555 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:25:38
Ending timer at: 18:26:14 
Elapsed CPU time: 249.84983803333307s
Used real time: 0:00:36
------------------
Starting tuning trial number #293 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 208, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.04127894426496226, 'batch_first': True, 'batch_size': 121, 'dropout': 0.2589656272553176, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 9}
Converting dataset to dataloader using batch size 121.
Initializing LSTM with 5 layers, 208 hidden size, 0.04127894426496226 learning rate, 0.2589656272553176 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:15
Starting tuning trial number #294 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 203, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.02837930379299083, 'batch_first': True, 'batch_size': 111, 'dropout': 0.2692452029248139, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 11}
Converting dataset to dataloader using batch size 111.
Initializing LSTM with 4 layers, 203 hidden size, 0.02837930379299083 learning rate, 0.2692452029248139 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:17
Starting tuning trial number #295 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 231, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.01809323485551834, 'batch_first': True, 'batch_size': 131, 'dropout': 0.24596044360717773, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 16}
Converting dataset to dataloader using batch size 131.
Initializing LSTM with 5 layers, 231 hidden size, 0.01809323485551834 learning rate, 0.24596044360717773 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:19
Starting tuning trial number #296 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 52, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.04130188117268412, 'batch_first': True, 'batch_size': 123, 'dropout': 0.2794929982144975, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 13}
Converting dataset to dataloader using batch size 123.
Initializing LSTM with 2 layers, 52 hidden size, 0.04130188117268412 learning rate, 0.2794929982144975 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:21
Starting tuning trial number #297 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 12, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.05039353543405255, 'batch_first': True, 'batch_size': 117, 'dropout': 0.23568076103334232, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 83}
Converting dataset to dataloader using batch size 117.
Initializing LSTM with 3 layers, 12 hidden size, 0.05039353543405255 learning rate, 0.23568076103334232 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:23
Starting tuning trial number #298 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 85, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.02450509726290733, 'batch_first': True, 'batch_size': 177, 'dropout': 0.2606369919854926, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 7}
Converting dataset to dataloader using batch size 177.
Initializing LSTM with 4 layers, 85 hidden size, 0.02450509726290733 learning rate, 0.2606369919854926 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:32
Starting tuning trial number #299 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 194, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.09590420679460147, 'batch_first': True, 'batch_size': 129, 'dropout': 0.14715835833230634, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 5}
Converting dataset to dataloader using batch size 129.
Initializing LSTM with 5 layers, 194 hidden size, 0.09590420679460147 learning rate, 0.14715835833230634 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:34
Starting tuning trial number #300 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 219, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0325346027629188, 'batch_first': True, 'batch_size': 190, 'dropout': 0.17197487557854646, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 190.
Initializing LSTM with 3 layers, 219 hidden size, 0.0325346027629188 learning rate, 0.17197487557854646 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:35
Starting tuning trial number #301 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 68, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.06374455611883999, 'batch_first': True, 'batch_size': 45, 'dropout': 0.16604776138539812, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 75}
Converting dataset to dataloader using batch size 45.
Initializing LSTM with 6 layers, 68 hidden size, 0.06374455611883999 learning rate, 0.16604776138539812 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:37
Starting tuning trial number #302 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 36, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0009038745596983342, 'batch_first': True, 'batch_size': 139, 'dropout': 0.252774872366119, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 12}
Converting dataset to dataloader using batch size 139.
Initializing LSTM with 4 layers, 36 hidden size, 0.0009038745596983342 learning rate, 0.252774872366119 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:40
Starting tuning trial number #303 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 239, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.04796383468325728, 'batch_first': True, 'batch_size': 107, 'dropout': 0.2710282269350395, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 22}
Converting dataset to dataloader using batch size 107.
Initializing LSTM with 5 layers, 239 hidden size, 0.04796383468325728 learning rate, 0.2710282269350395 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:42
Starting tuning trial number #304 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 63, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0005711675928317247, 'batch_first': True, 'batch_size': 101, 'dropout': 0.036081400998181745, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 101.
Initializing LSTM with 2 layers, 63 hidden size, 0.0005711675928317247 learning rate, 0.036081400998181745 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:44
Starting tuning trial number #305 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 53, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.01499114544084433, 'batch_first': True, 'batch_size': 152, 'dropout': 0.2949056779332351, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 90}
Converting dataset to dataloader using batch size 152.
Initializing LSTM with 7 layers, 53 hidden size, 0.01499114544084433 learning rate, 0.2949056779332351 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:46
Starting tuning trial number #306 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 77, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.03389740120355329, 'batch_first': True, 'batch_size': 117, 'dropout': 0.1963317061175193, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 14}
Converting dataset to dataloader using batch size 117.
Initializing LSTM with 4 layers, 77 hidden size, 0.03389740120355329 learning rate, 0.1963317061175193 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:48
Starting tuning trial number #307 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 24, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.005701758967687542, 'batch_first': True, 'batch_size': 73, 'dropout': 0.0009239930454757626, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 73.
Initializing LSTM with 3 layers, 24 hidden size, 0.005701758967687542 learning rate, 0.0009239930454757626 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:26:50
Ending timer at: 18:29:20 
Elapsed CPU time: 1005.7564591666732s
Used real time: 0:02:30
------------------
Starting tuning trial number #308 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 29, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 4.175900110991212e-05, 'batch_first': True, 'batch_size': 70, 'dropout': 0.020949960815537308, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 68}
Converting dataset to dataloader using batch size 70.
Initializing LSTM with 1 layers, 29 hidden size, 4.175900110991212e-05 learning rate, 0.020949960815537308 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:21
Starting tuning trial number #309 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 20, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.007243242757290226, 'batch_first': True, 'batch_size': 87, 'dropout': 0.003558370383687165, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 87.
Initializing LSTM with 3 layers, 20 hidden size, 0.007243242757290226 learning rate, 0.003558370383687165 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:29
Starting tuning trial number #310 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 5.979352902593346e-05, 'batch_first': True, 'batch_size': 58, 'dropout': 0.002521589420465683, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 18}
Converting dataset to dataloader using batch size 58.
Initializing LSTM with 3 layers, 41 hidden size, 5.979352902593346e-05 learning rate, 0.002521589420465683 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:31
Starting tuning trial number #311 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 19, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.00010889010454245858, 'batch_first': True, 'batch_size': 74, 'dropout': 0.010917469319045001, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 28}
Converting dataset to dataloader using batch size 74.
Initializing LSTM with 2 layers, 19 hidden size, 0.00010889010454245858 learning rate, 0.010917469319045001 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:33
Starting tuning trial number #312 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 34, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.005889511234003056, 'batch_first': True, 'batch_size': 65, 'dropout': 0.017351887190208873, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 72}
Converting dataset to dataloader using batch size 65.
Initializing LSTM with 4 layers, 34 hidden size, 0.005889511234003056 learning rate, 0.017351887190208873 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:35
Starting tuning trial number #313 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 48, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 8.843087996306675e-05, 'batch_first': True, 'batch_size': 203, 'dropout': 0.13241379617694032, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 70}
Converting dataset to dataloader using batch size 203.
Initializing LSTM with 3 layers, 48 hidden size, 8.843087996306675e-05 learning rate, 0.13241379617694032 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:37
Starting tuning trial number #314 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 94, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.004994565495380898, 'batch_first': True, 'batch_size': 52, 'dropout': 0.28335257807101005, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 24}
Converting dataset to dataloader using batch size 52.
Initializing LSTM with 6 layers, 94 hidden size, 0.004994565495380898 learning rate, 0.28335257807101005 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:39
Starting tuning trial number #315 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 58, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.004289857594595803, 'batch_first': True, 'batch_size': 81, 'dropout': 0.3806246069446624, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 88}
Converting dataset to dataloader using batch size 81.
Initializing LSTM with 5 layers, 58 hidden size, 0.004289857594595803 learning rate, 0.3806246069446624 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:41
Starting tuning trial number #316 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 24, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0031676756966996515, 'batch_first': True, 'batch_size': 145, 'dropout': 0.1597771854220008, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 100}
Converting dataset to dataloader using batch size 145.
Initializing LSTM with 4 layers, 24 hidden size, 0.0031676756966996515 learning rate, 0.1597771854220008 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:43
Starting tuning trial number #317 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 109, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.008901720861214836, 'batch_first': True, 'batch_size': 92, 'dropout': 0.12389501591657656, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 77}
Converting dataset to dataloader using batch size 92.
Initializing LSTM with 3 layers, 109 hidden size, 0.008901720861214836 learning rate, 0.12389501591657656 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:45
Starting tuning trial number #318 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 280, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.001325957068861356, 'batch_first': True, 'batch_size': 127, 'dropout': 0.00992317571144383, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 127.
Initializing LSTM with 5 layers, 280 hidden size, 0.001325957068861356 learning rate, 0.00992317571144383 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:47
Starting tuning trial number #319 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.02171714996110923, 'batch_first': True, 'batch_size': 38, 'dropout': 0.1905084416927096, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 38.
Initializing LSTM with 4 layers, 42 hidden size, 0.02171714996110923 learning rate, 0.1905084416927096 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:49
Starting tuning trial number #320 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 184, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 20, 'learning_rate': 0.003645966265218969, 'batch_first': True, 'batch_size': 133, 'dropout': 0.13970399610308304, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 30}
Converting dataset to dataloader using batch size 133.
Initializing LSTM with 20 layers, 184 hidden size, 0.003645966265218969 learning rate, 0.13970399610308304 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:51
Starting tuning trial number #321 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 72, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.002274891009784761, 'batch_first': True, 'batch_size': 169, 'dropout': 0.17950908847069413, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 37}
Converting dataset to dataloader using batch size 169.
Initializing LSTM with 6 layers, 72 hidden size, 0.002274891009784761 learning rate, 0.17950908847069413 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:29:53
Starting tuning trial number #322 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 215, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.06136497394157379, 'batch_first': True, 'batch_size': 68, 'dropout': 0.024564724522791154, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 97}
Converting dataset to dataloader using batch size 68.
Initializing LSTM with 5 layers, 215 hidden size, 0.06136497394157379 learning rate, 0.024564724522791154 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:00
Starting tuning trial number #323 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 198, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.07654898521080146, 'batch_first': True, 'batch_size': 113, 'dropout': 0.15077496955690362, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 113.
Initializing LSTM with 2 layers, 198 hidden size, 0.07654898521080146 learning rate, 0.15077496955690362 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:02
Starting tuning trial number #324 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 86, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.03810903645696506, 'batch_first': True, 'batch_size': 124, 'dropout': 0.263383295165897, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 20}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 3 layers, 86 hidden size, 0.03810903645696506 learning rate, 0.263383295165897 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:04
Starting tuning trial number #325 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 59, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0017416096103760731, 'batch_first': True, 'batch_size': 187, 'dropout': 0.00039198058575227405, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 10}
Converting dataset to dataloader using batch size 187.
Initializing LSTM with 1 layers, 59 hidden size, 0.0017416096103760731 learning rate, 0.00039198058575227405 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:06
Starting tuning trial number #326 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.00036904804612923495, 'batch_first': True, 'batch_size': 76, 'dropout': 0.29188319194577217, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 95}
Converting dataset to dataloader using batch size 76.
Initializing LSTM with 4 layers, 32 hidden size, 0.00036904804612923495 learning rate, 0.29188319194577217 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:08
Starting tuning trial number #327 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 154, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.000626398964656324, 'batch_first': True, 'batch_size': 159, 'dropout': 0.20630023598718641, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 23}
Converting dataset to dataloader using batch size 159.
Initializing LSTM with 7 layers, 154 hidden size, 0.000626398964656324 learning rate, 0.20630023598718641 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:10
Starting tuning trial number #328 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 48, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.025150060412117002, 'batch_first': True, 'batch_size': 98, 'dropout': 0.3100398647646036, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 64}
Converting dataset to dataloader using batch size 98.
Initializing LSTM with 8 layers, 48 hidden size, 0.025150060412117002 learning rate, 0.3100398647646036 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:12
Starting tuning trial number #329 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 73, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.05491677042402764, 'batch_first': True, 'batch_size': 197, 'dropout': 0.3390764621463479, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 16}
Converting dataset to dataloader using batch size 197.
Initializing LSTM with 5 layers, 73 hidden size, 0.05491677042402764 learning rate, 0.3390764621463479 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:14
Starting tuning trial number #330 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 411, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.09878883759356048, 'batch_first': True, 'batch_size': 218, 'dropout': 0.426762126935185, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 218.
Initializing LSTM with 6 layers, 411 hidden size, 0.09878883759356048 learning rate, 0.426762126935185 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:16
Starting tuning trial number #331 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 99, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 6.681223421502872e-05, 'batch_first': True, 'batch_size': 62, 'dropout': 0.27809937842358895, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 69}
Converting dataset to dataloader using batch size 62.
Initializing LSTM with 3 layers, 99 hidden size, 6.681223421502872e-05 learning rate, 0.27809937842358895 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:18
Starting tuning trial number #332 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 204, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.001142397049773718, 'batch_first': True, 'batch_size': 178, 'dropout': 0.1711384637065075, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 40}
Converting dataset to dataloader using batch size 178.
Initializing LSTM with 2 layers, 204 hidden size, 0.001142397049773718 learning rate, 0.1711384637065075 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:20
Starting tuning trial number #333 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 64, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0007698204614428206, 'batch_first': True, 'batch_size': 108, 'dropout': 0.3226664195856531, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 43}
Converting dataset to dataloader using batch size 108.
Initializing LSTM with 4 layers, 64 hidden size, 0.0007698204614428206 learning rate, 0.3226664195856531 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:22
Starting tuning trial number #334 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 45, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.04583781628317245, 'batch_first': True, 'batch_size': 141, 'dropout': 0.11665373045039379, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 74}
Converting dataset to dataloader using batch size 141.
Initializing LSTM with 5 layers, 45 hidden size, 0.04583781628317245 learning rate, 0.11665373045039379 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:24
Starting tuning trial number #335 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 56, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0026476969388454946, 'batch_first': True, 'batch_size': 85, 'dropout': 0.049787871873183114, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 91}
Converting dataset to dataloader using batch size 85.
Initializing LSTM with 3 layers, 56 hidden size, 0.0026476969388454946 learning rate, 0.049787871873183114 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:31
Starting tuning trial number #336 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 336, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.03170097687836095, 'batch_first': True, 'batch_size': 120, 'dropout': 0.2690851505714328, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 47}
Converting dataset to dataloader using batch size 120.
Initializing LSTM with 6 layers, 336 hidden size, 0.03170097687836095 learning rate, 0.2690851505714328 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:33
Starting tuning trial number #337 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 130, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.012547808222331194, 'batch_first': True, 'batch_size': 124, 'dropout': 0.40009645697655316, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 124.
Initializing LSTM with 4 layers, 130 hidden size, 0.012547808222331194 learning rate, 0.40009645697655316 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:35
Starting tuning trial number #338 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 176, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0004897807865708152, 'batch_first': True, 'batch_size': 131, 'dropout': 0.24432452020344753, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 14}
Converting dataset to dataloader using batch size 131.
Initializing LSTM with 2 layers, 176 hidden size, 0.0004897807865708152 learning rate, 0.24432452020344753 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:37
Starting tuning trial number #339 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 83, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0009312226334330577, 'batch_first': True, 'batch_size': 117, 'dropout': 0.19749813011156586, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 71}
Converting dataset to dataloader using batch size 117.
Initializing LSTM with 4 layers, 83 hidden size, 0.0009312226334330577 learning rate, 0.19749813011156586 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:39
Starting tuning trial number #340 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 25, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.005940832751025288, 'batch_first': True, 'batch_size': 228, 'dropout': 0.06101928722619455, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 228.
Initializing LSTM with 3 layers, 25 hidden size, 0.005940832751025288 learning rate, 0.06101928722619455 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:30:41
Ending timer at: 18:33:15 
Elapsed CPU time: 1037.5654292833285s
Used real time: 0:02:34
------------------
Starting tuning trial number #341 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 11, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.006269348992202585, 'batch_first': True, 'batch_size': 228, 'dropout': 0.04148307049851789, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 228.
Initializing LSTM with 3 layers, 11 hidden size, 0.006269348992202585 learning rate, 0.04148307049851789 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:33:16
Starting tuning trial number #342 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 22, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.004155714217836292, 'batch_first': True, 'batch_size': 229, 'dropout': 0.017035155176000037, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 229.
Initializing LSTM with 5 layers, 22 hidden size, 0.004155714217836292 learning rate, 0.017035155176000037 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:33:18
Starting tuning trial number #343 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 36, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.008961681367251215, 'batch_first': True, 'batch_size': 192, 'dropout': 0.25720816173541994, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 192.
Initializing LSTM with 5 layers, 36 hidden size, 0.008961681367251215 learning rate, 0.25720816173541994 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:33:21
Starting tuning trial number #344 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 28, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.004723085908813008, 'batch_first': True, 'batch_size': 213, 'dropout': 0.18023999589061052, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 213.
Initializing LSTM with 7 layers, 28 hidden size, 0.004723085908813008 learning rate, 0.18023999589061052 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:33:23
Ending timer at: 18:35:58 
Elapsed CPU time: 1045.545013283337s
Used real time: 0:02:35
------------------
Starting tuning trial number #345 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 18, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.006409038192308725, 'batch_first': True, 'batch_size': 213, 'dropout': 0.18106228255356704, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 213.
Initializing LSTM with 8 layers, 18 hidden size, 0.006409038192308725 learning rate, 0.18106228255356704 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:35:59
Ending timer at: 18:38:31 
Elapsed CPU time: 1040.053616516669s
Used real time: 0:02:32
------------------
Starting tuning trial number #346 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.005479883875380167, 'batch_first': True, 'batch_size': 209, 'dropout': 0.18655719702195137, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 209.
Initializing LSTM with 8 layers, 10 hidden size, 0.005479883875380167 learning rate, 0.18655719702195137 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:31
Starting tuning trial number #347 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 23, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.007374768409059343, 'batch_first': True, 'batch_size': 224, 'dropout': 0.17809511039958126, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 224.
Initializing LSTM with 9 layers, 23 hidden size, 0.007374768409059343 learning rate, 0.17809511039958126 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:39
Starting tuning trial number #348 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 30, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.004607098305687827, 'batch_first': True, 'batch_size': 219, 'dropout': 0.16086672462579435, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 52}
Converting dataset to dataloader using batch size 219.
Initializing LSTM with 8 layers, 30 hidden size, 0.004607098305687827 learning rate, 0.16086672462579435 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:41
Starting tuning trial number #349 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 38, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.007161145344677659, 'batch_first': True, 'batch_size': 200, 'dropout': 0.176893617218576, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 200.
Initializing LSTM with 7 layers, 38 hidden size, 0.007161145344677659 learning rate, 0.176893617218576 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:43
Starting tuning trial number #350 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 25, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.004132526891013421, 'batch_first': True, 'batch_size': 210, 'dropout': 0.1544793510229321, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 210.
Initializing LSTM with 8 layers, 25 hidden size, 0.004132526891013421 learning rate, 0.1544793510229321 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:44
Starting tuning trial number #351 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 17, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.005803356324482022, 'batch_first': True, 'batch_size': 217, 'dropout': 0.16809875176890499, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 217.
Initializing LSTM with 7 layers, 17 hidden size, 0.005803356324482022 learning rate, 0.16809875176890499 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:46
Starting tuning trial number #352 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 28, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.0027320013671483265, 'batch_first': True, 'batch_size': 241, 'dropout': 0.1802110089904719, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 241.
Initializing LSTM with 7 layers, 28 hidden size, 0.0027320013671483265 learning rate, 0.1802110089904719 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:48
Starting tuning trial number #353 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 15, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.0031567243420639966, 'batch_first': True, 'batch_size': 212, 'dropout': 0.45397974329089863, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 212.
Initializing LSTM with 8 layers, 15 hidden size, 0.0031567243420639966 learning rate, 0.45397974329089863 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:56
Starting tuning trial number #354 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.011199741979336287, 'batch_first': True, 'batch_size': 213, 'dropout': 0.14125589345292644, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 52}
Converting dataset to dataloader using batch size 213.
Initializing LSTM with 2 layers, 10 hidden size, 0.011199741979336287 learning rate, 0.14125589345292644 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:38:58
Ending timer at: 18:41:22 
Elapsed CPU time: 964.9965893166685s
Used real time: 0:02:24
------------------
Starting tuning trial number #355 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.012171198982274367, 'batch_first': True, 'batch_size': 213, 'dropout': 0.1388827604832017, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 51}
Converting dataset to dataloader using batch size 213.
Initializing LSTM with 1 layers, 10 hidden size, 0.012171198982274367 learning rate, 0.1388827604832017 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:23
Starting tuning trial number #356 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 18, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.010100615932741405, 'batch_first': True, 'batch_size': 204, 'dropout': 0.15438853519643508, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 55}
Converting dataset to dataloader using batch size 204.
Initializing LSTM with 2 layers, 18 hidden size, 0.010100615932741405 learning rate, 0.15438853519643508 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:24
Starting tuning trial number #357 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 26, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.008839410523739126, 'batch_first': True, 'batch_size': 235, 'dropout': 0.13185307098373472, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 54}
Converting dataset to dataloader using batch size 235.
Initializing LSTM with 1 layers, 26 hidden size, 0.008839410523739126 learning rate, 0.13185307098373472 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:27
Starting tuning trial number #358 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.006366244411035678, 'batch_first': True, 'batch_size': 203, 'dropout': 0.14976416972817283, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 52}
Converting dataset to dataloader using batch size 203.
Initializing LSTM with 2 layers, 32 hidden size, 0.006366244411035678 learning rate, 0.14976416972817283 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:28
Starting tuning trial number #359 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 20, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.014945841011710754, 'batch_first': True, 'batch_size': 216, 'dropout': 0.14369310952006614, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 216.
Initializing LSTM with 2 layers, 20 hidden size, 0.014945841011710754 learning rate, 0.14369310952006614 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:30
Starting tuning trial number #360 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 37, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.005107316090458531, 'batch_first': True, 'batch_size': 206, 'dropout': 0.16655401421549948, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 58}
Converting dataset to dataloader using batch size 206.
Initializing LSTM with 3 layers, 37 hidden size, 0.005107316090458531 learning rate, 0.16655401421549948 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:32
Starting tuning trial number #361 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 11, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.003641901261637751, 'batch_first': True, 'batch_size': 246, 'dropout': 0.1928680662795596, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 246.
Initializing LSTM with 2 layers, 11 hidden size, 0.003641901261637751 learning rate, 0.1928680662795596 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:34
Starting tuning trial number #362 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 41, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.008810144574888408, 'batch_first': True, 'batch_size': 229, 'dropout': 0.1238071821817123, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 229.
Initializing LSTM with 1 layers, 41 hidden size, 0.008810144574888408 learning rate, 0.1238071821817123 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:36
Starting tuning trial number #363 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 23, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.018203521744848458, 'batch_first': True, 'batch_size': 216, 'dropout': 0.1592481319435771, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 216.
Initializing LSTM with 3 layers, 23 hidden size, 0.018203521744848458 learning rate, 0.1592481319435771 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:44
Starting tuning trial number #364 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.01104529615417282, 'batch_first': True, 'batch_size': 195, 'dropout': 0.14042641599155498, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 62}
Converting dataset to dataloader using batch size 195.
Initializing LSTM with 3 layers, 32 hidden size, 0.01104529615417282 learning rate, 0.14042641599155498 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:46
Starting tuning trial number #365 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 42, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0061566178681788574, 'batch_first': True, 'batch_size': 224, 'dropout': 0.26246158181571266, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 6}
Converting dataset to dataloader using batch size 224.
Initializing LSTM with 4 layers, 42 hidden size, 0.0061566178681788574 learning rate, 0.26246158181571266 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:48
Starting tuning trial number #366 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 49, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 9, 'learning_rate': 0.004602329435907029, 'batch_first': True, 'batch_size': 208, 'dropout': 0.18556946126086643, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 81}
Converting dataset to dataloader using batch size 208.
Initializing LSTM with 9 layers, 49 hidden size, 0.004602329435907029 learning rate, 0.18556946126086643 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:50
Starting tuning trial number #367 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 19, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.002131208382875864, 'batch_first': True, 'batch_size': 223, 'dropout': 0.25329030318881424, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 11}
Converting dataset to dataloader using batch size 223.
Initializing LSTM with 2 layers, 19 hidden size, 0.002131208382875864 learning rate, 0.25329030318881424 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:51
Starting tuning trial number #368 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 222, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0015736806067286056, 'batch_first': True, 'batch_size': 196, 'dropout': 0.17374631958638606, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 196.
Initializing LSTM with 3 layers, 222 hidden size, 0.0015736806067286056 learning rate, 0.17374631958638606 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:55
Starting tuning trial number #369 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 30, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.01432925110484624, 'batch_first': True, 'batch_size': 188, 'dropout': 0.21960919040279112, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 57}
Converting dataset to dataloader using batch size 188.
Initializing LSTM with 1 layers, 30 hidden size, 0.01432925110484624 learning rate, 0.21960919040279112 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:41:58
Starting tuning trial number #370 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0035941835396067897, 'batch_first': True, 'batch_size': 200, 'dropout': 0.23097321161347398, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 86}
Converting dataset to dataloader using batch size 200.
Initializing LSTM with 3 layers, 10 hidden size, 0.0035941835396067897 learning rate, 0.23097321161347398 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:05
Starting tuning trial number #371 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 48, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 23, 'learning_rate': 5.060942235464228e-05, 'batch_first': True, 'batch_size': 236, 'dropout': 0.2729706542245549, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 84}
Converting dataset to dataloader using batch size 236.
Initializing LSTM with 23 layers, 48 hidden size, 5.060942235464228e-05 learning rate, 0.2729706542245549 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:12
Starting tuning trial number #372 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 32, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.007255224362871465, 'batch_first': True, 'batch_size': 212, 'dropout': 0.4894273640809864, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 17}
Converting dataset to dataloader using batch size 212.
Initializing LSTM with 4 layers, 32 hidden size, 0.007255224362871465 learning rate, 0.4894273640809864 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:15
Starting tuning trial number #373 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 53, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.002698458849640862, 'batch_first': True, 'batch_size': 94, 'dropout': 0.14710104779792096, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 60}
Converting dataset to dataloader using batch size 94.
Initializing LSTM with 4 layers, 53 hidden size, 0.002698458849640862 learning rate, 0.14710104779792096 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:23
Starting tuning trial number #374 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 23, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.005171472211700087, 'batch_first': True, 'batch_size': 81, 'dropout': 0.1296390420215326, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 8}
Converting dataset to dataloader using batch size 81.
Initializing LSTM with 2 layers, 23 hidden size, 0.005171472211700087 learning rate, 0.1296390420215326 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:25
Starting tuning trial number #375 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 39, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.01977231586253346, 'batch_first': True, 'batch_size': 103, 'dropout': 0.1626420359038231, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 66}
Converting dataset to dataloader using batch size 103.
Initializing LSTM with 2 layers, 39 hidden size, 0.01977231586253346 learning rate, 0.1626420359038231 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:27
Starting tuning trial number #376 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 65, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.010178545325786874, 'batch_first': True, 'batch_size': 111, 'dropout': 0.2014327394653725, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 94}
Converting dataset to dataloader using batch size 111.
Initializing LSTM with 3 layers, 65 hidden size, 0.010178545325786874 learning rate, 0.2014327394653725 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:29
Starting tuning trial number #377 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 235, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0039406156587748434, 'batch_first': True, 'batch_size': 182, 'dropout': 0.24564926523136232, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 182.
Initializing LSTM with 4 layers, 235 hidden size, 0.0039406156587748434 learning rate, 0.24564926523136232 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:31
Starting tuning trial number #378 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 57, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.0012473190429860411, 'batch_first': True, 'batch_size': 71, 'dropout': 0.2850047455253278, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 88}
Converting dataset to dataloader using batch size 71.
Initializing LSTM with 3 layers, 57 hidden size, 0.0012473190429860411 learning rate, 0.2850047455253278 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:33
Starting tuning trial number #379 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 210, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0055385805184479196, 'batch_first': True, 'batch_size': 133, 'dropout': 0.1811650372950301, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 19}
Converting dataset to dataloader using batch size 133.
Initializing LSTM with 6 layers, 210 hidden size, 0.0055385805184479196 learning rate, 0.1811650372950301 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:35
Starting tuning trial number #380 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 44, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0016422468887061158, 'batch_first': True, 'batch_size': 125, 'dropout': 0.1376387509185455, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 13}
Converting dataset to dataloader using batch size 125.
Initializing LSTM with 4 layers, 44 hidden size, 0.0016422468887061158 learning rate, 0.1376387509185455 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:37
Starting tuning trial number #381 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 31, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 10, 'learning_rate': 8.311502421603873e-05, 'batch_first': True, 'batch_size': 226, 'dropout': 0.26803194044934353, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 50}
Converting dataset to dataloader using batch size 226.
Initializing LSTM with 10 layers, 31 hidden size, 8.311502421603873e-05 learning rate, 0.26803194044934353 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:39
Starting tuning trial number #382 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 248, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.00708575631090852, 'batch_first': True, 'batch_size': 116, 'dropout': 0.16995364667221785, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 98}
Converting dataset to dataloader using batch size 116.
Initializing LSTM with 3 layers, 248 hidden size, 0.00708575631090852 learning rate, 0.16995364667221785 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:41
Starting tuning trial number #383 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 22, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.024759513066674066, 'batch_first': True, 'batch_size': 89, 'dropout': 0.1509086717848397, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 65}
Converting dataset to dataloader using batch size 89.
Initializing LSTM with 5 layers, 22 hidden size, 0.024759513066674066 learning rate, 0.1509086717848397 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:43
Starting tuning trial number #384 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 70, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.002252400291270017, 'batch_first': True, 'batch_size': 191, 'dropout': 0.11139201211284772, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 52}
Converting dataset to dataloader using batch size 191.
Initializing LSTM with 3 layers, 70 hidden size, 0.002252400291270017 learning rate, 0.11139201211284772 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:45
Starting tuning trial number #385 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 44, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 17, 'learning_rate': 0.00015952050262878833, 'batch_first': True, 'batch_size': 24, 'dropout': 0.23597693396983083, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 24.
Initializing LSTM with 17 layers, 44 hidden size, 0.00015952050262878833 learning rate, 0.23597693396983083 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:48
Starting tuning trial number #386 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 115, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.0032396953084797664, 'batch_first': True, 'batch_size': 105, 'dropout': 0.1943075179597548, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 59}
Converting dataset to dataloader using batch size 105.
Initializing LSTM with 6 layers, 115 hidden size, 0.0032396953084797664 learning rate, 0.1943075179597548 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:50
Starting tuning trial number #387 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 57, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 1, 'learning_rate': 0.0083545601594682, 'batch_first': True, 'batch_size': 252, 'dropout': 0.026763437697463144, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 91}
Converting dataset to dataloader using batch size 252.
Initializing LSTM with 1 layers, 57 hidden size, 0.0083545601594682 learning rate, 0.026763437697463144 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:52
Starting tuning trial number #388 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 28, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 7, 'learning_rate': 0.012919869805989971, 'batch_first': True, 'batch_size': 207, 'dropout': 0.011236803421168326, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 63}
Converting dataset to dataloader using batch size 207.
Initializing LSTM with 7 layers, 28 hidden size, 0.012919869805989971 learning rate, 0.011236803421168326 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:54
Starting tuning trial number #389 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 10, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0010711194537483008, 'batch_first': True, 'batch_size': 128, 'dropout': 0.25449148306736324, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 21}
Converting dataset to dataloader using batch size 128.
Initializing LSTM with 2 layers, 10 hidden size, 0.0010711194537483008 learning rate, 0.25449148306736324 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:56
Starting tuning trial number #390 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 78, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 19, 'learning_rate': 0.018216440530663368, 'batch_first': True, 'batch_size': 217, 'dropout': 0.16070620624953844, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 93}
Converting dataset to dataloader using batch size 217.
Initializing LSTM with 19 layers, 78 hidden size, 0.018216440530663368 learning rate, 0.16070620624953844 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:42:58
Starting tuning trial number #391 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 226, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0014170099200593183, 'batch_first': True, 'batch_size': 138, 'dropout': 0.2798420689389243, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 15}
Converting dataset to dataloader using batch size 138.
Initializing LSTM with 4 layers, 226 hidden size, 0.0014170099200593183 learning rate, 0.2798420689389243 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:00
Starting tuning trial number #392 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 38, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.003919866376906565, 'batch_first': True, 'batch_size': 119, 'dropout': 0.00033937628813057133, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 18}
Converting dataset to dataloader using batch size 119.
Initializing LSTM with 3 layers, 38 hidden size, 0.003919866376906565 learning rate, 0.00033937628813057133 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:02
Starting tuning trial number #393 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 52, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0019254769923356713, 'batch_first': True, 'batch_size': 52, 'dropout': 0.21258943967069727, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 67}
Converting dataset to dataloader using batch size 52.
Initializing LSTM with 5 layers, 52 hidden size, 0.0019254769923356713 learning rate, 0.21258943967069727 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:04
Starting tuning trial number #394 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 66, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.026875768008612, 'batch_first': True, 'batch_size': 74, 'dropout': 0.18737173347914465, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 78}
Converting dataset to dataloader using batch size 74.
Initializing LSTM with 4 layers, 66 hidden size, 0.026875768008612 learning rate, 0.18737173347914465 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:06
Starting tuning trial number #395 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 95, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 2, 'learning_rate': 0.0007315557696185214, 'batch_first': True, 'batch_size': 231, 'dropout': 0.12528740168549127, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 5}
Converting dataset to dataloader using batch size 231.
Initializing LSTM with 2 layers, 95 hidden size, 0.0007315557696185214 learning rate, 0.12528740168549127 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:08
Starting tuning trial number #396 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 21, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 6, 'learning_rate': 0.005600557634793913, 'batch_first': True, 'batch_size': 133, 'dropout': 0.4715518766235124, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 61}
Converting dataset to dataloader using batch size 133.
Initializing LSTM with 6 layers, 21 hidden size, 0.005600557634793913 learning rate, 0.4715518766235124 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:10
Starting tuning trial number #397 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 39, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 5, 'learning_rate': 0.0009724843856530305, 'batch_first': True, 'batch_size': 204, 'dropout': 0.14540489106106744, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 10}
Converting dataset to dataloader using batch size 204.
Initializing LSTM with 5 layers, 39 hidden size, 0.0009724843856530305 learning rate, 0.14540489106106744 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:12
Starting tuning trial number #398 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 202, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 4, 'learning_rate': 0.0027728601296151405, 'batch_first': True, 'batch_size': 112, 'dropout': 0.26141039711114344, 'bidirectional': False, 'optimizer_name': 'Adam', 'number_of_epochs': 53}
Converting dataset to dataloader using batch size 112.
Initializing LSTM with 4 layers, 202 hidden size, 0.0027728601296151405 learning rate, 0.26141039711114344 dropout. Optimiser: Adam,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:14
Starting tuning trial number #399 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 212, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 8, 'learning_rate': 0.015447294007592713, 'batch_first': True, 'batch_size': 59, 'dropout': 0.23962491956113613, 'bidirectional': False, 'optimizer_name': 'AdaDelta', 'number_of_epochs': 82}
Converting dataset to dataloader using batch size 59.
Initializing LSTM with 8 layers, 212 hidden size, 0.015447294007592713 learning rate, 0.23962491956113613 dropout. Optimiser: AdaDelta,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:16
Starting tuning trial number #400 of total 400
with params: {'number_of_features': 1, 'hidden_layer_size': 140, 'input_window_size': 10, 'output_window_size': 1, 'number_of_layers': 3, 'learning_rate': 0.010863659945925514, 'batch_first': True, 'batch_size': 80, 'dropout': 0.1560985825969743, 'bidirectional': False, 'optimizer_name': 'RMSprop', 'number_of_epochs': 56}
Converting dataset to dataloader using batch size 80.
Initializing LSTM with 3 layers, 140 hidden size, 0.010863659945925514 learning rate, 0.1560985825969743 dropout. Optimiser: RMSprop,  input window size: 10, output window size: 1
Using pre-existing neptune run for PytorchLightning

------------------
Starting new timer at 18:43:18
Best params!
Params updated with best params
Initializing LSTM with 4 layers, 29 hidden size, 0.0011303272466904957 learning rate, 0.14299357931140072 dropout. Optimiser: Adam,  input window size: 5, output window size: 1
Using pre-existing neptune run for PytorchLightning
Best trial: 10,58
best_score: 0.003281844314187765
best_params: {'batch_size': 53, 'dropout': 0.14299357931140072, 'hidden_layer_size': 29, 'learning_rate': 0.0011303272466904957, 'number_of_epochs': 75, 'number_of_layers': 4, 'optimizer_name': 'Adam'}
Training model
Testing model
Testing error: {'MSE': 0.005301004275679588, 'MAE': 0.05658806674182415, 'MASE': 0.975614607334137, 'SMAPE': 0.07696926221251488}.
Saving model
Finished
